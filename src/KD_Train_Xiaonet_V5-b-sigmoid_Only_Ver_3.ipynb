{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a73e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Distillation Training for XiaoNet\n",
    "Teacher: PhaseNet (from STEAD)\n",
    "Student: XiaoNet (v2, v3, v4, or v5)\n",
    "Dataset: OKLA regional seismic data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seismology & SeisBench\n",
    "import obspy\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing local modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# XiaoNet modules\n",
    "from models.xn_xiao_net_v2 import XiaoNet as XiaoNetV2\n",
    "from models.xn_xiao_net_v3 import XiaoNet as XiaoNetV3\n",
    "from models.xn_xiao_net_v4 import XiaoNetFast as XiaoNetV4\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge as XiaoNetV5\n",
    "from models.xn_xiao_net_v5a import XiaoNetV5A as XiaoNetV5A\n",
    "from models.xn_xiao_net_v5b_sigmoid import XiaoNetV5B as XiaoNetV5B\n",
    "\n",
    "print(\"✓ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a324eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to halt training when validation loss stops improving.\n",
    "    \n",
    "    Args:\n",
    "        patience: Epochs to wait before stopping\n",
    "        min_delta: Minimum improvement threshold (positive value)\n",
    "        checkpoint_dir: Directory for checkpoints\n",
    "        model_name: Name of the model (for checkpoint filename)\n",
    "        sample_fraction: Sample fraction used in training (for checkpoint filename)\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.0, checkpoint_dir='checkpoints/', model_name='model', sample_fraction=1.0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_loss = None  # Store actual loss for clarity\n",
    "        self.early_stop = False\n",
    "        self.checkpoint_path = self.checkpoint_dir / f'{model_name}_sf{sample_fraction}_best_model.pth'\n",
    "    \n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        \"\"\"\n",
    "        Check if training should stop early.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss (lower is better)\n",
    "            model: Model to save if improvement found\n",
    "            epoch: Current epoch number\n",
    "        \"\"\"\n",
    "        # First epoch - initialize\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model, epoch, val_loss)\n",
    "            return\n",
    "        \n",
    "        # Check for improvement (val_loss decreased by at least min_delta)\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model, epoch, val_loss)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience} (best loss: {self.best_loss:.6f})')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, val_loss):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'best_loss': self.best_loss\n",
    "        }, self.checkpoint_path)\n",
    "        if self.verbose:\n",
    "            print(f'✓ Validation loss improved: {val_loss:.6f}. Saved to {self.checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c1b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5926c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device(device_type='cuda'):\n",
    "    \"\"\"Setup compute device (CUDA if available, else CPU).\"\"\"\n",
    "    if device_type == 'cuda' and torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    return torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c646aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = setup_device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22312606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /Users/hongyuxiao/Hongyu_File/xiao_net/config.json\n",
      "{\n",
      "  \"peak_detection\": {\n",
      "    \"sampling_rate\": 100,\n",
      "    \"height\": 0.5,\n",
      "    \"distance\": 100\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"dataset_name\": \"OKLA_1Mil_120s_Ver_3\",\n",
      "    \"sampling_rate\": 100,\n",
      "    \"window_len\": 3001,\n",
      "    \"samples_before\": 3000,\n",
      "    \"windowlen_large\": 6000,\n",
      "    \"sample_fraction\": 0.4\n",
      "  },\n",
      "  \"data_filter\": {\n",
      "    \"min_magnitude\": -1.0,\n",
      "    \"max_magnitude\": 10.0\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 10,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"epochs\": 50,\n",
      "    \"patience\": 5,\n",
      "    \"loss_weights\": [\n",
      "      0.01,\n",
      "      0.4,\n",
      "      0.59\n",
      "    ],\n",
      "    \"optimization\": {\n",
      "      \"mixed_precision\": true,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"pin_memory\": true,\n",
      "      \"prefetch_factor\": 2,\n",
      "      \"persistent_workers\": true\n",
      "    }\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"use_cuda\": true,\n",
      "    \"device_id\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "config_path = Path.cwd().parent / \"config.json\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration from: {config_path}\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing student model...\n",
      "Model class name: XiaoNetV5B\n",
      "\n",
      "✓ XiaoNetV5B student loaded successfully!\n",
      "Total parameters: 20,208\n",
      "Trainable parameters: 20,208\n",
      "Model on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing student model...\")\n",
    "model = XiaoNetV5B(window_len=3001, in_channels=3, num_phases=3, base_channels=8)\n",
    "model.to(device)\n",
    "model.train()  # Set to training mode for student\n",
    "\n",
    "# Extract model name from class\n",
    "model_name = model.__class__.__name__\n",
    "print(f\"Model class name: {model_name}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ {model_name} student loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "887fb320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OKLA regional seismic dataset...\n",
      "Sampling 40.0% of data for faster training...\n",
      "Sampled dataset size: 455,251\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "Training samples: 318,230\n",
      "Validation samples: 68,437\n",
      "Test samples: 68,584\n",
      "Total samples: 455,251\n"
     ]
    }
   ],
   "source": [
    "# Load OKLA dataset\n",
    "print(\"Loading OKLA regional seismic dataset...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n",
    "# Optional: Use subset for faster experimentation\n",
    "sample_fraction = config.get('data', {}).get('sample_fraction', 0.1)\n",
    "if sample_fraction < 1.0:\n",
    "    print(f\"Sampling {sample_fraction*100}% of data for faster training...\")\n",
    "    # Create a random mask for sampling\n",
    "    mask = np.random.random(len(data)) < sample_fraction\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"Sampled dataset size: {len(data):,}\")\n",
    "\n",
    "# Split into train/dev/test\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train):,}\")\n",
    "print(f\"Validation samples: {len(dev):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Total samples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa20617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying magnitude filters: -1.0 < M < 10.0\n",
      "✓ [Data Filter]: Start - magnitude > -1.0\n",
      "✓ [Data Filter]: Applied - magnitude > -1.0, remaining samples: 455,142\n",
      "✓ [Data Filter]: Start - magnitude < 10.0\n",
      "✓ [Data Filter]: Applied - magnitude < 10.0, remaining samples: 455,142\n",
      "\n",
      "✓ Magnitude filtering complete: 455,142 traces in range [-1.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "# Magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', 1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 2.0)\n",
    "\n",
    "print(f\"Applying magnitude filters: {min_magnitude} < M < {max_magnitude}\")\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude > {min_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude < {max_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Magnitude filtering complete: {len(data):,} traces in range [{min_magnitude}, {max_magnitude}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d3b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total dataset size: 455,142\n",
      "Train size: 318,230\n",
      "Validation size: 68,437\n",
      "Test size: 68,584\n",
      "Sampling rate: 100 Hz\n",
      "Window length: 3001 samples\n",
      "Magnitude stats: min=-0.94, max=5.58, mean=2.13\n",
      "Metadata columns: ['index', 'station_network_code', 'station_code', 'trace_channel', 'station_latitude_deg', 'station_longitude_deg', 'station_elevation_m', 'trace_p_arrival_sample', 'trace_p_status', 'trace_p_weight', 'path_p_travel_sec', 'trace_s_arrival_sample', 'trace_s_status', 'trace_s_weight', 'source_id', 'source_origin_time', 'source_origin_uncertainty_sec', 'source_latitude_deg', 'source_longitude_deg', 'source_error_sec', 'source_gap_deg', 'source_horizontal_uncertainty_km', 'source_depth_km', 'source_depth_uncertainty_km', 'source_magnitude', 'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake', 'source_distance_deg', 'source_distance_km', 'path_back_azimuth_deg', 'trace_snr_db', 'trace_coda_end_sample', 'trace_start_time', 'trace_category', 'trace_name', 'split', 'trace_name_original', 'trace_chunk', 'trace_sampling_rate_hz', 'trace_component_order']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary for training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core sizes\n",
    "print(f\"Total dataset size: {len(data):,}\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Sampling configuration\n",
    "sampling_rate = config.get('data', {}).get('sampling_rate', 'unknown')\n",
    "window_len = config.get('data', {}).get('window_len', 'unknown')\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Window length: {window_len} samples\")\n",
    "\n",
    "# Metadata summary (if available)\n",
    "if hasattr(data, 'metadata') and data.metadata is not None:\n",
    "    if 'source_magnitude' in data.metadata:\n",
    "        mags = data.metadata['source_magnitude']\n",
    "        print(f\"Magnitude stats: min={mags.min():.2f}, max={mags.max():.2f}, mean={mags.mean():.2f}\")\n",
    "    print(f\"Metadata columns: {list(data.metadata.columns)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ab888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset split after filtering\n",
      "Train size: 318,152\n",
      "Validation size: 68,425\n",
      "Test size: 68,565\n",
      "Split ratios: train=69.90%, dev=15.03%, test=15.06%\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/dev/test after filtering\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(\"\\n✓ Dataset split after filtering\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Split ratios\n",
    "n_total = len(train) + len(dev) + len(test)\n",
    "if n_total > 0:\n",
    "    print(f\"Split ratios: train={len(train)/n_total:.2%}, dev={len(dev)/n_total:.2%}, test={len(test)/n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c2672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET OBJECTS\n",
      "============================================================\n",
      "Train dataset: OKLA_1Mil_120s_Ver_3 - 318152 traces\n",
      "Dev dataset:   OKLA_1Mil_120s_Ver_3 - 68425 traces\n",
      "Test dataset:  OKLA_1Mil_120s_Ver_3 - 68565 traces\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset objects (compact summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET OBJECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train dataset: {train}\")\n",
    "print(f\"Dev dataset:   {dev}\")\n",
    "print(f\"Test dataset:  {test}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81187cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b64fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c83d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dddf3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PEAK DETECTION SETTINGS\n",
      "============================================================\n",
      "Sampling rate: 100 Hz\n",
      "Height threshold: 0.5\n",
      "Minimum peak distance: 100 samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PEAK DETECTION SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Height threshold: {height}\")\n",
    "print(f\"Minimum peak distance: {distance} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0b4ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ [DataLoader]: batch_size=64, num_workers=10\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "print(f\"✓ [DataLoader]: batch_size={batch_size}, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "570e9c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "[Dataset]\n",
      "  Total samples:      455,142\n",
      "  Train/Validation/Test:     318,152 / 68,425 / 68,565\n",
      "  Sample fraction:    40.0%\n",
      "\n",
      "[Device]\n",
      "  Device:             cpu\n",
      "\n",
      "[Training]\n",
      "  Batch size:         64\n",
      "  Num workers:        10\n",
      "  Learning rate:      0.01\n",
      "  Epochs:             50\n",
      "  Patience:           5\n",
      "\n",
      "[Peak Detection]\n",
      "  Sampling rate:      100 Hz\n",
      "  Height threshold:   0.5\n",
      "  Min peak distance:  100 samples\n",
      "============================================================\n",
      "Ready to start training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset info\n",
    "print(\"[Dataset]\")\n",
    "print(f\"  Total samples:      {len(data):,}\")\n",
    "print(f\"  Train/Validation/Test:     {len(train):,} / {len(dev):,} / {len(test):,}\")\n",
    "print(f\"  Sample fraction:    {sample_fraction*100:.1f}%\")\n",
    "\n",
    "# Device\n",
    "print(\"\\n[Device]\")\n",
    "print(f\"  Device:             {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "print(\"\\n[Training]\")\n",
    "print(f\"  Batch size:         {batch_size}\")\n",
    "print(f\"  Num workers:        {num_workers}\")\n",
    "print(f\"  Learning rate:      {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs:             {config['training']['epochs']}\")\n",
    "print(f\"  Patience:           {config['training']['patience']}\")\n",
    "\n",
    "# Peak detection\n",
    "print(\"\\n[Peak Detection]\")\n",
    "print(f\"  Sampling rate:      {sampling_rate} Hz\")\n",
    "print(f\"  Height threshold:   {height}\")\n",
    "print(f\"  Min peak distance:  {distance} samples\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to start training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830aeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c530a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6364f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss for sigmoid outputs.\n",
    "    Each phase channel is treated independently.\n",
    "    \"\"\"\n",
    "    # Clamp predictions to valid range\n",
    "    y_pred = torch.clamp(y_pred, eps, 1.0 - eps)\n",
    "    \n",
    "    # Binary CE: -[y*log(p) + (1-y)*log(1-p)]\n",
    "    bce = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    \n",
    "    # Mean across all dimensions (batch, channels, samples)\n",
    "    return bce.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce428776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZER SETTINGS\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZER SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61328216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EARLY STOPPING & CHECKPOINTS\n",
      "============================================================\n",
      "Model name:     XiaoNetV5B\n",
      "Sample fraction: 0.4\n",
      "Checkpoint dir: /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints\n",
      "Best model:     /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/XiaoNetV5B_sf0.4_best_model.pth\n",
      "Final model:    /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/XiaoNetV5B_sf0.4_final_model.pth\n",
      "History file:   /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/XiaoNetV5B_sf0.4_loss_history.json\n",
      "Patience:       5\n",
      "Min delta:      0.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and checkpoint setup\n",
    "checkpoint_dir = Path.cwd().parent / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use extracted model name for all checkpoint files\n",
    "best_model_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_best_model.pth\"\n",
    "final_model_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_final_model.pth\"\n",
    "history_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_loss_history.json\"\n",
    "\n",
    "patience = config.get('training', {}).get('patience', 5)\n",
    "min_delta = config.get('training', {}).get('min_delta', 0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    model_name=model_name,\n",
    "    sample_fraction=sample_fraction,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Loss history container\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "\n",
    "# Helper functions for saving\n",
    "def save_loss_history(history_dict, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(history_dict, f, indent=2)\n",
    "    print(f\"✓ Loss history saved to {path}\")\n",
    "\n",
    "\n",
    "def save_final_model(model, path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, path)\n",
    "    print(f\"✓ Final model saved to {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLY STOPPING & CHECKPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model name:     {model_name}\")\n",
    "print(f\"Sample fraction: {sample_fraction}\")\n",
    "print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
    "print(f\"Best model:     {best_model_path}\")\n",
    "print(f\"Final model:    {final_model_path}\")\n",
    "print(f\"History file:   {history_path}\")\n",
    "print(f\"Patience:       {patience}\")\n",
    "print(f\"Min delta:      {min_delta}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "  loss: 0.766407  [    0/318152]\n",
      "  loss: 0.648639  [  320/318152]\n",
      "  loss: 0.527638  [  640/318152]\n",
      "  loss: 0.413735  [  960/318152]\n",
      "  loss: 0.310016  [ 1280/318152]\n",
      "  loss: 0.225324  [ 1600/318152]\n",
      "  loss: 0.169217  [ 1920/318152]\n",
      "  loss: 0.133700  [ 2240/318152]\n",
      "  loss: 0.110482  [ 2560/318152]\n",
      "  loss: 0.103926  [ 2880/318152]\n",
      "  loss: 0.093087  [ 3200/318152]\n",
      "  loss: 0.089772  [ 3520/318152]\n",
      "  loss: 0.089254  [ 3840/318152]\n",
      "  loss: 0.083394  [ 4160/318152]\n",
      "  loss: 0.087726  [ 4480/318152]\n",
      "  loss: 0.082319  [ 4800/318152]\n",
      "  loss: 0.086310  [ 5120/318152]\n",
      "  loss: 0.089001  [ 5440/318152]\n",
      "  loss: 0.087340  [ 5760/318152]\n",
      "  loss: 0.081560  [ 6080/318152]\n",
      "  loss: 0.086775  [ 6400/318152]\n",
      "  loss: 0.086758  [ 6720/318152]\n",
      "  loss: 0.080503  [ 7040/318152]\n",
      "  loss: 0.081517  [ 7360/318152]\n",
      "  loss: 0.071667  [ 7680/318152]\n",
      "  loss: 0.082593  [ 8000/318152]\n",
      "  loss: 0.075414  [ 8320/318152]\n",
      "  loss: 0.075261  [ 8640/318152]\n",
      "  loss: 0.072817  [ 8960/318152]\n",
      "  loss: 0.075632  [ 9280/318152]\n",
      "  loss: 0.082786  [ 9600/318152]\n",
      "  loss: 0.078912  [ 9920/318152]\n",
      "  loss: 0.079591  [10240/318152]\n",
      "  loss: 0.081158  [10560/318152]\n",
      "  loss: 0.073807  [10880/318152]\n",
      "  loss: 0.076268  [11200/318152]\n",
      "  loss: 0.072634  [11520/318152]\n",
      "  loss: 0.071813  [11840/318152]\n",
      "  loss: 0.074364  [12160/318152]\n",
      "  loss: 0.069527  [12480/318152]\n",
      "  loss: 0.078081  [12800/318152]\n",
      "  loss: 0.069900  [13120/318152]\n",
      "  loss: 0.075379  [13440/318152]\n",
      "  loss: 0.076351  [13760/318152]\n",
      "  loss: 0.073985  [14080/318152]\n",
      "  loss: 0.067496  [14400/318152]\n",
      "  loss: 0.078201  [14720/318152]\n",
      "  loss: 0.064415  [15040/318152]\n",
      "  loss: 0.077933  [15360/318152]\n",
      "  loss: 0.068781  [15680/318152]\n",
      "  loss: 0.070061  [16000/318152]\n",
      "  loss: 0.078802  [16320/318152]\n",
      "  loss: 0.073233  [16640/318152]\n",
      "  loss: 0.076575  [16960/318152]\n",
      "  loss: 0.069199  [17280/318152]\n",
      "  loss: 0.075424  [17600/318152]\n",
      "  loss: 0.065697  [17920/318152]\n",
      "  loss: 0.060796  [18240/318152]\n",
      "  loss: 0.057241  [18560/318152]\n",
      "  loss: 0.068334  [18880/318152]\n",
      "  loss: 0.065022  [19200/318152]\n",
      "  loss: 0.064555  [19520/318152]\n",
      "  loss: 0.064142  [19840/318152]\n",
      "  loss: 0.063682  [20160/318152]\n",
      "  loss: 0.070244  [20480/318152]\n",
      "  loss: 0.068478  [20800/318152]\n",
      "  loss: 0.066329  [21120/318152]\n",
      "  loss: 0.064581  [21440/318152]\n",
      "  loss: 0.057272  [21760/318152]\n",
      "  loss: 0.063202  [22080/318152]\n",
      "  loss: 0.069371  [22400/318152]\n",
      "  loss: 0.062594  [22720/318152]\n",
      "  loss: 0.066321  [23040/318152]\n",
      "  loss: 0.057904  [23360/318152]\n",
      "  loss: 0.062656  [23680/318152]\n",
      "  loss: 0.069248  [24000/318152]\n",
      "  loss: 0.067170  [24320/318152]\n",
      "  loss: 0.065528  [24640/318152]\n",
      "  loss: 0.061366  [24960/318152]\n",
      "  loss: 0.061685  [25280/318152]\n",
      "  loss: 0.060533  [25600/318152]\n",
      "  loss: 0.060631  [25920/318152]\n",
      "  loss: 0.063364  [26240/318152]\n",
      "  loss: 0.062889  [26560/318152]\n",
      "  loss: 0.064214  [26880/318152]\n",
      "  loss: 0.065191  [27200/318152]\n",
      "  loss: 0.064768  [27520/318152]\n",
      "  loss: 0.070439  [27840/318152]\n",
      "  loss: 0.068085  [28160/318152]\n",
      "  loss: 0.057180  [28480/318152]\n",
      "  loss: 0.056764  [28800/318152]\n",
      "  loss: 0.061105  [29120/318152]\n",
      "  loss: 0.061750  [29440/318152]\n",
      "  loss: 0.056383  [29760/318152]\n",
      "  loss: 0.054525  [30080/318152]\n",
      "  loss: 0.061377  [30400/318152]\n",
      "  loss: 0.061478  [30720/318152]\n",
      "  loss: 0.062521  [31040/318152]\n",
      "  loss: 0.069514  [31360/318152]\n",
      "  loss: 0.060835  [31680/318152]\n",
      "  loss: 0.052840  [32000/318152]\n",
      "  loss: 0.060972  [32320/318152]\n",
      "  loss: 0.061490  [32640/318152]\n",
      "  loss: 0.058081  [32960/318152]\n",
      "  loss: 0.065406  [33280/318152]\n",
      "  loss: 0.065067  [33600/318152]\n",
      "  loss: 0.060491  [33920/318152]\n",
      "  loss: 0.067805  [34240/318152]\n",
      "  loss: 0.053594  [34560/318152]\n",
      "  loss: 0.060375  [34880/318152]\n",
      "  loss: 0.057040  [35200/318152]\n",
      "  loss: 0.057347  [35520/318152]\n",
      "  loss: 0.057335  [35840/318152]\n",
      "  loss: 0.056259  [36160/318152]\n",
      "  loss: 0.059078  [36480/318152]\n",
      "  loss: 0.053048  [36800/318152]\n",
      "  loss: 0.058730  [37120/318152]\n",
      "  loss: 0.059000  [37440/318152]\n",
      "  loss: 0.058972  [37760/318152]\n",
      "  loss: 0.057398  [38080/318152]\n",
      "  loss: 0.056250  [38400/318152]\n",
      "  loss: 0.052884  [38720/318152]\n",
      "  loss: 0.053441  [39040/318152]\n",
      "  loss: 0.056858  [39360/318152]\n",
      "  loss: 0.062901  [39680/318152]\n",
      "  loss: 0.057890  [40000/318152]\n",
      "  loss: 0.050668  [40320/318152]\n",
      "  loss: 0.054802  [40640/318152]\n",
      "  loss: 0.061881  [40960/318152]\n",
      "  loss: 0.058339  [41280/318152]\n",
      "  loss: 0.055669  [41600/318152]\n",
      "  loss: 0.053336  [41920/318152]\n",
      "  loss: 0.062350  [42240/318152]\n",
      "  loss: 0.051624  [42560/318152]\n",
      "  loss: 0.051686  [42880/318152]\n",
      "  loss: 0.054543  [43200/318152]\n",
      "  loss: 0.054089  [43520/318152]\n",
      "  loss: 0.056897  [43840/318152]\n",
      "  loss: 0.058098  [44160/318152]\n",
      "  loss: 0.051216  [44480/318152]\n",
      "  loss: 0.060267  [44800/318152]\n",
      "  loss: 0.055862  [45120/318152]\n",
      "  loss: 0.060795  [45440/318152]\n",
      "  loss: 0.057678  [45760/318152]\n",
      "  loss: 0.054463  [46080/318152]\n",
      "  loss: 0.052922  [46400/318152]\n",
      "  loss: 0.049862  [46720/318152]\n",
      "  loss: 0.052568  [47040/318152]\n",
      "  loss: 0.052107  [47360/318152]\n",
      "  loss: 0.055570  [47680/318152]\n",
      "  loss: 0.059917  [48000/318152]\n",
      "  loss: 0.055985  [48320/318152]\n",
      "  loss: 0.057922  [48640/318152]\n",
      "  loss: 0.050605  [48960/318152]\n",
      "  loss: 0.053046  [49280/318152]\n",
      "  loss: 0.052379  [49600/318152]\n",
      "  loss: 0.059400  [49920/318152]\n",
      "  loss: 0.058997  [50240/318152]\n",
      "  loss: 0.056979  [50560/318152]\n",
      "  loss: 0.057203  [50880/318152]\n",
      "  loss: 0.059467  [51200/318152]\n",
      "  loss: 0.057635  [51520/318152]\n",
      "  loss: 0.047242  [51840/318152]\n",
      "  loss: 0.054938  [52160/318152]\n",
      "  loss: 0.052295  [52480/318152]\n",
      "  loss: 0.056216  [52800/318152]\n",
      "  loss: 0.061525  [53120/318152]\n",
      "  loss: 0.055426  [53440/318152]\n",
      "  loss: 0.051633  [53760/318152]\n",
      "  loss: 0.056102  [54080/318152]\n",
      "  loss: 0.052477  [54400/318152]\n",
      "  loss: 0.057634  [54720/318152]\n",
      "  loss: 0.056945  [55040/318152]\n",
      "  loss: 0.059461  [55360/318152]\n",
      "  loss: 0.050216  [55680/318152]\n",
      "  loss: 0.053615  [56000/318152]\n",
      "  loss: 0.051156  [56320/318152]\n",
      "  loss: 0.055476  [56640/318152]\n",
      "  loss: 0.046746  [56960/318152]\n",
      "  loss: 0.051499  [57280/318152]\n",
      "  loss: 0.052945  [57600/318152]\n",
      "  loss: 0.051723  [57920/318152]\n",
      "  loss: 0.050526  [58240/318152]\n",
      "  loss: 0.055826  [58560/318152]\n",
      "  loss: 0.056683  [58880/318152]\n",
      "  loss: 0.057271  [59200/318152]\n",
      "  loss: 0.049064  [59520/318152]\n",
      "  loss: 0.052811  [59840/318152]\n",
      "  loss: 0.056526  [60160/318152]\n",
      "  loss: 0.053681  [60480/318152]\n",
      "  loss: 0.052819  [60800/318152]\n",
      "  loss: 0.051832  [61120/318152]\n",
      "  loss: 0.051507  [61440/318152]\n",
      "  loss: 0.061781  [61760/318152]\n",
      "  loss: 0.054130  [62080/318152]\n",
      "  loss: 0.054572  [62400/318152]\n",
      "  loss: 0.056112  [62720/318152]\n",
      "  loss: 0.047102  [63040/318152]\n",
      "  loss: 0.059213  [63360/318152]\n",
      "  loss: 0.055000  [63680/318152]\n",
      "  loss: 0.055993  [64000/318152]\n",
      "  loss: 0.049322  [64320/318152]\n",
      "  loss: 0.052622  [64640/318152]\n",
      "  loss: 0.053183  [64960/318152]\n",
      "  loss: 0.058019  [65280/318152]\n",
      "  loss: 0.049075  [65600/318152]\n",
      "  loss: 0.053807  [65920/318152]\n",
      "  loss: 0.054205  [66240/318152]\n",
      "  loss: 0.053332  [66560/318152]\n",
      "  loss: 0.056051  [66880/318152]\n",
      "  loss: 0.052976  [67200/318152]\n",
      "  loss: 0.054645  [67520/318152]\n",
      "  loss: 0.054091  [67840/318152]\n",
      "  loss: 0.052540  [68160/318152]\n",
      "  loss: 0.051702  [68480/318152]\n",
      "  loss: 0.049910  [68800/318152]\n",
      "  loss: 0.057975  [69120/318152]\n",
      "  loss: 0.052629  [69440/318152]\n",
      "  loss: 0.050814  [69760/318152]\n",
      "  loss: 0.055914  [70080/318152]\n",
      "  loss: 0.051335  [70400/318152]\n",
      "  loss: 0.052605  [70720/318152]\n",
      "  loss: 0.051157  [71040/318152]\n",
      "  loss: 0.060920  [71360/318152]\n",
      "  loss: 0.052343  [71680/318152]\n",
      "  loss: 0.055049  [72000/318152]\n",
      "  loss: 0.056559  [72320/318152]\n",
      "  loss: 0.050165  [72640/318152]\n",
      "  loss: 0.056273  [72960/318152]\n",
      "  loss: 0.050749  [73280/318152]\n",
      "  loss: 0.052942  [73600/318152]\n",
      "  loss: 0.050707  [73920/318152]\n",
      "  loss: 0.050159  [74240/318152]\n",
      "  loss: 0.054498  [74560/318152]\n",
      "  loss: 0.050307  [74880/318152]\n",
      "  loss: 0.049598  [75200/318152]\n",
      "  loss: 0.052045  [75520/318152]\n",
      "  loss: 0.046389  [75840/318152]\n",
      "  loss: 0.052107  [76160/318152]\n",
      "  loss: 0.043841  [76480/318152]\n",
      "  loss: 0.049067  [76800/318152]\n",
      "  loss: 0.051311  [77120/318152]\n",
      "  loss: 0.050624  [77440/318152]\n",
      "  loss: 0.055413  [77760/318152]\n",
      "  loss: 0.049442  [78080/318152]\n",
      "  loss: 0.055957  [78400/318152]\n",
      "  loss: 0.054398  [78720/318152]\n",
      "  loss: 0.052422  [79040/318152]\n",
      "  loss: 0.054279  [79360/318152]\n",
      "  loss: 0.055574  [79680/318152]\n",
      "  loss: 0.056577  [80000/318152]\n",
      "  loss: 0.053670  [80320/318152]\n",
      "  loss: 0.051736  [80640/318152]\n",
      "  loss: 0.051616  [80960/318152]\n",
      "  loss: 0.055330  [81280/318152]\n",
      "  loss: 0.055464  [81600/318152]\n",
      "  loss: 0.052341  [81920/318152]\n",
      "  loss: 0.053444  [82240/318152]\n",
      "  loss: 0.055944  [82560/318152]\n",
      "  loss: 0.047165  [82880/318152]\n",
      "  loss: 0.058544  [83200/318152]\n",
      "  loss: 0.057355  [83520/318152]\n",
      "  loss: 0.056731  [83840/318152]\n",
      "  loss: 0.058139  [84160/318152]\n",
      "  loss: 0.051979  [84480/318152]\n",
      "  loss: 0.044667  [84800/318152]\n",
      "  loss: 0.050987  [85120/318152]\n",
      "  loss: 0.057548  [85440/318152]\n",
      "  loss: 0.045580  [85760/318152]\n",
      "  loss: 0.049181  [86080/318152]\n",
      "  loss: 0.057478  [86400/318152]\n",
      "  loss: 0.052068  [86720/318152]\n",
      "  loss: 0.048723  [87040/318152]\n",
      "  loss: 0.049015  [87360/318152]\n",
      "  loss: 0.054933  [87680/318152]\n",
      "  loss: 0.054956  [88000/318152]\n",
      "  loss: 0.047670  [88320/318152]\n",
      "  loss: 0.058941  [88640/318152]\n",
      "  loss: 0.045971  [88960/318152]\n",
      "  loss: 0.060256  [89280/318152]\n",
      "  loss: 0.053405  [89600/318152]\n",
      "  loss: 0.054380  [89920/318152]\n",
      "  loss: 0.055339  [90240/318152]\n",
      "  loss: 0.059974  [90560/318152]\n",
      "  loss: 0.047472  [90880/318152]\n",
      "  loss: 0.045827  [91200/318152]\n",
      "  loss: 0.054730  [91520/318152]\n",
      "  loss: 0.047411  [91840/318152]\n",
      "  loss: 0.049358  [92160/318152]\n",
      "  loss: 0.046721  [92480/318152]\n",
      "  loss: 0.060120  [92800/318152]\n",
      "  loss: 0.046086  [93120/318152]\n",
      "  loss: 0.050540  [93440/318152]\n",
      "  loss: 0.052384  [93760/318152]\n",
      "  loss: 0.060757  [94080/318152]\n",
      "  loss: 0.050817  [94400/318152]\n",
      "  loss: 0.056647  [94720/318152]\n",
      "  loss: 0.057002  [95040/318152]\n",
      "  loss: 0.045563  [95360/318152]\n",
      "  loss: 0.053452  [95680/318152]\n",
      "  loss: 0.055556  [96000/318152]\n",
      "  loss: 0.051780  [96320/318152]\n",
      "  loss: 0.057612  [96640/318152]\n",
      "  loss: 0.056426  [96960/318152]\n",
      "  loss: 0.050417  [97280/318152]\n",
      "  loss: 0.052948  [97600/318152]\n",
      "  loss: 0.052202  [97920/318152]\n",
      "  loss: 0.051545  [98240/318152]\n",
      "  loss: 0.055115  [98560/318152]\n",
      "  loss: 0.048680  [98880/318152]\n",
      "  loss: 0.049161  [99200/318152]\n",
      "  loss: 0.054912  [99520/318152]\n",
      "  loss: 0.052608  [99840/318152]\n",
      "  loss: 0.052194  [100160/318152]\n",
      "  loss: 0.046825  [100480/318152]\n",
      "  loss: 0.050497  [100800/318152]\n",
      "  loss: 0.051201  [101120/318152]\n",
      "  loss: 0.050850  [101440/318152]\n",
      "  loss: 0.045598  [101760/318152]\n",
      "  loss: 0.051120  [102080/318152]\n",
      "  loss: 0.052573  [102400/318152]\n",
      "  loss: 0.043043  [102720/318152]\n",
      "  loss: 0.053431  [103040/318152]\n",
      "  loss: 0.049945  [103360/318152]\n",
      "  loss: 0.047200  [103680/318152]\n",
      "  loss: 0.053141  [104000/318152]\n",
      "  loss: 0.051181  [104320/318152]\n",
      "  loss: 0.053501  [104640/318152]\n",
      "  loss: 0.055764  [104960/318152]\n",
      "  loss: 0.052127  [105280/318152]\n",
      "  loss: 0.053701  [105600/318152]\n",
      "  loss: 0.055265  [105920/318152]\n",
      "  loss: 0.055526  [106240/318152]\n",
      "  loss: 0.053214  [106560/318152]\n",
      "  loss: 0.046390  [106880/318152]\n",
      "  loss: 0.054820  [107200/318152]\n",
      "  loss: 0.053666  [107520/318152]\n",
      "  loss: 0.050910  [107840/318152]\n",
      "  loss: 0.051493  [108160/318152]\n",
      "  loss: 0.040410  [108480/318152]\n",
      "  loss: 0.059763  [108800/318152]\n",
      "  loss: 0.053870  [109120/318152]\n",
      "  loss: 0.050674  [109440/318152]\n",
      "  loss: 0.048938  [109760/318152]\n",
      "  loss: 0.045963  [110080/318152]\n",
      "  loss: 0.049945  [110400/318152]\n",
      "  loss: 0.057216  [110720/318152]\n",
      "  loss: 0.045072  [111040/318152]\n",
      "  loss: 0.047586  [111360/318152]\n",
      "  loss: 0.046393  [111680/318152]\n",
      "  loss: 0.047700  [112000/318152]\n",
      "  loss: 0.045768  [112320/318152]\n",
      "  loss: 0.048701  [112640/318152]\n",
      "  loss: 0.060837  [112960/318152]\n",
      "  loss: 0.044719  [113280/318152]\n",
      "  loss: 0.048945  [113600/318152]\n",
      "  loss: 0.049293  [113920/318152]\n",
      "  loss: 0.051719  [114240/318152]\n",
      "  loss: 0.048254  [114560/318152]\n",
      "  loss: 0.044669  [114880/318152]\n",
      "  loss: 0.045562  [115200/318152]\n",
      "  loss: 0.050787  [115520/318152]\n",
      "  loss: 0.046043  [115840/318152]\n",
      "  loss: 0.055832  [116160/318152]\n",
      "  loss: 0.045854  [116480/318152]\n",
      "  loss: 0.047875  [116800/318152]\n",
      "  loss: 0.047027  [117120/318152]\n",
      "  loss: 0.053797  [117440/318152]\n",
      "  loss: 0.049486  [117760/318152]\n",
      "  loss: 0.046648  [118080/318152]\n",
      "  loss: 0.051614  [118400/318152]\n",
      "  loss: 0.048896  [118720/318152]\n",
      "  loss: 0.050876  [119040/318152]\n",
      "  loss: 0.055127  [119360/318152]\n",
      "  loss: 0.051707  [119680/318152]\n",
      "  loss: 0.053679  [120000/318152]\n",
      "  loss: 0.045460  [120320/318152]\n",
      "  loss: 0.051779  [120640/318152]\n",
      "  loss: 0.052354  [120960/318152]\n",
      "  loss: 0.051065  [121280/318152]\n",
      "  loss: 0.051155  [121600/318152]\n",
      "  loss: 0.045374  [121920/318152]\n",
      "  loss: 0.049906  [122240/318152]\n",
      "  loss: 0.050505  [122560/318152]\n",
      "  loss: 0.045066  [122880/318152]\n",
      "  loss: 0.053794  [123200/318152]\n",
      "  loss: 0.055994  [123520/318152]\n",
      "  loss: 0.048035  [123840/318152]\n",
      "  loss: 0.053725  [124160/318152]\n",
      "  loss: 0.049905  [124480/318152]\n",
      "  loss: 0.051842  [124800/318152]\n",
      "  loss: 0.042703  [125120/318152]\n",
      "  loss: 0.052211  [125440/318152]\n",
      "  loss: 0.049798  [125760/318152]\n",
      "  loss: 0.051150  [126080/318152]\n",
      "  loss: 0.049888  [126400/318152]\n",
      "  loss: 0.050088  [126720/318152]\n",
      "  loss: 0.047877  [127040/318152]\n",
      "  loss: 0.048588  [127360/318152]\n",
      "  loss: 0.054535  [127680/318152]\n",
      "  loss: 0.054865  [128000/318152]\n",
      "  loss: 0.054601  [128320/318152]\n",
      "  loss: 0.050474  [128640/318152]\n",
      "  loss: 0.049901  [128960/318152]\n",
      "  loss: 0.046415  [129280/318152]\n",
      "  loss: 0.045405  [129600/318152]\n",
      "  loss: 0.045787  [129920/318152]\n",
      "  loss: 0.051228  [130240/318152]\n",
      "  loss: 0.048078  [130560/318152]\n",
      "  loss: 0.044393  [130880/318152]\n",
      "  loss: 0.054625  [131200/318152]\n",
      "  loss: 0.049775  [131520/318152]\n",
      "  loss: 0.048697  [131840/318152]\n",
      "  loss: 0.055137  [132160/318152]\n",
      "  loss: 0.053627  [132480/318152]\n",
      "  loss: 0.049301  [132800/318152]\n",
      "  loss: 0.046474  [133120/318152]\n",
      "  loss: 0.055176  [133440/318152]\n",
      "  loss: 0.046094  [133760/318152]\n",
      "  loss: 0.046034  [134080/318152]\n",
      "  loss: 0.054290  [134400/318152]\n",
      "  loss: 0.051128  [134720/318152]\n",
      "  loss: 0.055085  [135040/318152]\n",
      "  loss: 0.045500  [135360/318152]\n",
      "  loss: 0.055333  [135680/318152]\n",
      "  loss: 0.050026  [136000/318152]\n",
      "  loss: 0.044921  [136320/318152]\n",
      "  loss: 0.049031  [136640/318152]\n",
      "  loss: 0.049580  [136960/318152]\n",
      "  loss: 0.049846  [137280/318152]\n",
      "  loss: 0.048852  [137600/318152]\n",
      "  loss: 0.048947  [137920/318152]\n",
      "  loss: 0.047634  [138240/318152]\n",
      "  loss: 0.057431  [138560/318152]\n",
      "  loss: 0.056489  [138880/318152]\n",
      "  loss: 0.049403  [139200/318152]\n",
      "  loss: 0.050135  [139520/318152]\n",
      "  loss: 0.045356  [139840/318152]\n",
      "  loss: 0.056398  [140160/318152]\n",
      "  loss: 0.050155  [140480/318152]\n",
      "  loss: 0.053494  [140800/318152]\n",
      "  loss: 0.048431  [141120/318152]\n",
      "  loss: 0.053263  [141440/318152]\n",
      "  loss: 0.047262  [141760/318152]\n",
      "  loss: 0.048413  [142080/318152]\n",
      "  loss: 0.050435  [142400/318152]\n",
      "  loss: 0.052778  [142720/318152]\n",
      "  loss: 0.047220  [143040/318152]\n",
      "  loss: 0.053552  [143360/318152]\n",
      "  loss: 0.048504  [143680/318152]\n",
      "  loss: 0.059112  [144000/318152]\n",
      "  loss: 0.053185  [144320/318152]\n",
      "  loss: 0.050078  [144640/318152]\n",
      "  loss: 0.047458  [144960/318152]\n",
      "  loss: 0.043513  [145280/318152]\n",
      "  loss: 0.042889  [145600/318152]\n",
      "  loss: 0.049336  [145920/318152]\n",
      "  loss: 0.057135  [146240/318152]\n",
      "  loss: 0.057421  [146560/318152]\n",
      "  loss: 0.045403  [146880/318152]\n",
      "  loss: 0.045885  [147200/318152]\n",
      "  loss: 0.047629  [147520/318152]\n",
      "  loss: 0.055590  [147840/318152]\n",
      "  loss: 0.054360  [148160/318152]\n",
      "  loss: 0.045468  [148480/318152]\n",
      "  loss: 0.049973  [148800/318152]\n",
      "  loss: 0.057055  [149120/318152]\n",
      "  loss: 0.044962  [149440/318152]\n",
      "  loss: 0.053762  [149760/318152]\n",
      "  loss: 0.051587  [150080/318152]\n",
      "  loss: 0.047611  [150400/318152]\n",
      "  loss: 0.049312  [150720/318152]\n",
      "  loss: 0.051737  [151040/318152]\n",
      "  loss: 0.047146  [151360/318152]\n",
      "  loss: 0.053594  [151680/318152]\n",
      "  loss: 0.045672  [152000/318152]\n",
      "  loss: 0.050481  [152320/318152]\n",
      "  loss: 0.050258  [152640/318152]\n",
      "  loss: 0.049614  [152960/318152]\n",
      "  loss: 0.047128  [153280/318152]\n",
      "  loss: 0.047351  [153600/318152]\n",
      "  loss: 0.052764  [153920/318152]\n",
      "  loss: 0.043641  [154240/318152]\n",
      "  loss: 0.054344  [154560/318152]\n",
      "  loss: 0.048579  [154880/318152]\n",
      "  loss: 0.047230  [155200/318152]\n",
      "  loss: 0.056961  [155520/318152]\n",
      "  loss: 0.050014  [155840/318152]\n",
      "  loss: 0.045865  [156160/318152]\n",
      "  loss: 0.050677  [156480/318152]\n",
      "  loss: 0.044457  [156800/318152]\n",
      "  loss: 0.047452  [157120/318152]\n",
      "  loss: 0.052750  [157440/318152]\n",
      "  loss: 0.051582  [157760/318152]\n",
      "  loss: 0.050550  [158080/318152]\n",
      "  loss: 0.047659  [158400/318152]\n",
      "  loss: 0.044736  [158720/318152]\n",
      "  loss: 0.046034  [159040/318152]\n",
      "  loss: 0.050205  [159360/318152]\n",
      "  loss: 0.050228  [159680/318152]\n",
      "  loss: 0.051522  [160000/318152]\n",
      "  loss: 0.047788  [160320/318152]\n",
      "  loss: 0.055939  [160640/318152]\n",
      "  loss: 0.052965  [160960/318152]\n",
      "  loss: 0.050997  [161280/318152]\n",
      "  loss: 0.053770  [161600/318152]\n",
      "  loss: 0.046768  [161920/318152]\n",
      "  loss: 0.049664  [162240/318152]\n",
      "  loss: 0.048550  [162560/318152]\n",
      "  loss: 0.048503  [162880/318152]\n",
      "  loss: 0.053703  [163200/318152]\n",
      "  loss: 0.047657  [163520/318152]\n",
      "  loss: 0.050119  [163840/318152]\n",
      "  loss: 0.053515  [164160/318152]\n",
      "  loss: 0.050487  [164480/318152]\n",
      "  loss: 0.056239  [164800/318152]\n",
      "  loss: 0.046598  [165120/318152]\n",
      "  loss: 0.043482  [165440/318152]\n",
      "  loss: 0.040636  [165760/318152]\n",
      "  loss: 0.048176  [166080/318152]\n",
      "  loss: 0.045411  [166400/318152]\n",
      "  loss: 0.062177  [166720/318152]\n",
      "  loss: 0.053612  [167040/318152]\n",
      "  loss: 0.049525  [167360/318152]\n",
      "  loss: 0.043616  [167680/318152]\n",
      "  loss: 0.049624  [168000/318152]\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Progress tracking\n",
    "        if batch_id % 5 == 0:\n",
    "            current = batch_id * len(batch[\"X\"])\n",
    "            print(f\"  loss: {loss.item():>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss, model, epoch)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save final model and history\n",
    "save_final_model(model, final_model_path)\n",
    "save_loss_history(history, history_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Loss history saved to: {history_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49106c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder\n",
    "output_folder = \"single_examples\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(f\"✓ [Peak Detection]: sampling_rate={sampling_rate} Hz, height={height}, distance={distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bda353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction 10 times\n",
    "for i in range(1, 11):\n",
    "    # Visualizing Predictions\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    waveform = sample[\"X\"]  # Shape: (3, N)\n",
    "    labels = sample[\"y\"]  # Shape: (3, N)\n",
    "\n",
    "    time_axis = np.arange(waveform.shape[1])  # Create a time axis\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axs = fig.subplots(5, 1, sharex=True, gridspec_kw={\"hspace\": 0.3})\n",
    "\n",
    "    # Color setup\n",
    "    channel_names = [\"Channel E\", \"Channel N\", \"Channel Z\"]\n",
    "    waveform_colors = ['#a3b18a', '#588157', '#344e41']  # Custom colors for channels\n",
    "    label_colors = ['#15616d', '#ff7d00']\n",
    "\n",
    "    # Plot waveforms\n",
    "    for j in range(3):\n",
    "        axs[j].plot(time_axis, waveform[j], color=waveform_colors[j], linewidth=1.5)\n",
    "        axs[j].set_title(f\"{channel_names[j]} - Seismic Waveform\", fontsize=12, fontweight='bold')\n",
    "        axs[j].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "        axs[j].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    #axs[0].plot(sample[\"X\"].T)\n",
    "    #axs[1].plot(sample[\"y\"].T)\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    axs[3].plot(time_axis, labels[0], color=label_colors[0], linewidth=1.5, label=\"P-phase\")\n",
    "    axs[3].plot(time_axis, labels[1], color=label_colors[1], linewidth=1.5, label=\"S-phase\")\n",
    "    axs[3].plot(y_p_peaks, sample[\"y\"][0, y_p_peaks], 'o', label='P arrivals', color='red')\n",
    "    axs[3].plot(y_s_peaks, sample[\"y\"][1, y_s_peaks], 'o', label='S arrivals', color='blue')\n",
    "    axs[3].set_title(\"Dataset Ground Truth\", fontsize=12, fontweight='bold')\n",
    "    axs[3].set_ylim(0,1.1)\n",
    "    axs[3].set_ylabel(\"Probability\", fontsize=10)\n",
    "    axs[3].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[3].legend(fontsize=10, loc=\"upper left\")\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals\n",
    "    residual_p_arrival_times = p_arrival_times - y_p_arrival_times[:, np.newaxis]\n",
    "    residual_s_arrival_times = s_arrival_times - y_s_arrival_times[:, np.newaxis]\n",
    "\n",
    "    # Plot the probability distributions and the detected peaks\n",
    "    axs[4].plot(p_prob, color=label_colors[0], linewidth=1.5,label='P-phase')\n",
    "    axs[4].plot(p_peaks, p_prob[p_peaks], 'x', label='Detected P Arrival', color='red')\n",
    "    axs[4].plot(s_prob, color=label_colors[1], linewidth=1.5,label='S-phase')\n",
    "    axs[4].plot(s_peaks, s_prob[s_peaks], 'x', label='Detected S Arrival', color='blue')\n",
    "    axs[4].set_title('Model Prediction', fontsize=12, fontweight='bold')\n",
    "    axs[4].set_ylim(0,1.1)\n",
    "    axs[4].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[4].set_ylabel('Probability', fontsize=10)\n",
    "    axs[4].legend(fontsize=10, loc=\"upper left\")\n",
    "\n",
    "    # Improve x-axis visibility\n",
    "    axs[4].set_xlabel(\"Time (samples)\", fontsize=11, fontweight='bold')\n",
    "    axs[4].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plot_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Plot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save the results to a text file\n",
    "    results_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Results.txt\")\n",
    "    with open(results_filename, \"w\") as f:\n",
    "        f.write(f\"Ground Truth P arrival times: {y_p_arrival_times}\\n\")\n",
    "        f.write(f\"Ground Truth S arrival times: {y_s_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted P arrival times: {p_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted S arrival times: {s_arrival_times}\\n\")\n",
    "        f.write(f\"Residual P arrival times: {residual_p_arrival_times}\\n\")\n",
    "        f.write(f\"Residual S arrival times: {residual_s_arrival_times}\\n\")\n",
    "\n",
    "    # Save the parameters to a text file\n",
    "    parameters_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Parameters.txt\")\n",
    "    with open(parameters_filename, \"w\") as f:\n",
    "        f.write(f\"Data Sampling Rate: {sampling_rate}\\n\")\n",
    "        f.write(f\"Detection Height Parameter: {height}\\n\")\n",
    "        f.write(f\"Detection Distance Parameter: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b353cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction 10 times\n",
    "for i in range(1, 11):\n",
    "    # Visualizing Predictions\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    waveform = sample[\"X\"]  # Shape: (3, N)\n",
    "    labels = sample[\"y\"]  # Shape: (3, N)\n",
    "\n",
    "    time_axis = np.arange(waveform.shape[1])  # Create a time axis\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axs = fig.subplots(5, 1, sharex=True, gridspec_kw={\"hspace\": 0.3})\n",
    "\n",
    "    # Color setup\n",
    "    channel_names = [\"Channel E\", \"Channel N\", \"Channel Z\"]\n",
    "    waveform_colors = ['#a3b18a', '#588157', '#344e41']  # Custom colors for channels\n",
    "    label_colors = ['#15616d', '#ff7d00']\n",
    "\n",
    "    # Plot waveforms\n",
    "    for j in range(3):\n",
    "        axs[j].plot(time_axis, waveform[j], color=waveform_colors[j], linewidth=1.5)\n",
    "        axs[j].set_title(f\"{channel_names[j]} - Seismic Waveform\", fontsize=12, fontweight='bold')\n",
    "        axs[j].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "        axs[j].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    #axs[0].plot(sample[\"X\"].T)\n",
    "    #axs[1].plot(sample[\"y\"].T)\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    axs[3].plot(time_axis, labels[0], color=label_colors[0], linewidth=1.5, label=\"P-phase\")\n",
    "    axs[3].plot(time_axis, labels[1], color=label_colors[1], linewidth=1.5, label=\"S-phase\")\n",
    "    axs[3].plot(y_p_peaks, sample[\"y\"][0, y_p_peaks], 'o', label='P arrivals', color='red')\n",
    "    axs[3].plot(y_s_peaks, sample[\"y\"][1, y_s_peaks], 'o', label='S arrivals', color='blue')\n",
    "    axs[3].set_title(\"Dataset Ground Truth\", fontsize=12, fontweight='bold')\n",
    "    axs[3].set_ylim(0,1.1)\n",
    "    axs[3].set_ylabel(\"Probability\", fontsize=10)\n",
    "    axs[3].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[3].legend(fontsize=10, loc=\"upper left\")\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals\n",
    "    residual_p_arrival_times = p_arrival_times - y_p_arrival_times[:, np.newaxis]\n",
    "    residual_s_arrival_times = s_arrival_times - y_s_arrival_times[:, np.newaxis]\n",
    "\n",
    "    # Plot the probability distributions and the detected peaks\n",
    "    axs[4].plot(p_prob, color=label_colors[0], linewidth=1.5,label='P-phase')\n",
    "    axs[4].plot(p_peaks, p_prob[p_peaks], 'x', label='Detected P Arrival', color='red')\n",
    "    axs[4].plot(s_prob, color=label_colors[1], linewidth=1.5,label='S-phase')\n",
    "    axs[4].plot(s_peaks, s_prob[s_peaks], 'x', label='Detected S Arrival', color='blue')\n",
    "    axs[4].set_title('Model Prediction', fontsize=12, fontweight='bold')\n",
    "    axs[4].set_ylim(0,1.1)\n",
    "    axs[4].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[4].set_ylabel('Probability', fontsize=10)\n",
    "    axs[4].legend(fontsize=10, loc=\"upper left\")\n",
    "\n",
    "    # Improve x-axis visibility\n",
    "    axs[4].set_xlabel(\"Time (samples)\", fontsize=11, fontweight='bold')\n",
    "    axs[4].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plot_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Plot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save the results to a text file\n",
    "    results_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Results.txt\")\n",
    "    with open(results_filename, \"w\") as f:\n",
    "        f.write(f\"Ground Truth P arrival times: {y_p_arrival_times}\\n\")\n",
    "        f.write(f\"Ground Truth S arrival times: {y_s_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted P arrival times: {p_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted S arrival times: {s_arrival_times}\\n\")\n",
    "        f.write(f\"Residual P arrival times: {residual_p_arrival_times}\\n\")\n",
    "        f.write(f\"Residual S arrival times: {residual_s_arrival_times}\\n\")\n",
    "\n",
    "    # Save the parameters to a text file\n",
    "    parameters_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Parameters.txt\")\n",
    "    with open(parameters_filename, \"w\") as f:\n",
    "        f.write(f\"Data Sampling Rate: {sampling_rate}\\n\")\n",
    "        f.write(f\"Detection Height Parameter: {height}\\n\")\n",
    "        f.write(f\"Detection Distance Parameter: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples = len(test_generator)\n",
    "\n",
    "all_residual_p_arrival_times = []\n",
    "all_residual_s_arrival_times = []\n",
    "\n",
    "# Initialize counters for ground truth P and S labels\n",
    "groundtruth_p_peaks = 0\n",
    "groundtruth_s_peaks = 0\n",
    "\n",
    "# Initialize counters for residuals smaller than 0.6 (absolute value)\n",
    "count_residuals_p_under_0_6 = 0\n",
    "count_residuals_s_under_0_6 = 0\n",
    "\n",
    "# Only Commenting this out to reflect the randome samples to squential samples \n",
    "\n",
    "for i in range(n_samples):\n",
    "#for i in range(len(test_generator)):\n",
    "    \n",
    "    # Hongyu Xiao: randome sample works effectively\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    #sample = test_generator[i]\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Update the counters\n",
    "    groundtruth_p_peaks += len(y_p_peaks)\n",
    "    groundtruth_s_peaks += len(y_s_peaks)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    sampling_rate = 100  # Samples per second (100 Hz)\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals for P and S peaks, keeping only the smallest one by absolute value\n",
    "    for y_p_time in y_p_arrival_times:\n",
    "        residual_p_arrival_times = p_arrival_times - y_p_time\n",
    "        if len(residual_p_arrival_times) > 0:\n",
    "            min_residual_p = residual_p_arrival_times[np.argmin(np.abs(residual_p_arrival_times))]\n",
    "            all_residual_p_arrival_times.append(min_residual_p)\n",
    "            if np.abs(min_residual_p) < 0.6:\n",
    "                count_residuals_p_under_0_6 += 1\n",
    "        \n",
    "    for y_s_time in y_s_arrival_times:\n",
    "        residual_s_arrival_times = s_arrival_times - y_s_time\n",
    "        if len(residual_s_arrival_times) > 0:\n",
    "            min_residual_s = residual_s_arrival_times[np.argmin(np.abs(residual_s_arrival_times))]\n",
    "            all_residual_s_arrival_times.append(min_residual_s)\n",
    "            if np.abs(min_residual_s) < 0.6:\n",
    "                count_residuals_s_under_0_6 += 1\n",
    "\n",
    "# Display the total counts of ground truth P and S peaks\n",
    "print(f\"Total ground truth P peaks: {groundtruth_p_peaks}\")\n",
    "print(f\"Total ground truth S peaks: {groundtruth_s_peaks}\")\n",
    "\n",
    "# Display the counts of residuals under 0.6 seconds\n",
    "print(f\"Total P-phase residuals under 0.6s: {count_residuals_p_under_0_6}\")\n",
    "print(f\"Total S-phase residuals under 0.6s: {count_residuals_s_under_0_6}\")\n",
    "\n",
    "# Plot the histogram of residual P peak arrival times\n",
    "plt.figure(figsize=(10, 5))\n",
    "counts_p, bins_p, patches_p = plt.hist(all_residual_p_arrival_times, bins=30, color='skyblue', edgecolor='black', range=(-1, 1))\n",
    "\n",
    "# Add labels for the number counts on each column\n",
    "for count, bin_, patch in zip(counts_p, bins_p, patches_p):\n",
    "    plt.text(bin_ + (bins_p[1] - bins_p[0]) / 2, count, f'{int(count)}', ha='center', va='bottom')\n",
    "\n",
    "# Print total pick count, mean, and standard deviation\n",
    "total_picks_p = len(all_residual_p_arrival_times)\n",
    "mean_p = np.mean(all_residual_p_arrival_times)\n",
    "std_p = np.std(all_residual_p_arrival_times)\n",
    "plt.text(0.95, 0.95, f'Total Picks: {total_picks_p}', ha='right', va='top', transform=plt.gca().transAxes)\n",
    "print(f\"P-phase Residuals: Mean = {mean_p:.4f}, Std = {std_p:.4f}\")\n",
    "print(f\"Total detected P picks: {total_picks_p}\")\n",
    "\n",
    "plt.title('Histogram of Residual P Peak Arrival Times')\n",
    "plt.xlabel('Residual P Arrival Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram.png\")\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot the histogram of residual S peak arrival times\n",
    "plt.figure(figsize=(10, 5))\n",
    "counts_s, bins_s, patches_s = plt.hist(all_residual_s_arrival_times, bins=30, color='salmon', edgecolor='grey', range=(-1, 1))\n",
    "\n",
    "# Add labels for the number counts on each column\n",
    "for count, bin_, patch in zip(counts_s, bins_s, patches_s):\n",
    "    plt.text(bin_ + (bins_s[1] - bins_s[0]) / 2, count, f'{int(count)}', ha='center', va='bottom')\n",
    "\n",
    "# Print total pick count, mean, and standard deviation\n",
    "total_picks_s = len(all_residual_s_arrival_times)\n",
    "mean_s = np.mean(all_residual_s_arrival_times)\n",
    "std_s = np.std(all_residual_s_arrival_times)\n",
    "plt.text(0.95, 0.95, f'Total Picks: {total_picks_s}', ha='right', va='top', transform=plt.gca().transAxes)\n",
    "print(f\"S-phase Residuals: Mean = {mean_s:.4f}, Std = {std_s:.4f}\")\n",
    "print(f\"Total detected S picks: {total_picks_s}\")\n",
    "\n",
    "plt.title('Histogram of Residual S Peak Arrival Times')\n",
    "plt.xlabel('Residual S Arrival Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram.png\")\n",
    "#plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294eefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "# Parameters\n",
    "x_min, x_max = -1, 1\n",
    "bins = np.linspace(x_min, x_max, 31)  # 30 bins between -1 and 1\n",
    "\n",
    "# Custom colors\n",
    "p_color = '#15616d'\n",
    "s_color = '#ff7d00'\n",
    "shading_color = '#d3d3d3'  # light gray for shaded success zone\n",
    "\n",
    "# === P-phase residuals ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_residual_p_arrival_times, bins=bins, kde=False, color=p_color, edgecolor='black', stat='count')\n",
    "\n",
    "# Shaded ±0.6s zone\n",
    "plt.axvspan(-0.6, 0.6, color=shading_color, alpha=0.3, label='Residual < 0.6s')\n",
    "\n",
    "# Reference lines\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(-0.6, color='gray', linestyle=':', linewidth=1)\n",
    "plt.axvline(0.6, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# Stats and annotation\n",
    "mean_p = np.mean(all_residual_p_arrival_times)\n",
    "std_p = np.std(all_residual_p_arrival_times)\n",
    "total_picks_p = len(all_residual_p_arrival_times)\n",
    "fraction_p = count_residuals_p_under_0_6 / groundtruth_p_peaks\n",
    "\n",
    "plt.text(x_min + 0.02, plt.gca().get_ylim()[1]*0.95,\n",
    "         f'Total Picks: {total_picks_p}\\nMean: {mean_p:.3f}s\\nStd: {std_p:.3f}s\\nUnder 0.6s: {count_residuals_p_under_0_6}/{groundtruth_p_peaks} ({fraction_p:.1%})',\n",
    "         ha='left', va='top', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\"))\n",
    "\n",
    "plt.title('Residual P Peak Arrival Times', fontsize=16)\n",
    "plt.xlabel('Residual P Arrival Time (s)', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram_shaded.png\")\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram_shaded.ps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === S-phase residuals ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_residual_s_arrival_times, bins=bins, kde=False, color=s_color, edgecolor='black', stat='count')\n",
    "\n",
    "# Shaded ±0.6s zone\n",
    "plt.axvspan(-0.6, 0.6, color=shading_color, alpha=0.3, label='Residual < 0.6s')\n",
    "\n",
    "# Reference lines\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(-0.6, color='gray', linestyle=':', linewidth=1)\n",
    "plt.axvline(0.6, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# Stats and annotation\n",
    "mean_s = np.mean(all_residual_s_arrival_times)\n",
    "std_s = np.std(all_residual_s_arrival_times)\n",
    "total_picks_s = len(all_residual_s_arrival_times)\n",
    "fraction_s = count_residuals_s_under_0_6 / groundtruth_s_peaks\n",
    "\n",
    "plt.text(x_min + 0.02, plt.gca().get_ylim()[1]*0.95,\n",
    "         f'Total Picks: {total_picks_s}\\nMean: {mean_s:.3f}s\\nStd: {std_s:.3f}s\\nUnder 0.6s: {count_residuals_s_under_0_6}/{groundtruth_s_peaks} ({fraction_s:.1%})',\n",
    "         ha='left', va='top', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\"))\n",
    "\n",
    "plt.title('Residual S Peak Arrival Times', fontsize=16)\n",
    "plt.xlabel('Residual S Arrival Time (s)', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram_shaded.png\")\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram_shaded.ps\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
