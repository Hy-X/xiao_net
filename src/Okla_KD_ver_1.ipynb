{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a73e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Distillation Training for XiaoNet\n",
    "Teacher: PhaseNet (from STEAD)\n",
    "Student: XiaoNet (v2, v3, v4, or v5)\n",
    "Dataset: OKLA regional seismic data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seismology & SeisBench\n",
    "import obspy\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing local modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# XiaoNet modules\n",
    "from models.xn_xiao_net_v2 import XiaoNet as XiaoNetV2\n",
    "from models.xn_xiao_net_v3 import XiaoNet as XiaoNetV3\n",
    "from models.xn_xiao_net_v4 import XiaoNetFast as XiaoNetV4\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge as XiaoNetV5\n",
    "from loss.xn_distillation_loss import DistillationLoss\n",
    "from xn_utils import set_seed, setup_device\n",
    "\n",
    "\n",
    "# Early stopping class definition\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping class to stop training when validation loss stops improving.\n",
    "    \n",
    "    Args:\n",
    "        patience: Number of epochs to wait before stopping\n",
    "        min_delta: Minimum change to qualify as an improvement\n",
    "        checkpoint_dir: Directory to save model checkpoints\n",
    "        verbose: Whether to print early stopping messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.0, checkpoint_dir='checkpoints/', verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.checkpoint_path = self.checkpoint_dir / 'best_model.pth'\n",
    "    \n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        \"\"\"\n",
    "        Check if training should stop early.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "            model: Model to save if improvement is found\n",
    "            epoch: Current epoch number\n",
    "        \"\"\"\n",
    "        score = -val_loss  # Negative because lower is better\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, epoch)\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, epoch)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'best_score': self.best_score\n",
    "        }, self.checkpoint_path)\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss improved. Saving model to {self.checkpoint_path}')\n",
    "\n",
    "\n",
    "print(\"✓ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c646aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = setup_device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22312606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /Users/hongyuxiao/Hongyu_File/xiao_net/config.json\n",
      "{\n",
      "  \"peak_detection\": {\n",
      "    \"sampling_rate\": 100,\n",
      "    \"height\": 0.5,\n",
      "    \"distance\": 100\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"dataset_name\": \"OKLA_1Mil_120s_Ver_3\",\n",
      "    \"sampling_rate\": 100,\n",
      "    \"window_len\": 3001,\n",
      "    \"samples_before\": 3000,\n",
      "    \"windowlen_large\": 6000,\n",
      "    \"sample_fraction\": 0.1\n",
      "  },\n",
      "  \"data_filter\": {\n",
      "    \"min_magnitude\": 1.0,\n",
      "    \"max_magnitude\": 2.0\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 4,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"epochs\": 50,\n",
      "    \"patience\": 5,\n",
      "    \"loss_weights\": [\n",
      "      0.01,\n",
      "      0.4,\n",
      "      0.59\n",
      "    ],\n",
      "    \"optimization\": {\n",
      "      \"mixed_precision\": true,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"pin_memory\": true,\n",
      "      \"prefetch_factor\": 2,\n",
      "      \"persistent_workers\": true\n",
      "    }\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"use_cuda\": true,\n",
      "    \"device_id\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "config_path = Path.cwd().parent / \"config.json\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration from: {config_path}\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a55f45a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available PhaseNet pretrained models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['diting',\n",
       " 'ethz',\n",
       " 'geofon',\n",
       " 'instance',\n",
       " 'iquique',\n",
       " 'jma',\n",
       " 'jma_wc',\n",
       " 'lendb',\n",
       " 'neic',\n",
       " 'obs',\n",
       " 'original',\n",
       " 'phasenet_sn',\n",
       " 'pisdl',\n",
       " 'scedc',\n",
       " 'stead',\n",
       " 'volpick']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PhaseNet teacher model (pretrained on STEAD)\n",
    "print(\"Available PhaseNet pretrained models:\")\n",
    "sbm.PhaseNet.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PhaseNet teacher model...\n",
      "\n",
      "✓ PhaseNet teacher loaded successfully!\n",
      "Total parameters: 268,443\n",
      "Trainable parameters: 268,443\n",
      "Model on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading PhaseNet teacher model...\")\n",
    "model = sbm.PhaseNet.from_pretrained(\"stead\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode for teacher\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ PhaseNet teacher loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "887fb320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OKLA regional seismic dataset...\n",
      "Sampling 10.0% of data for faster training...\n",
      "Sampled dataset size: 113,884\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "Training samples: 79,670\n",
      "Validation samples: 17,090\n",
      "Test samples: 17,124\n",
      "Total samples: 113,884\n"
     ]
    }
   ],
   "source": [
    "# Load OKLA dataset\n",
    "print(\"Loading OKLA regional seismic dataset...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n",
    "# Optional: Use subset for faster experimentation\n",
    "sample_fraction = config.get('data', {}).get('sample_fraction', 0.1)\n",
    "if sample_fraction < 1.0:\n",
    "    print(f\"Sampling {sample_fraction*100}% of data for faster training...\")\n",
    "    # Create a random mask for sampling\n",
    "    mask = np.random.random(len(data)) < sample_fraction\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"Sampled dataset size: {len(data):,}\")\n",
    "\n",
    "# Split into train/dev/test\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train):,}\")\n",
    "print(f\"Validation samples: {len(dev):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Total samples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa20617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying magnitude filters: 1.0 < M < 2.0\n",
      "✓ [Data Filter]: Start - magnitude > 1.0\n",
      "✓ [Data Filter]: Applied - magnitude > 1.0, remaining samples: 108,944\n",
      "✓ [Data Filter]: Start - magnitude < 2.0\n",
      "✓ [Data Filter]: Applied - magnitude < 2.0, remaining samples: 36,880\n",
      "\n",
      "✓ Magnitude filtering complete: 36,880 traces in range [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# Magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', 1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 2.0)\n",
    "\n",
    "print(f\"Applying magnitude filters: {min_magnitude} < M < {max_magnitude}\")\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude > {min_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude < {max_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Magnitude filtering complete: {len(data):,} traces in range [{min_magnitude}, {max_magnitude}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78d3b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total dataset size: 36,880\n",
      "Train size: 79,670\n",
      "Validation size: 17,090\n",
      "Test size: 17,124\n",
      "Sampling rate: 100 Hz\n",
      "Window length: 3001 samples\n",
      "Magnitude stats: min=1.00, max=2.00, mean=1.53\n",
      "Metadata columns: ['index', 'station_network_code', 'station_code', 'trace_channel', 'station_latitude_deg', 'station_longitude_deg', 'station_elevation_m', 'trace_p_arrival_sample', 'trace_p_status', 'trace_p_weight', 'path_p_travel_sec', 'trace_s_arrival_sample', 'trace_s_status', 'trace_s_weight', 'source_id', 'source_origin_time', 'source_origin_uncertainty_sec', 'source_latitude_deg', 'source_longitude_deg', 'source_error_sec', 'source_gap_deg', 'source_horizontal_uncertainty_km', 'source_depth_km', 'source_depth_uncertainty_km', 'source_magnitude', 'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake', 'source_distance_deg', 'source_distance_km', 'path_back_azimuth_deg', 'trace_snr_db', 'trace_coda_end_sample', 'trace_start_time', 'trace_category', 'trace_name', 'split', 'trace_name_original', 'trace_chunk', 'trace_sampling_rate_hz', 'trace_component_order']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary for training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core sizes\n",
    "print(f\"Total dataset size: {len(data):,}\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Sampling configuration\n",
    "sampling_rate = config.get('data', {}).get('sampling_rate', 'unknown')\n",
    "window_len = config.get('data', {}).get('window_len', 'unknown')\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Window length: {window_len} samples\")\n",
    "\n",
    "# Metadata summary (if available)\n",
    "if hasattr(data, 'metadata') and data.metadata is not None:\n",
    "    if 'source_magnitude' in data.metadata:\n",
    "        mags = data.metadata['source_magnitude']\n",
    "        print(f\"Magnitude stats: min={mags.min():.2f}, max={mags.max():.2f}, mean={mags.mean():.2f}\")\n",
    "    print(f\"Metadata columns: {list(data.metadata.columns)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28ab888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset split after filtering\n",
      "Train size: 25,901\n",
      "Validation size: 5,518\n",
      "Test size: 5,461\n",
      "Split ratios: train=70.23%, dev=14.96%, test=14.81%\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/dev/test after filtering\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(\"\\n✓ Dataset split after filtering\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Split ratios\n",
    "n_total = len(train) + len(dev) + len(test)\n",
    "if n_total > 0:\n",
    "    print(f\"Split ratios: train={len(train)/n_total:.2%}, dev={len(dev)/n_total:.2%}, test={len(test)/n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2c2672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET OBJECTS\n",
      "============================================================\n",
      "Train dataset: OKLA_1Mil_120s_Ver_3 - 25901 traces\n",
      "Dev dataset:   OKLA_1Mil_120s_Ver_3 - 5518 traces\n",
      "Test dataset:  OKLA_1Mil_120s_Ver_3 - 5461 traces\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset objects (compact summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET OBJECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train dataset: {train}\")\n",
    "print(f\"Dev dataset:   {dev}\")\n",
    "print(f\"Test dataset:  {test}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81187cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7b64fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7c83d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dddf3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PEAK DETECTION SETTINGS\n",
      "============================================================\n",
      "Sampling rate: 100 Hz\n",
      "Height threshold: 0.5\n",
      "Minimum peak distance: 100 samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PEAK DETECTION SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Height threshold: {height}\")\n",
    "print(f\"Minimum peak distance: {distance} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb0b4ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ [DataLoader]: batch_size=64, num_workers=4\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "print(f\"✓ [DataLoader]: batch_size={batch_size}, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "570e9c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "[Dataset]\n",
      "  Total samples:      36,880\n",
      "  Train/Validation/Test:     25,901 / 5,518 / 5,461\n",
      "  Sample fraction:    10.0%\n",
      "\n",
      "[Device]\n",
      "  Device:             cpu\n",
      "\n",
      "[Training]\n",
      "  Batch size:         64\n",
      "  Num workers:        4\n",
      "  Learning rate:      0.01\n",
      "  Epochs:             50\n",
      "  Patience:           5\n",
      "\n",
      "[Peak Detection]\n",
      "  Sampling rate:      100 Hz\n",
      "  Height threshold:   0.5\n",
      "  Min peak distance:  100 samples\n",
      "============================================================\n",
      "Ready to start training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset info\n",
    "print(\"[Dataset]\")\n",
    "print(f\"  Total samples:      {len(data):,}\")\n",
    "print(f\"  Train/Validation/Test:     {len(train):,} / {len(dev):,} / {len(test):,}\")\n",
    "print(f\"  Sample fraction:    {sample_fraction*100:.1f}%\")\n",
    "\n",
    "# Device\n",
    "print(\"\\n[Device]\")\n",
    "print(f\"  Device:             {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "print(\"\\n[Training]\")\n",
    "print(f\"  Batch size:         {batch_size}\")\n",
    "print(f\"  Num workers:        {num_workers}\")\n",
    "print(f\"  Learning rate:      {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs:             {config['training']['epochs']}\")\n",
    "print(f\"  Patience:           {config['training']['patience']}\")\n",
    "\n",
    "# Peak detection\n",
    "print(\"\\n[Peak Detection]\")\n",
    "print(f\"  Sampling rate:      {sampling_rate} Hz\")\n",
    "print(f\"  Height threshold:   {height}\")\n",
    "print(f\"  Min peak distance:  {distance} samples\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to start training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "830aeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c530a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce428776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZER SETTINGS\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZER SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61328216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EARLY STOPPING & CHECKPOINTS\n",
      "============================================================\n",
      "Checkpoint dir: /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints\n",
      "Best model:     /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "Final model:    /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/final_model.pth\n",
      "History file:   /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/loss_history.json\n",
      "Patience:       5\n",
      "Min delta:      0.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and checkpoint setup\n",
    "checkpoint_dir = Path.cwd().parent / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "final_model_path = checkpoint_dir / \"final_model.pth\"\n",
    "history_path = checkpoint_dir / \"loss_history.json\"\n",
    "\n",
    "patience = config.get('training', {}).get('patience', 5)\n",
    "min_delta = config.get('training', {}).get('min_delta', 0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Loss history container\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "\n",
    "# Helper functions for saving\n",
    "def save_loss_history(history_dict, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(history_dict, f, indent=2)\n",
    "    print(f\"✓ Loss history saved to {path}\")\n",
    "\n",
    "\n",
    "def save_final_model(model, path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, path)\n",
    "    print(f\"✓ Final model saved to {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLY STOPPING & CHECKPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
    "print(f\"Best model:     {best_model_path}\")\n",
    "print(f\"Final model:    {final_model_path}\")\n",
    "print(f\"History file:   {history_path}\")\n",
    "print(f\"Patience:       {patience}\")\n",
    "print(f\"Min delta:      {min_delta}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6327c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "  loss: 0.311706  [    0/25901]\n",
      "  loss: 0.211790  [  320/25901]\n",
      "  loss: 0.186872  [  640/25901]\n",
      "  loss: 0.142735  [  960/25901]\n",
      "  loss: 0.127199  [ 1280/25901]\n",
      "  loss: 0.135213  [ 1600/25901]\n",
      "  loss: 0.115860  [ 1920/25901]\n",
      "  loss: 0.118073  [ 2240/25901]\n",
      "  loss: 0.094561  [ 2560/25901]\n",
      "  loss: 0.092510  [ 2880/25901]\n",
      "  loss: 0.096483  [ 3200/25901]\n",
      "  loss: 0.090208  [ 3520/25901]\n",
      "  loss: 0.085121  [ 3840/25901]\n",
      "  loss: 0.093303  [ 4160/25901]\n",
      "  loss: 0.094025  [ 4480/25901]\n",
      "  loss: 0.095065  [ 4800/25901]\n",
      "  loss: 0.101441  [ 5120/25901]\n",
      "  loss: 0.092257  [ 5440/25901]\n",
      "  loss: 0.081163  [ 5760/25901]\n",
      "  loss: 0.083620  [ 6080/25901]\n",
      "  loss: 0.075914  [ 6400/25901]\n",
      "  loss: 0.095000  [ 6720/25901]\n",
      "  loss: 0.089997  [ 7040/25901]\n",
      "  loss: 0.072806  [ 7360/25901]\n",
      "  loss: 0.080931  [ 7680/25901]\n",
      "  loss: 0.087355  [ 8000/25901]\n",
      "  loss: 0.083313  [ 8320/25901]\n",
      "  loss: 0.087002  [ 8640/25901]\n",
      "  loss: 0.075286  [ 8960/25901]\n",
      "  loss: 0.083303  [ 9280/25901]\n",
      "  loss: 0.094178  [ 9600/25901]\n",
      "  loss: 0.086240  [ 9920/25901]\n",
      "  loss: 0.087397  [10240/25901]\n",
      "  loss: 0.074548  [10560/25901]\n",
      "  loss: 0.077025  [10880/25901]\n",
      "  loss: 0.088333  [11200/25901]\n",
      "  loss: 0.076281  [11520/25901]\n",
      "  loss: 0.084164  [11840/25901]\n",
      "  loss: 0.073964  [12160/25901]\n",
      "  loss: 0.075878  [12480/25901]\n",
      "  loss: 0.084708  [12800/25901]\n",
      "  loss: 0.076529  [13120/25901]\n",
      "  loss: 0.076080  [13440/25901]\n",
      "  loss: 0.080374  [13760/25901]\n",
      "  loss: 0.079121  [14080/25901]\n",
      "  loss: 0.084412  [14400/25901]\n",
      "  loss: 0.080582  [14720/25901]\n",
      "  loss: 0.073950  [15040/25901]\n",
      "  loss: 0.077272  [15360/25901]\n",
      "  loss: 0.073383  [15680/25901]\n",
      "  loss: 0.078201  [16000/25901]\n",
      "  loss: 0.075431  [16320/25901]\n",
      "  loss: 0.072969  [16640/25901]\n",
      "  loss: 0.084278  [16960/25901]\n",
      "  loss: 0.083606  [17280/25901]\n",
      "  loss: 0.115013  [17600/25901]\n",
      "  loss: 0.082635  [17920/25901]\n",
      "  loss: 0.077876  [18240/25901]\n",
      "  loss: 0.076493  [18560/25901]\n",
      "  loss: 0.069808  [18880/25901]\n",
      "  loss: 0.075016  [19200/25901]\n",
      "  loss: 0.078048  [19520/25901]\n",
      "  loss: 0.071235  [19840/25901]\n",
      "  loss: 0.080807  [20160/25901]\n",
      "  loss: 0.086252  [20480/25901]\n",
      "  loss: 0.076462  [20800/25901]\n",
      "  loss: 0.072053  [21120/25901]\n",
      "  loss: 0.085606  [21440/25901]\n",
      "  loss: 0.080209  [21760/25901]\n",
      "  loss: 0.070451  [22080/25901]\n",
      "  loss: 0.086037  [22400/25901]\n",
      "  loss: 0.074469  [22720/25901]\n",
      "  loss: 0.084652  [23040/25901]\n",
      "  loss: 0.077389  [23360/25901]\n",
      "  loss: 0.069128  [23680/25901]\n",
      "  loss: 0.075274  [24000/25901]\n",
      "  loss: 0.069039  [24320/25901]\n",
      "  loss: 0.073826  [24640/25901]\n",
      "  loss: 0.070481  [24960/25901]\n",
      "  loss: 0.069814  [25280/25901]\n",
      "  loss: 0.071808  [25600/25901]\n",
      "\n",
      "Epoch 1/50 Summary:\n",
      "  Train Loss: 0.0892\n",
      "  Val Loss:   0.0763\n",
      "Validation loss improved. Saving model to /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "------------------------------------------------------------\n",
      "  loss: 0.075836  [    0/25901]\n",
      "  loss: 0.076783  [  320/25901]\n",
      "  loss: 0.076234  [  640/25901]\n",
      "  loss: 0.073072  [  960/25901]\n",
      "  loss: 0.073129  [ 1280/25901]\n",
      "  loss: 0.083358  [ 1600/25901]\n",
      "  loss: 0.075339  [ 1920/25901]\n",
      "  loss: 0.072898  [ 2240/25901]\n",
      "  loss: 0.082827  [ 2560/25901]\n",
      "  loss: 0.074465  [ 2880/25901]\n",
      "  loss: 0.079393  [ 3200/25901]\n",
      "  loss: 0.075790  [ 3520/25901]\n",
      "  loss: 0.075870  [ 3840/25901]\n",
      "  loss: 0.078705  [ 4160/25901]\n",
      "  loss: 0.077217  [ 4480/25901]\n",
      "  loss: 0.076134  [ 4800/25901]\n",
      "  loss: 0.071560  [ 5120/25901]\n",
      "  loss: 0.079089  [ 5440/25901]\n",
      "  loss: 0.082123  [ 5760/25901]\n",
      "  loss: 0.075267  [ 6080/25901]\n",
      "  loss: 0.061833  [ 6400/25901]\n",
      "  loss: 0.074001  [ 6720/25901]\n",
      "  loss: 0.077451  [ 7040/25901]\n",
      "  loss: 0.070065  [ 7360/25901]\n",
      "  loss: 0.067760  [ 7680/25901]\n",
      "  loss: 0.063957  [ 8000/25901]\n",
      "  loss: 0.081682  [ 8320/25901]\n",
      "  loss: 0.085154  [ 8640/25901]\n",
      "  loss: 0.072905  [ 8960/25901]\n",
      "  loss: 0.078285  [ 9280/25901]\n",
      "  loss: 0.067180  [ 9600/25901]\n",
      "  loss: 0.077302  [ 9920/25901]\n",
      "  loss: 0.074522  [10240/25901]\n",
      "  loss: 0.062552  [10560/25901]\n",
      "  loss: 0.073954  [10880/25901]\n",
      "  loss: 0.080012  [11200/25901]\n",
      "  loss: 0.071162  [11520/25901]\n",
      "  loss: 0.070748  [11840/25901]\n",
      "  loss: 0.075492  [12160/25901]\n",
      "  loss: 0.073787  [12480/25901]\n",
      "  loss: 0.076225  [12800/25901]\n",
      "  loss: 0.060387  [13120/25901]\n",
      "  loss: 0.078922  [13440/25901]\n",
      "  loss: 0.084982  [13760/25901]\n",
      "  loss: 0.070044  [14080/25901]\n",
      "  loss: 0.070958  [14400/25901]\n",
      "  loss: 0.080271  [14720/25901]\n",
      "  loss: 0.082428  [15040/25901]\n",
      "  loss: 0.074863  [15360/25901]\n",
      "  loss: 0.078968  [15680/25901]\n",
      "  loss: 0.063700  [16000/25901]\n",
      "  loss: 0.070150  [16320/25901]\n",
      "  loss: 0.066129  [16640/25901]\n",
      "  loss: 0.075453  [16960/25901]\n",
      "  loss: 0.069910  [17280/25901]\n",
      "  loss: 0.068939  [17600/25901]\n",
      "  loss: 0.068604  [17920/25901]\n",
      "  loss: 0.081856  [18240/25901]\n",
      "  loss: 0.073733  [18560/25901]\n",
      "  loss: 0.075999  [18880/25901]\n",
      "  loss: 0.073395  [19200/25901]\n",
      "  loss: 0.080393  [19520/25901]\n",
      "  loss: 0.082072  [19840/25901]\n",
      "  loss: 0.068407  [20160/25901]\n",
      "  loss: 0.080023  [20480/25901]\n",
      "  loss: 0.065670  [20800/25901]\n",
      "  loss: 0.069121  [21120/25901]\n",
      "  loss: 0.065106  [21440/25901]\n",
      "  loss: 0.070407  [21760/25901]\n",
      "  loss: 0.082003  [22080/25901]\n",
      "  loss: 0.074972  [22400/25901]\n",
      "  loss: 0.076353  [22720/25901]\n",
      "  loss: 0.073317  [23040/25901]\n",
      "  loss: 0.067149  [23360/25901]\n",
      "  loss: 0.073719  [23680/25901]\n",
      "  loss: 0.076214  [24000/25901]\n",
      "  loss: 0.072653  [24320/25901]\n",
      "  loss: 0.061402  [24640/25901]\n",
      "  loss: 0.063003  [24960/25901]\n",
      "  loss: 0.069056  [25280/25901]\n",
      "  loss: 0.069483  [25600/25901]\n",
      "\n",
      "Epoch 2/50 Summary:\n",
      "  Train Loss: 0.0738\n",
      "  Val Loss:   0.0724\n",
      "Validation loss improved. Saving model to /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "------------------------------------------------------------\n",
      "  loss: 0.074731  [    0/25901]\n",
      "  loss: 0.078123  [  320/25901]\n",
      "  loss: 0.066476  [  640/25901]\n",
      "  loss: 0.065795  [  960/25901]\n",
      "  loss: 0.079749  [ 1280/25901]\n",
      "  loss: 0.075887  [ 1600/25901]\n",
      "  loss: 0.061882  [ 1920/25901]\n",
      "  loss: 0.068058  [ 2240/25901]\n",
      "  loss: 0.072420  [ 2560/25901]\n",
      "  loss: 0.075932  [ 2880/25901]\n",
      "  loss: 0.077790  [ 3200/25901]\n",
      "  loss: 0.066563  [ 3520/25901]\n",
      "  loss: 0.070810  [ 3840/25901]\n",
      "  loss: 0.078451  [ 4160/25901]\n",
      "  loss: 0.075434  [ 4480/25901]\n",
      "  loss: 0.082711  [ 4800/25901]\n",
      "  loss: 0.071000  [ 5120/25901]\n",
      "  loss: 0.072424  [ 5440/25901]\n",
      "  loss: 0.070840  [ 5760/25901]\n",
      "  loss: 0.067785  [ 6080/25901]\n",
      "  loss: 0.064129  [ 6400/25901]\n",
      "  loss: 0.083844  [ 6720/25901]\n",
      "  loss: 0.079220  [ 7040/25901]\n",
      "  loss: 0.063515  [ 7360/25901]\n",
      "  loss: 0.070434  [ 7680/25901]\n",
      "  loss: 0.075382  [ 8000/25901]\n",
      "  loss: 0.065036  [ 8320/25901]\n",
      "  loss: 0.070384  [ 8640/25901]\n",
      "  loss: 0.070794  [ 8960/25901]\n",
      "  loss: 0.068727  [ 9280/25901]\n",
      "  loss: 0.060701  [ 9600/25901]\n",
      "  loss: 0.069934  [ 9920/25901]\n",
      "  loss: 0.068435  [10240/25901]\n",
      "  loss: 0.066993  [10560/25901]\n",
      "  loss: 0.071509  [10880/25901]\n",
      "  loss: 0.081933  [11200/25901]\n",
      "  loss: 0.073191  [11520/25901]\n",
      "  loss: 0.077015  [11840/25901]\n",
      "  loss: 0.072679  [12160/25901]\n",
      "  loss: 0.069988  [12480/25901]\n",
      "  loss: 0.074342  [12800/25901]\n",
      "  loss: 0.074700  [13120/25901]\n",
      "  loss: 0.073056  [13440/25901]\n",
      "  loss: 0.062609  [13760/25901]\n",
      "  loss: 0.065780  [14080/25901]\n",
      "  loss: 0.071641  [14400/25901]\n",
      "  loss: 0.068139  [14720/25901]\n",
      "  loss: 0.078632  [15040/25901]\n",
      "  loss: 0.075926  [15360/25901]\n",
      "  loss: 0.066134  [15680/25901]\n",
      "  loss: 0.063853  [16000/25901]\n",
      "  loss: 0.072723  [16320/25901]\n",
      "  loss: 0.067455  [16640/25901]\n",
      "  loss: 0.075931  [16960/25901]\n",
      "  loss: 0.075836  [17280/25901]\n",
      "  loss: 0.068127  [17600/25901]\n",
      "  loss: 0.074860  [17920/25901]\n",
      "  loss: 0.069834  [18240/25901]\n",
      "  loss: 0.068197  [18560/25901]\n",
      "  loss: 0.069250  [18880/25901]\n",
      "  loss: 0.070116  [19200/25901]\n",
      "  loss: 0.063399  [19520/25901]\n",
      "  loss: 0.072344  [19840/25901]\n",
      "  loss: 0.073169  [20160/25901]\n",
      "  loss: 0.063528  [20480/25901]\n",
      "  loss: 0.064192  [20800/25901]\n",
      "  loss: 0.075119  [21120/25901]\n",
      "  loss: 0.069546  [21440/25901]\n",
      "  loss: 0.070598  [21760/25901]\n",
      "  loss: 0.064020  [22080/25901]\n",
      "  loss: 0.073913  [22400/25901]\n",
      "  loss: 0.062477  [22720/25901]\n",
      "  loss: 0.064899  [23040/25901]\n",
      "  loss: 0.072302  [23360/25901]\n",
      "  loss: 0.073741  [23680/25901]\n",
      "  loss: 0.072739  [24000/25901]\n",
      "  loss: 0.066167  [24320/25901]\n",
      "  loss: 0.072248  [24640/25901]\n",
      "  loss: 0.073362  [24960/25901]\n",
      "  loss: 0.071568  [25280/25901]\n",
      "  loss: 0.074367  [25600/25901]\n",
      "\n",
      "Epoch 3/50 Summary:\n",
      "  Train Loss: 0.0714\n",
      "  Val Loss:   0.0699\n",
      "Validation loss improved. Saving model to /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "------------------------------------------------------------\n",
      "  loss: 0.068558  [    0/25901]\n",
      "  loss: 0.069022  [  320/25901]\n",
      "  loss: 0.067124  [  640/25901]\n",
      "  loss: 0.070478  [  960/25901]\n",
      "  loss: 0.068846  [ 1280/25901]\n",
      "  loss: 0.077198  [ 1600/25901]\n",
      "  loss: 0.079148  [ 1920/25901]\n",
      "  loss: 0.069723  [ 2240/25901]\n",
      "  loss: 0.072099  [ 2560/25901]\n",
      "  loss: 0.069340  [ 2880/25901]\n",
      "  loss: 0.069024  [ 3200/25901]\n",
      "  loss: 0.076465  [ 3520/25901]\n",
      "  loss: 0.068772  [ 3840/25901]\n",
      "  loss: 0.076864  [ 4160/25901]\n",
      "  loss: 0.074548  [ 4480/25901]\n",
      "  loss: 0.076921  [ 4800/25901]\n",
      "  loss: 0.076732  [ 5120/25901]\n",
      "  loss: 0.074321  [ 5440/25901]\n",
      "  loss: 0.074927  [ 5760/25901]\n",
      "  loss: 0.069014  [ 6080/25901]\n",
      "  loss: 0.075365  [ 6400/25901]\n",
      "  loss: 0.069096  [ 6720/25901]\n",
      "  loss: 0.067280  [ 7040/25901]\n",
      "  loss: 0.061911  [ 7360/25901]\n",
      "  loss: 0.061471  [ 7680/25901]\n",
      "  loss: 0.069123  [ 8000/25901]\n",
      "  loss: 0.073685  [ 8320/25901]\n",
      "  loss: 0.057759  [ 8640/25901]\n",
      "  loss: 0.070795  [ 8960/25901]\n",
      "  loss: 0.075061  [ 9280/25901]\n",
      "  loss: 0.076761  [ 9600/25901]\n",
      "  loss: 0.075847  [ 9920/25901]\n",
      "  loss: 0.060695  [10240/25901]\n",
      "  loss: 0.064287  [10560/25901]\n",
      "  loss: 0.065652  [10880/25901]\n",
      "  loss: 0.069691  [11200/25901]\n",
      "  loss: 0.067233  [11520/25901]\n",
      "  loss: 0.073335  [11840/25901]\n",
      "  loss: 0.060642  [12160/25901]\n",
      "  loss: 0.072107  [12480/25901]\n",
      "  loss: 0.075494  [12800/25901]\n",
      "  loss: 0.064297  [13120/25901]\n",
      "  loss: 0.074802  [13440/25901]\n",
      "  loss: 0.067102  [13760/25901]\n",
      "  loss: 0.079200  [14080/25901]\n",
      "  loss: 0.062238  [14400/25901]\n",
      "  loss: 0.066106  [14720/25901]\n",
      "  loss: 0.073854  [15040/25901]\n",
      "  loss: 0.061141  [15360/25901]\n",
      "  loss: 0.076057  [15680/25901]\n",
      "  loss: 0.067462  [16000/25901]\n",
      "  loss: 0.068951  [16320/25901]\n",
      "  loss: 0.096168  [16640/25901]\n",
      "  loss: 0.062394  [16960/25901]\n",
      "  loss: 0.067450  [17280/25901]\n",
      "  loss: 0.072665  [17600/25901]\n",
      "  loss: 0.073572  [17920/25901]\n",
      "  loss: 0.065580  [18240/25901]\n",
      "  loss: 0.067067  [18560/25901]\n",
      "  loss: 0.070680  [18880/25901]\n",
      "  loss: 0.059813  [19200/25901]\n",
      "  loss: 0.073411  [19520/25901]\n",
      "  loss: 0.065668  [19840/25901]\n",
      "  loss: 0.066416  [20160/25901]\n",
      "  loss: 0.068960  [20480/25901]\n",
      "  loss: 0.064521  [20800/25901]\n",
      "  loss: 0.080108  [21120/25901]\n",
      "  loss: 0.074560  [21440/25901]\n",
      "  loss: 0.069672  [21760/25901]\n",
      "  loss: 0.071383  [22080/25901]\n",
      "  loss: 0.064283  [22400/25901]\n",
      "  loss: 0.069802  [22720/25901]\n",
      "  loss: 0.077900  [23040/25901]\n",
      "  loss: 0.069587  [23360/25901]\n",
      "  loss: 0.065891  [23680/25901]\n",
      "  loss: 0.074940  [24000/25901]\n",
      "  loss: 0.058339  [24320/25901]\n",
      "  loss: 0.063118  [24640/25901]\n",
      "  loss: 0.073936  [24960/25901]\n",
      "  loss: 0.073926  [25280/25901]\n",
      "  loss: 0.067654  [25600/25901]\n",
      "\n",
      "Epoch 4/50 Summary:\n",
      "  Train Loss: 0.0694\n",
      "  Val Loss:   0.0689\n",
      "Validation loss improved. Saving model to /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "------------------------------------------------------------\n",
      "  loss: 0.073481  [    0/25901]\n",
      "  loss: 0.060689  [  320/25901]\n",
      "  loss: 0.065360  [  640/25901]\n",
      "  loss: 0.069184  [  960/25901]\n",
      "  loss: 0.069327  [ 1280/25901]\n",
      "  loss: 0.066275  [ 1600/25901]\n",
      "  loss: 0.062183  [ 1920/25901]\n",
      "  loss: 0.066893  [ 2240/25901]\n",
      "  loss: 0.075369  [ 2560/25901]\n",
      "  loss: 0.074927  [ 2880/25901]\n",
      "  loss: 0.060058  [ 3200/25901]\n",
      "  loss: 0.070438  [ 3520/25901]\n",
      "  loss: 0.055561  [ 3840/25901]\n",
      "  loss: 0.072740  [ 4160/25901]\n",
      "  loss: 0.071508  [ 4480/25901]\n",
      "  loss: 0.069110  [ 4800/25901]\n",
      "  loss: 0.062576  [ 5120/25901]\n",
      "  loss: 0.068288  [ 5440/25901]\n",
      "  loss: 0.075301  [ 5760/25901]\n",
      "  loss: 0.076066  [ 6080/25901]\n",
      "  loss: 0.060215  [ 6400/25901]\n",
      "  loss: 0.064965  [ 6720/25901]\n",
      "  loss: 0.057549  [ 7040/25901]\n",
      "  loss: 0.083724  [ 7360/25901]\n",
      "  loss: 0.069459  [ 7680/25901]\n",
      "  loss: 0.066770  [ 8000/25901]\n",
      "  loss: 0.066183  [ 8320/25901]\n",
      "  loss: 0.072418  [ 8640/25901]\n",
      "  loss: 0.083791  [ 8960/25901]\n",
      "  loss: 0.062543  [ 9280/25901]\n",
      "  loss: 0.056907  [ 9600/25901]\n",
      "  loss: 0.068917  [ 9920/25901]\n",
      "  loss: 0.074649  [10240/25901]\n",
      "  loss: 0.066632  [10560/25901]\n",
      "  loss: 0.060905  [10880/25901]\n",
      "  loss: 0.064521  [11200/25901]\n",
      "  loss: 0.075975  [11520/25901]\n",
      "  loss: 0.068308  [11840/25901]\n",
      "  loss: 0.065521  [12160/25901]\n",
      "  loss: 0.070907  [12480/25901]\n",
      "  loss: 0.072707  [12800/25901]\n",
      "  loss: 0.062166  [13120/25901]\n",
      "  loss: 0.058632  [13440/25901]\n",
      "  loss: 0.080532  [13760/25901]\n",
      "  loss: 0.069682  [14080/25901]\n",
      "  loss: 0.071542  [14400/25901]\n",
      "  loss: 0.061505  [14720/25901]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Progress tracking\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Progress tracking\n",
    "        if batch_id % 5 == 0:\n",
    "            current = batch_id * len(batch[\"X\"])\n",
    "            print(f\"  loss: {loss.item():>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss, model, epoch)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save final model and history\n",
    "save_final_model(model, final_model_path)\n",
    "save_loss_history(history, history_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Loss history saved to: {history_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49106c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
