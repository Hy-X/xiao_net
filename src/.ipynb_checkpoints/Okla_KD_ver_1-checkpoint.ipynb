{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a73e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Distillation Training for XiaoNet\n",
    "Teacher: PhaseNet (from STEAD)\n",
    "Student: XiaoNet (v2, v3, v4, or v5)\n",
    "Dataset: OKLA regional seismic data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seismology & SeisBench\n",
    "import obspy\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing local modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# XiaoNet modules\n",
    "from models.xn_xiao_net_v2 import XiaoNet as XiaoNetV2\n",
    "from models.xn_xiao_net_v3 import XiaoNet as XiaoNetV3\n",
    "from models.xn_xiao_net_v4 import XiaoNetFast as XiaoNetV4\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge as XiaoNetV5\n",
    "from loss.xn_distillation_loss import DistillationLoss\n",
    "from xn_utils import set_seed, setup_device\n",
    "from xn_early_stopping import EarlyStopping\n",
    "\n",
    "print(\"✓ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c646aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = setup_device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22312606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /Users/hongyuxiao/Hongyu_File/xiao_net/config.json\n",
      "{\n",
      "  \"peak_detection\": {\n",
      "    \"sampling_rate\": 100,\n",
      "    \"height\": 0.5,\n",
      "    \"distance\": 100\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"dataset_name\": \"OKLA_1Mil_120s_Ver_3\",\n",
      "    \"sampling_rate\": 100,\n",
      "    \"window_len\": 3001,\n",
      "    \"samples_before\": 3000,\n",
      "    \"windowlen_large\": 6000,\n",
      "    \"sample_fraction\": 0.1\n",
      "  },\n",
      "  \"data_filter\": {\n",
      "    \"min_magnitude\": 1.0,\n",
      "    \"max_magnitude\": 2.0\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 4,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"epochs\": 50,\n",
      "    \"patience\": 5,\n",
      "    \"loss_weights\": [\n",
      "      0.01,\n",
      "      0.4,\n",
      "      0.59\n",
      "    ],\n",
      "    \"optimization\": {\n",
      "      \"mixed_precision\": true,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"pin_memory\": true,\n",
      "      \"prefetch_factor\": 2,\n",
      "      \"persistent_workers\": true\n",
      "    }\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"use_cuda\": true,\n",
      "    \"device_id\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "config_path = Path.cwd().parent / \"config.json\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration from: {config_path}\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a55f45a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available PhaseNet pretrained models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['diting',\n",
       " 'ethz',\n",
       " 'geofon',\n",
       " 'instance',\n",
       " 'iquique',\n",
       " 'jma',\n",
       " 'jma_wc',\n",
       " 'lendb',\n",
       " 'neic',\n",
       " 'obs',\n",
       " 'original',\n",
       " 'phasenet_sn',\n",
       " 'pisdl',\n",
       " 'scedc',\n",
       " 'stead',\n",
       " 'volpick']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PhaseNet teacher model (pretrained on STEAD)\n",
    "print(\"Available PhaseNet pretrained models:\")\n",
    "sbm.PhaseNet.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50f1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PhaseNet teacher model...\n",
      "\n",
      "✓ PhaseNet teacher loaded successfully!\n",
      "Total parameters: 268,443\n",
      "Trainable parameters: 268,443\n",
      "Model on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading PhaseNet teacher model...\")\n",
    "model = sbm.PhaseNet.from_pretrained(\"stead\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode for teacher\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ PhaseNet teacher loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "887fb320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OKLA regional seismic dataset...\n",
      "Sampling 10.0% of data for faster training...\n",
      "Sampled dataset size: 113,884\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "Training samples: 79,670\n",
      "Validation samples: 17,090\n",
      "Test samples: 17,124\n",
      "Total samples: 113,884\n"
     ]
    }
   ],
   "source": [
    "# Load OKLA dataset\n",
    "print(\"Loading OKLA regional seismic dataset...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n",
    "# Optional: Use subset for faster experimentation\n",
    "sample_fraction = config.get('data', {}).get('sample_fraction', 0.1)\n",
    "if sample_fraction < 1.0:\n",
    "    print(f\"Sampling {sample_fraction*100}% of data for faster training...\")\n",
    "    # Create a random mask for sampling\n",
    "    mask = np.random.random(len(data)) < sample_fraction\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"Sampled dataset size: {len(data):,}\")\n",
    "\n",
    "# Split into train/dev/test\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train):,}\")\n",
    "print(f\"Validation samples: {len(dev):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Total samples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaa20617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying magnitude filters: 1.0 < M < 2.0\n",
      "✓ [Data Filter]: Start - magnitude > 1.0\n",
      "✓ [Data Filter]: Applied - magnitude > 1.0, remaining samples: 108,944\n",
      "✓ [Data Filter]: Start - magnitude < 2.0\n",
      "✓ [Data Filter]: Applied - magnitude < 2.0, remaining samples: 36,880\n",
      "\n",
      "✓ Magnitude filtering complete: 36,880 traces in range [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# Magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', 1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 2.0)\n",
    "\n",
    "print(f\"Applying magnitude filters: {min_magnitude} < M < {max_magnitude}\")\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude > {min_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude < {max_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Magnitude filtering complete: {len(data):,} traces in range [{min_magnitude}, {max_magnitude}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78d3b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total dataset size: 36,880\n",
      "Train size: 79,670\n",
      "Validation size: 17,090\n",
      "Test size: 17,124\n",
      "Sampling rate: 100 Hz\n",
      "Window length: 3001 samples\n",
      "Magnitude stats: min=1.00, max=2.00, mean=1.53\n",
      "Metadata columns: ['index', 'station_network_code', 'station_code', 'trace_channel', 'station_latitude_deg', 'station_longitude_deg', 'station_elevation_m', 'trace_p_arrival_sample', 'trace_p_status', 'trace_p_weight', 'path_p_travel_sec', 'trace_s_arrival_sample', 'trace_s_status', 'trace_s_weight', 'source_id', 'source_origin_time', 'source_origin_uncertainty_sec', 'source_latitude_deg', 'source_longitude_deg', 'source_error_sec', 'source_gap_deg', 'source_horizontal_uncertainty_km', 'source_depth_km', 'source_depth_uncertainty_km', 'source_magnitude', 'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake', 'source_distance_deg', 'source_distance_km', 'path_back_azimuth_deg', 'trace_snr_db', 'trace_coda_end_sample', 'trace_start_time', 'trace_category', 'trace_name', 'split', 'trace_name_original', 'trace_chunk', 'trace_sampling_rate_hz', 'trace_component_order']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary for training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core sizes\n",
    "print(f\"Total dataset size: {len(data):,}\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Sampling configuration\n",
    "sampling_rate = config.get('data', {}).get('sampling_rate', 'unknown')\n",
    "window_len = config.get('data', {}).get('window_len', 'unknown')\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Window length: {window_len} samples\")\n",
    "\n",
    "# Metadata summary (if available)\n",
    "if hasattr(data, 'metadata') and data.metadata is not None:\n",
    "    if 'source_magnitude' in data.metadata:\n",
    "        mags = data.metadata['source_magnitude']\n",
    "        print(f\"Magnitude stats: min={mags.min():.2f}, max={mags.max():.2f}, mean={mags.mean():.2f}\")\n",
    "    print(f\"Metadata columns: {list(data.metadata.columns)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28ab888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset split after filtering\n",
      "Train size: 25,901\n",
      "Validation size: 5,518\n",
      "Test size: 5,461\n",
      "Split ratios: train=70.23%, dev=14.96%, test=14.81%\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/dev/test after filtering\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(\"\\n✓ Dataset split after filtering\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Split ratios\n",
    "n_total = len(train) + len(dev) + len(test)\n",
    "if n_total > 0:\n",
    "    print(f\"Split ratios: train={len(train)/n_total:.2%}, dev={len(dev)/n_total:.2%}, test={len(test)/n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2c2672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET OBJECTS\n",
      "============================================================\n",
      "Train dataset: OKLA_1Mil_120s_Ver_3 - 25901 traces\n",
      "Dev dataset:   OKLA_1Mil_120s_Ver_3 - 5518 traces\n",
      "Test dataset:  OKLA_1Mil_120s_Ver_3 - 5461 traces\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset objects (compact summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET OBJECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train dataset: {train}\")\n",
    "print(f\"Dev dataset:   {dev}\")\n",
    "print(f\"Test dataset:  {test}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81187cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7b64fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7c83d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dddf3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PEAK DETECTION SETTINGS\n",
      "============================================================\n",
      "Sampling rate: 100 Hz\n",
      "Height threshold: 0.5\n",
      "Minimum peak distance: 100 samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PEAK DETECTION SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Height threshold: {height}\")\n",
    "print(f\"Minimum peak distance: {distance} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb0b4ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ [DataLoader]: batch_size=64, num_workers=4\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "print(f\"✓ [DataLoader]: batch_size={batch_size}, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "570e9c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "[Dataset]\n",
      "  Total samples:      36,880\n",
      "  Train/Validation/Test:     25,901 / 5,518 / 5,461\n",
      "  Sample fraction:    10.0%\n",
      "\n",
      "[Device]\n",
      "  Device:             cpu\n",
      "\n",
      "[Training]\n",
      "  Batch size:         64\n",
      "  Num workers:        4\n",
      "  Learning rate:      0.01\n",
      "  Epochs:             50\n",
      "  Patience:           5\n",
      "\n",
      "[Peak Detection]\n",
      "  Sampling rate:      100 Hz\n",
      "  Height threshold:   0.5\n",
      "  Min peak distance:  100 samples\n",
      "============================================================\n",
      "Ready to start training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset info\n",
    "print(\"[Dataset]\")\n",
    "print(f\"  Total samples:      {len(data):,}\")\n",
    "print(f\"  Train/Validation/Test:     {len(train):,} / {len(dev):,} / {len(test):,}\")\n",
    "print(f\"  Sample fraction:    {sample_fraction*100:.1f}%\")\n",
    "\n",
    "# Device\n",
    "print(\"\\n[Device]\")\n",
    "print(f\"  Device:             {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "print(\"\\n[Training]\")\n",
    "print(f\"  Batch size:         {batch_size}\")\n",
    "print(f\"  Num workers:        {num_workers}\")\n",
    "print(f\"  Learning rate:      {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs:             {config['training']['epochs']}\")\n",
    "print(f\"  Patience:           {config['training']['patience']}\")\n",
    "\n",
    "# Peak detection\n",
    "print(\"\\n[Peak Detection]\")\n",
    "print(f\"  Sampling rate:      {sampling_rate} Hz\")\n",
    "print(f\"  Height threshold:   {height}\")\n",
    "print(f\"  Min peak distance:  {distance} samples\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to start training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "830aeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c530a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce428776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZER SETTINGS\n",
      "============================================================\n",
      "Optimizer: Adam\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZER SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61328216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EARLY STOPPING & CHECKPOINTS\n",
      "============================================================\n",
      "Checkpoint dir: /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints\n",
      "Best model:     /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/best_model.pth\n",
      "Final model:    /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/final_model.pth\n",
      "History file:   /Users/hongyuxiao/Hongyu_File/xiao_net/checkpoints/loss_history.json\n",
      "Patience:       5\n",
      "Min delta:      0.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and checkpoint setup\n",
    "checkpoint_dir = Path.cwd().parent / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_model_path = checkpoint_dir / \"best_model.pth\"\n",
    "final_model_path = checkpoint_dir / \"final_model.pth\"\n",
    "history_path = checkpoint_dir / \"loss_history.json\"\n",
    "\n",
    "patience = config.get('training', {}).get('patience', 5)\n",
    "min_delta = config.get('training', {}).get('min_delta', 0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Loss history container\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "\n",
    "# Helper functions for saving\n",
    "def save_loss_history(history_dict, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(history_dict, f, indent=2)\n",
    "    print(f\"✓ Loss history saved to {path}\")\n",
    "\n",
    "\n",
    "def save_final_model(model, path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, path)\n",
    "    print(f\"✓ Final model saved to {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLY STOPPING & CHECKPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
    "print(f\"Best model:     {best_model_path}\")\n",
    "print(f\"Final model:    {final_model_path}\")\n",
    "print(f\"History file:   {history_path}\")\n",
    "print(f\"Patience:       {patience}\")\n",
    "print(f\"Min delta:      {min_delta}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6327c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 8578, 8586, 8590, 8595) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/multiprocessing/reductions.py:514\u001b[0m, in \u001b[0;36mrebuild_storage_filename\u001b[0;34m(cls, manager, handle, size, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_shared_filename_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Broken pipe",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     10\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:436\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1112\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1110\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1112\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 8578, 8586, 8590, 8595) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Progress tracking\n",
    "        if batch_id % 5 == 0:\n",
    "            current = batch_id * len(batch[\"X\"])\n",
    "            print(f\"  loss: {loss.item():>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save final model and history\n",
    "save_final_model(model, final_model_path)\n",
    "save_loss_history(history, history_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Loss history saved to: {history_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49106c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
