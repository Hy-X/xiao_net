{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a73e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Distillation Training for XiaoNet\n",
    "Teacher: PhaseNet (from STEAD)\n",
    "Student: XiaoNet (v2, v3, v4, or v5)\n",
    "Dataset: OKLA regional seismic data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seismology & SeisBench\n",
    "import obspy\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing local modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# XiaoNet modules\n",
    "from models.xn_xiao_net_v2 import XiaoNet as XiaoNetV2\n",
    "from models.xn_xiao_net_v3 import XiaoNet as XiaoNetV3\n",
    "from models.xn_xiao_net_v4 import XiaoNetFast as XiaoNetV4\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge as XiaoNetV5\n",
    "from models.xn_xiao_net_v5a import XiaoNetV5A as XiaoNetV5A\n",
    "from models.xn_xiao_net_v5b import XiaoNetV5B as XiaoNetV5B\n",
    "\n",
    "print(\"✓ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to halt training when validation loss stops improving.\n",
    "    \n",
    "    Args:\n",
    "        patience: Epochs to wait before stopping\n",
    "        min_delta: Minimum improvement threshold (positive value)\n",
    "        checkpoint_dir: Directory for checkpoints\n",
    "        model_name: Name of the model (for checkpoint filename)\n",
    "        sample_fraction: Sample fraction used in training (for checkpoint filename)\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.0, checkpoint_dir='checkpoints/', model_name='model', sample_fraction=1.0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_loss = None  # Store actual loss for clarity\n",
    "        self.early_stop = False\n",
    "        self.checkpoint_path = self.checkpoint_dir / f'{model_name}_sf{sample_fraction}_best_model.pth'\n",
    "    \n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        \"\"\"\n",
    "        Check if training should stop early.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss (lower is better)\n",
    "            model: Model to save if improvement found\n",
    "            epoch: Current epoch number\n",
    "        \"\"\"\n",
    "        # First epoch - initialize\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model, epoch, val_loss)\n",
    "            return\n",
    "        \n",
    "        # Check for improvement (val_loss decreased by at least min_delta)\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model, epoch, val_loss)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience} (best loss: {self.best_loss:.6f})')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, val_loss):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'best_loss': self.best_loss\n",
    "        }, self.checkpoint_path)\n",
    "        if self.verbose:\n",
    "            print(f'✓ Validation loss improved: {val_loss:.6f}. Saved to {self.checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2c1b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5926c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device(device_type='cuda'):\n",
    "    \"\"\"Setup compute device (CUDA if available, else CPU).\"\"\"\n",
    "    if device_type == 'cuda' and torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    return torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c646aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = setup_device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22312606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /Users/hongyuxiao/Hongyu_File/xiao_net/config.json\n",
      "{\n",
      "  \"peak_detection\": {\n",
      "    \"sampling_rate\": 100,\n",
      "    \"height\": 0.5,\n",
      "    \"distance\": 100\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"dataset_name\": \"OKLA_1Mil_120s_Ver_3\",\n",
      "    \"sampling_rate\": 100,\n",
      "    \"window_len\": 3001,\n",
      "    \"samples_before\": 3000,\n",
      "    \"windowlen_large\": 6000,\n",
      "    \"sample_fraction\": 0.01\n",
      "  },\n",
      "  \"data_filter\": {\n",
      "    \"min_magnitude\": -1.0,\n",
      "    \"max_magnitude\": 10.0\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 4,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"epochs\": 50,\n",
      "    \"patience\": 5,\n",
      "    \"loss_weights\": [\n",
      "      0.01,\n",
      "      0.4,\n",
      "      0.59\n",
      "    ],\n",
      "    \"optimization\": {\n",
      "      \"mixed_precision\": true,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"pin_memory\": true,\n",
      "      \"prefetch_factor\": 2,\n",
      "      \"persistent_workers\": true\n",
      "    }\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"use_cuda\": true,\n",
      "    \"device_id\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "config_path = Path.cwd().parent / \"config.json\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration from: {config_path}\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50f1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing XiaoNet V5B student model...\n",
      "Model class name: XiaoNetV5B\n",
      "\n",
      "✓ XiaoNetV5B student loaded successfully!\n",
      "Total parameters: 20,208\n",
      "Trainable parameters: 20,208\n",
      "Model on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing student model...\")\n",
    "model = XiaoNetV5B(window_len=3001, in_channels=3, num_phases=3, base_channels=8)\n",
    "model.to(device)\n",
    "model.train()  # Set to training mode for student\n",
    "\n",
    "# Extract model name from class\n",
    "model_name = model.__class__.__name__\n",
    "print(f\"Model class name: {model_name}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ {model_name} student loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fb320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OKLA dataset\n",
    "print(\"Loading OKLA regional seismic dataset...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n",
    "# Optional: Use subset for faster experimentation\n",
    "sample_fraction = config.get('data', {}).get('sample_fraction', 0.1)\n",
    "if sample_fraction < 1.0:\n",
    "    print(f\"Sampling {sample_fraction*100}% of data for faster training...\")\n",
    "    # Create a random mask for sampling\n",
    "    mask = np.random.random(len(data)) < sample_fraction\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"Sampled dataset size: {len(data):,}\")\n",
    "\n",
    "# Split into train/dev/test\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train):,}\")\n",
    "print(f\"Validation samples: {len(dev):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Total samples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa20617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', 1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 2.0)\n",
    "\n",
    "print(f\"Applying magnitude filters: {min_magnitude} < M < {max_magnitude}\")\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude > {min_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude < {max_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Magnitude filtering complete: {len(data):,} traces in range [{min_magnitude}, {max_magnitude}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset summary for training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core sizes\n",
    "print(f\"Total dataset size: {len(data):,}\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Sampling configuration\n",
    "sampling_rate = config.get('data', {}).get('sampling_rate', 'unknown')\n",
    "window_len = config.get('data', {}).get('window_len', 'unknown')\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Window length: {window_len} samples\")\n",
    "\n",
    "# Metadata summary (if available)\n",
    "if hasattr(data, 'metadata') and data.metadata is not None:\n",
    "    if 'source_magnitude' in data.metadata:\n",
    "        mags = data.metadata['source_magnitude']\n",
    "        print(f\"Magnitude stats: min={mags.min():.2f}, max={mags.max():.2f}, mean={mags.mean():.2f}\")\n",
    "    print(f\"Metadata columns: {list(data.metadata.columns)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/dev/test after filtering\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(\"\\n✓ Dataset split after filtering\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Split ratios\n",
    "n_total = len(train) + len(dev) + len(test)\n",
    "if n_total > 0:\n",
    "    print(f\"Split ratios: train={len(train)/n_total:.2%}, dev={len(dev)/n_total:.2%}, test={len(test)/n_total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset objects (compact summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET OBJECTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train dataset: {train}\")\n",
    "print(f\"Dev dataset:   {dev}\")\n",
    "print(f\"Test dataset:  {test}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81187cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b64fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c83d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PEAK DETECTION SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Height threshold: {height}\")\n",
    "print(f\"Minimum peak distance: {distance} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for peak detection\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "print(f\"✓ [DataLoader]: batch_size={batch_size}, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset info\n",
    "print(\"[Dataset]\")\n",
    "print(f\"  Total samples:      {len(data):,}\")\n",
    "print(f\"  Train/Validation/Test:     {len(train):,} / {len(dev):,} / {len(test):,}\")\n",
    "print(f\"  Sample fraction:    {sample_fraction*100:.1f}%\")\n",
    "\n",
    "# Device\n",
    "print(\"\\n[Device]\")\n",
    "print(f\"  Device:             {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "print(\"\\n[Training]\")\n",
    "print(f\"  Batch size:         {batch_size}\")\n",
    "print(f\"  Num workers:        {num_workers}\")\n",
    "print(f\"  Learning rate:      {config['training']['learning_rate']}\")\n",
    "print(f\"  Epochs:             {config['training']['epochs']}\")\n",
    "print(f\"  Patience:           {config['training']['patience']}\")\n",
    "\n",
    "# Peak detection\n",
    "print(\"\\n[Peak Detection]\")\n",
    "print(f\"  Sampling rate:      {sampling_rate} Hz\")\n",
    "print(f\"  Height threshold:   {height}\")\n",
    "print(f\"  Min peak distance:  {distance} samples\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to start training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830aeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c530a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce428776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZER SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61328216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and checkpoint setup\n",
    "checkpoint_dir = Path.cwd().parent / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use extracted model name for all checkpoint files\n",
    "best_model_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_best_model.pth\"\n",
    "final_model_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_final_model.pth\"\n",
    "history_path = checkpoint_dir / f\"{model_name}_sf{sample_fraction}_loss_history.json\"\n",
    "\n",
    "patience = config.get('training', {}).get('patience', 5)\n",
    "min_delta = config.get('training', {}).get('min_delta', 0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    model_name=model_name,\n",
    "    sample_fraction=sample_fraction,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Loss history container\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "\n",
    "# Helper functions for saving\n",
    "def save_loss_history(history_dict, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(history_dict, f, indent=2)\n",
    "    print(f\"✓ Loss history saved to {path}\")\n",
    "\n",
    "\n",
    "def save_final_model(model, path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, path)\n",
    "    print(f\"✓ Final model saved to {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLY STOPPING & CHECKPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model name:     {model_name}\")\n",
    "print(f\"Sample fraction: {sample_fraction}\")\n",
    "print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
    "print(f\"Best model:     {best_model_path}\")\n",
    "print(f\"Final model:    {final_model_path}\")\n",
    "print(f\"History file:   {history_path}\")\n",
    "print(f\"Patience:       {patience}\")\n",
    "print(f\"Min delta:      {min_delta}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Progress tracking\n",
    "        if batch_id % 5 == 0:\n",
    "            current = batch_id * len(batch[\"X\"])\n",
    "            print(f\"  loss: {loss.item():>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss, model, epoch)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Save final model and history\n",
    "save_final_model(model, final_model_path)\n",
    "save_loss_history(history, history_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Loss history saved to: {history_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49106c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder\n",
    "output_folder = \"single_examples\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(f\"✓ [Peak Detection]: sampling_rate={sampling_rate} Hz, height={height}, distance={distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bda353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction 10 times\n",
    "for i in range(1, 11):\n",
    "    # Visualizing Predictions\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    waveform = sample[\"X\"]  # Shape: (3, N)\n",
    "    labels = sample[\"y\"]  # Shape: (3, N)\n",
    "\n",
    "    time_axis = np.arange(waveform.shape[1])  # Create a time axis\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axs = fig.subplots(5, 1, sharex=True, gridspec_kw={\"hspace\": 0.3})\n",
    "\n",
    "    # Color setup\n",
    "    channel_names = [\"Channel E\", \"Channel N\", \"Channel Z\"]\n",
    "    waveform_colors = ['#a3b18a', '#588157', '#344e41']  # Custom colors for channels\n",
    "    label_colors = ['#15616d', '#ff7d00']\n",
    "\n",
    "    # Plot waveforms\n",
    "    for j in range(3):\n",
    "        axs[j].plot(time_axis, waveform[j], color=waveform_colors[j], linewidth=1.5)\n",
    "        axs[j].set_title(f\"{channel_names[j]} - Seismic Waveform\", fontsize=12, fontweight='bold')\n",
    "        axs[j].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "        axs[j].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    #axs[0].plot(sample[\"X\"].T)\n",
    "    #axs[1].plot(sample[\"y\"].T)\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    axs[3].plot(time_axis, labels[0], color=label_colors[0], linewidth=1.5, label=\"P-phase\")\n",
    "    axs[3].plot(time_axis, labels[1], color=label_colors[1], linewidth=1.5, label=\"S-phase\")\n",
    "    axs[3].plot(y_p_peaks, sample[\"y\"][0, y_p_peaks], 'o', label='P arrivals', color='red')\n",
    "    axs[3].plot(y_s_peaks, sample[\"y\"][1, y_s_peaks], 'o', label='S arrivals', color='blue')\n",
    "    axs[3].set_title(\"Dataset Ground Truth\", fontsize=12, fontweight='bold')\n",
    "    axs[3].set_ylim(0,1.1)\n",
    "    axs[3].set_ylabel(\"Probability\", fontsize=10)\n",
    "    axs[3].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[3].legend(fontsize=10, loc=\"upper left\")\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals\n",
    "    residual_p_arrival_times = p_arrival_times - y_p_arrival_times[:, np.newaxis]\n",
    "    residual_s_arrival_times = s_arrival_times - y_s_arrival_times[:, np.newaxis]\n",
    "\n",
    "    # Plot the probability distributions and the detected peaks\n",
    "    axs[4].plot(p_prob, color=label_colors[0], linewidth=1.5,label='P-phase')\n",
    "    axs[4].plot(p_peaks, p_prob[p_peaks], 'x', label='Detected P Arrival', color='red')\n",
    "    axs[4].plot(s_prob, color=label_colors[1], linewidth=1.5,label='S-phase')\n",
    "    axs[4].plot(s_peaks, s_prob[s_peaks], 'x', label='Detected S Arrival', color='blue')\n",
    "    axs[4].set_title('Model Prediction', fontsize=12, fontweight='bold')\n",
    "    axs[4].set_ylim(0,1.1)\n",
    "    axs[4].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[4].set_ylabel('Probability', fontsize=10)\n",
    "    axs[4].legend(fontsize=10, loc=\"upper left\")\n",
    "\n",
    "    # Improve x-axis visibility\n",
    "    axs[4].set_xlabel(\"Time (samples)\", fontsize=11, fontweight='bold')\n",
    "    axs[4].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plot_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Plot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save the results to a text file\n",
    "    results_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Results.txt\")\n",
    "    with open(results_filename, \"w\") as f:\n",
    "        f.write(f\"Ground Truth P arrival times: {y_p_arrival_times}\\n\")\n",
    "        f.write(f\"Ground Truth S arrival times: {y_s_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted P arrival times: {p_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted S arrival times: {s_arrival_times}\\n\")\n",
    "        f.write(f\"Residual P arrival times: {residual_p_arrival_times}\\n\")\n",
    "        f.write(f\"Residual S arrival times: {residual_s_arrival_times}\\n\")\n",
    "\n",
    "    # Save the parameters to a text file\n",
    "    parameters_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Parameters.txt\")\n",
    "    with open(parameters_filename, \"w\") as f:\n",
    "        f.write(f\"Data Sampling Rate: {sampling_rate}\\n\")\n",
    "        f.write(f\"Detection Height Parameter: {height}\\n\")\n",
    "        f.write(f\"Detection Distance Parameter: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b353cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction 10 times\n",
    "for i in range(1, 11):\n",
    "    # Visualizing Predictions\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    waveform = sample[\"X\"]  # Shape: (3, N)\n",
    "    labels = sample[\"y\"]  # Shape: (3, N)\n",
    "\n",
    "    time_axis = np.arange(waveform.shape[1])  # Create a time axis\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axs = fig.subplots(5, 1, sharex=True, gridspec_kw={\"hspace\": 0.3})\n",
    "\n",
    "    # Color setup\n",
    "    channel_names = [\"Channel E\", \"Channel N\", \"Channel Z\"]\n",
    "    waveform_colors = ['#a3b18a', '#588157', '#344e41']  # Custom colors for channels\n",
    "    label_colors = ['#15616d', '#ff7d00']\n",
    "\n",
    "    # Plot waveforms\n",
    "    for j in range(3):\n",
    "        axs[j].plot(time_axis, waveform[j], color=waveform_colors[j], linewidth=1.5)\n",
    "        axs[j].set_title(f\"{channel_names[j]} - Seismic Waveform\", fontsize=12, fontweight='bold')\n",
    "        axs[j].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "        axs[j].grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    #axs[0].plot(sample[\"X\"].T)\n",
    "    #axs[1].plot(sample[\"y\"].T)\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    axs[3].plot(time_axis, labels[0], color=label_colors[0], linewidth=1.5, label=\"P-phase\")\n",
    "    axs[3].plot(time_axis, labels[1], color=label_colors[1], linewidth=1.5, label=\"S-phase\")\n",
    "    axs[3].plot(y_p_peaks, sample[\"y\"][0, y_p_peaks], 'o', label='P arrivals', color='red')\n",
    "    axs[3].plot(y_s_peaks, sample[\"y\"][1, y_s_peaks], 'o', label='S arrivals', color='blue')\n",
    "    axs[3].set_title(\"Dataset Ground Truth\", fontsize=12, fontweight='bold')\n",
    "    axs[3].set_ylim(0,1.1)\n",
    "    axs[3].set_ylabel(\"Probability\", fontsize=10)\n",
    "    axs[3].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[3].legend(fontsize=10, loc=\"upper left\")\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals\n",
    "    residual_p_arrival_times = p_arrival_times - y_p_arrival_times[:, np.newaxis]\n",
    "    residual_s_arrival_times = s_arrival_times - y_s_arrival_times[:, np.newaxis]\n",
    "\n",
    "    # Plot the probability distributions and the detected peaks\n",
    "    axs[4].plot(p_prob, color=label_colors[0], linewidth=1.5,label='P-phase')\n",
    "    axs[4].plot(p_peaks, p_prob[p_peaks], 'x', label='Detected P Arrival', color='red')\n",
    "    axs[4].plot(s_prob, color=label_colors[1], linewidth=1.5,label='S-phase')\n",
    "    axs[4].plot(s_peaks, s_prob[s_peaks], 'x', label='Detected S Arrival', color='blue')\n",
    "    axs[4].set_title('Model Prediction', fontsize=12, fontweight='bold')\n",
    "    axs[4].set_ylim(0,1.1)\n",
    "    axs[4].grid(True, linestyle='--', alpha=0.6)\n",
    "    axs[4].set_ylabel('Probability', fontsize=10)\n",
    "    axs[4].legend(fontsize=10, loc=\"upper left\")\n",
    "\n",
    "    # Improve x-axis visibility\n",
    "    axs[4].set_xlabel(\"Time (samples)\", fontsize=11, fontweight='bold')\n",
    "    axs[4].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plot_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Plot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Save the results to a text file\n",
    "    results_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Results.txt\")\n",
    "    with open(results_filename, \"w\") as f:\n",
    "        f.write(f\"Ground Truth P arrival times: {y_p_arrival_times}\\n\")\n",
    "        f.write(f\"Ground Truth S arrival times: {y_s_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted P arrival times: {p_arrival_times}\\n\")\n",
    "        f.write(f\"Model Predicted S arrival times: {s_arrival_times}\\n\")\n",
    "        f.write(f\"Residual P arrival times: {residual_p_arrival_times}\\n\")\n",
    "        f.write(f\"Residual S arrival times: {residual_s_arrival_times}\\n\")\n",
    "\n",
    "    # Save the parameters to a text file\n",
    "    parameters_filename = os.path.join(output_folder, f\"{model_name}_sf{sample_fraction}_Pred_{i:03d}_Parameters.txt\")\n",
    "    with open(parameters_filename, \"w\") as f:\n",
    "        f.write(f\"Data Sampling Rate: {sampling_rate}\\n\")\n",
    "        f.write(f\"Detection Height Parameter: {height}\\n\")\n",
    "        f.write(f\"Detection Distance Parameter: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples = len(test_generator)\n",
    "\n",
    "all_residual_p_arrival_times = []\n",
    "all_residual_s_arrival_times = []\n",
    "\n",
    "# Initialize counters for ground truth P and S labels\n",
    "groundtruth_p_peaks = 0\n",
    "groundtruth_s_peaks = 0\n",
    "\n",
    "# Initialize counters for residuals smaller than 0.6 (absolute value)\n",
    "count_residuals_p_under_0_6 = 0\n",
    "count_residuals_s_under_0_6 = 0\n",
    "\n",
    "# Only Commenting this out to reflect the randome samples to squential samples \n",
    "\n",
    "for i in range(n_samples):\n",
    "#for i in range(len(test_generator)):\n",
    "    \n",
    "    # Hongyu Xiao: randome sample works effectively\n",
    "    sample = test_generator[np.random.randint(len(test_generator))]\n",
    "\n",
    "    #sample = test_generator[i]\n",
    "\n",
    "    # Find peaks in the ground truth labels\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "\n",
    "    # Update the counters\n",
    "    groundtruth_p_peaks += len(y_p_peaks)\n",
    "    groundtruth_s_peaks += len(y_s_peaks)\n",
    "\n",
    "    # Convert ground truth peak indices to time values\n",
    "    sampling_rate = 100  # Samples per second (100 Hz)\n",
    "    y_p_arrival_times = y_p_peaks / sampling_rate\n",
    "    y_s_arrival_times = y_s_peaks / sampling_rate\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))  # Add a fake batch dimension\n",
    "        pred = pred[0].cpu().numpy()\n",
    "\n",
    "    # Extract the probability distributions for P and S phases\n",
    "    p_prob = pred[0]\n",
    "    s_prob = pred[1]\n",
    "\n",
    "    # Identify peaks in the probability distributions\n",
    "    p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "    s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "\n",
    "    # Convert peak indices to time values\n",
    "    p_arrival_times = p_peaks / sampling_rate\n",
    "    s_arrival_times = s_peaks / sampling_rate\n",
    "\n",
    "    # Calculate residuals for P and S peaks, keeping only the smallest one by absolute value\n",
    "    for y_p_time in y_p_arrival_times:\n",
    "        residual_p_arrival_times = p_arrival_times - y_p_time\n",
    "        if len(residual_p_arrival_times) > 0:\n",
    "            min_residual_p = residual_p_arrival_times[np.argmin(np.abs(residual_p_arrival_times))]\n",
    "            all_residual_p_arrival_times.append(min_residual_p)\n",
    "            if np.abs(min_residual_p) < 0.6:\n",
    "                count_residuals_p_under_0_6 += 1\n",
    "        \n",
    "    for y_s_time in y_s_arrival_times:\n",
    "        residual_s_arrival_times = s_arrival_times - y_s_time\n",
    "        if len(residual_s_arrival_times) > 0:\n",
    "            min_residual_s = residual_s_arrival_times[np.argmin(np.abs(residual_s_arrival_times))]\n",
    "            all_residual_s_arrival_times.append(min_residual_s)\n",
    "            if np.abs(min_residual_s) < 0.6:\n",
    "                count_residuals_s_under_0_6 += 1\n",
    "\n",
    "# Display the total counts of ground truth P and S peaks\n",
    "print(f\"Total ground truth P peaks: {groundtruth_p_peaks}\")\n",
    "print(f\"Total ground truth S peaks: {groundtruth_s_peaks}\")\n",
    "\n",
    "# Display the counts of residuals under 0.6 seconds\n",
    "print(f\"Total P-phase residuals under 0.6s: {count_residuals_p_under_0_6}\")\n",
    "print(f\"Total S-phase residuals under 0.6s: {count_residuals_s_under_0_6}\")\n",
    "\n",
    "# Plot the histogram of residual P peak arrival times\n",
    "plt.figure(figsize=(10, 5))\n",
    "counts_p, bins_p, patches_p = plt.hist(all_residual_p_arrival_times, bins=30, color='skyblue', edgecolor='black', range=(-1, 1))\n",
    "\n",
    "# Add labels for the number counts on each column\n",
    "for count, bin_, patch in zip(counts_p, bins_p, patches_p):\n",
    "    plt.text(bin_ + (bins_p[1] - bins_p[0]) / 2, count, f'{int(count)}', ha='center', va='bottom')\n",
    "\n",
    "# Print total pick count, mean, and standard deviation\n",
    "total_picks_p = len(all_residual_p_arrival_times)\n",
    "mean_p = np.mean(all_residual_p_arrival_times)\n",
    "std_p = np.std(all_residual_p_arrival_times)\n",
    "plt.text(0.95, 0.95, f'Total Picks: {total_picks_p}', ha='right', va='top', transform=plt.gca().transAxes)\n",
    "print(f\"P-phase Residuals: Mean = {mean_p:.4f}, Std = {std_p:.4f}\")\n",
    "print(f\"Total detected P picks: {total_picks_p}\")\n",
    "\n",
    "plt.title('Histogram of Residual P Peak Arrival Times')\n",
    "plt.xlabel('Residual P Arrival Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram.png\")\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot the histogram of residual S peak arrival times\n",
    "plt.figure(figsize=(10, 5))\n",
    "counts_s, bins_s, patches_s = plt.hist(all_residual_s_arrival_times, bins=30, color='salmon', edgecolor='grey', range=(-1, 1))\n",
    "\n",
    "# Add labels for the number counts on each column\n",
    "for count, bin_, patch in zip(counts_s, bins_s, patches_s):\n",
    "    plt.text(bin_ + (bins_s[1] - bins_s[0]) / 2, count, f'{int(count)}', ha='center', va='bottom')\n",
    "\n",
    "# Print total pick count, mean, and standard deviation\n",
    "total_picks_s = len(all_residual_s_arrival_times)\n",
    "mean_s = np.mean(all_residual_s_arrival_times)\n",
    "std_s = np.std(all_residual_s_arrival_times)\n",
    "plt.text(0.95, 0.95, f'Total Picks: {total_picks_s}', ha='right', va='top', transform=plt.gca().transAxes)\n",
    "print(f\"S-phase Residuals: Mean = {mean_s:.4f}, Std = {std_s:.4f}\")\n",
    "print(f\"Total detected S picks: {total_picks_s}\")\n",
    "\n",
    "plt.title('Histogram of Residual S Peak Arrival Times')\n",
    "plt.xlabel('Residual S Arrival Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram.png\")\n",
    "#plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294eefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "# Parameters\n",
    "x_min, x_max = -1, 1\n",
    "bins = np.linspace(x_min, x_max, 31)  # 30 bins between -1 and 1\n",
    "\n",
    "# Custom colors\n",
    "p_color = '#15616d'\n",
    "s_color = '#ff7d00'\n",
    "shading_color = '#d3d3d3'  # light gray for shaded success zone\n",
    "\n",
    "# === P-phase residuals ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_residual_p_arrival_times, bins=bins, kde=False, color=p_color, edgecolor='black', stat='count')\n",
    "\n",
    "# Shaded ±0.6s zone\n",
    "plt.axvspan(-0.6, 0.6, color=shading_color, alpha=0.3, label='Residual < 0.6s')\n",
    "\n",
    "# Reference lines\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(-0.6, color='gray', linestyle=':', linewidth=1)\n",
    "plt.axvline(0.6, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# Stats and annotation\n",
    "mean_p = np.mean(all_residual_p_arrival_times)\n",
    "std_p = np.std(all_residual_p_arrival_times)\n",
    "total_picks_p = len(all_residual_p_arrival_times)\n",
    "fraction_p = count_residuals_p_under_0_6 / groundtruth_p_peaks\n",
    "\n",
    "plt.text(x_min + 0.02, plt.gca().get_ylim()[1]*0.95,\n",
    "         f'Total Picks: {total_picks_p}\\nMean: {mean_p:.3f}s\\nStd: {std_p:.3f}s\\nUnder 0.6s: {count_residuals_p_under_0_6}/{groundtruth_p_peaks} ({fraction_p:.1%})',\n",
    "         ha='left', va='top', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\"))\n",
    "\n",
    "plt.title('Residual P Peak Arrival Times', fontsize=16)\n",
    "plt.xlabel('Residual P Arrival Time (s)', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram_shaded.png\")\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_p_histogram_shaded.ps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === S-phase residuals ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(all_residual_s_arrival_times, bins=bins, kde=False, color=s_color, edgecolor='black', stat='count')\n",
    "\n",
    "# Shaded ±0.6s zone\n",
    "plt.axvspan(-0.6, 0.6, color=shading_color, alpha=0.3, label='Residual < 0.6s')\n",
    "\n",
    "# Reference lines\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(-0.6, color='gray', linestyle=':', linewidth=1)\n",
    "plt.axvline(0.6, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# Stats and annotation\n",
    "mean_s = np.mean(all_residual_s_arrival_times)\n",
    "std_s = np.std(all_residual_s_arrival_times)\n",
    "total_picks_s = len(all_residual_s_arrival_times)\n",
    "fraction_s = count_residuals_s_under_0_6 / groundtruth_s_peaks\n",
    "\n",
    "plt.text(x_min + 0.02, plt.gca().get_ylim()[1]*0.95,\n",
    "         f'Total Picks: {total_picks_s}\\nMean: {mean_s:.3f}s\\nStd: {std_s:.3f}s\\nUnder 0.6s: {count_residuals_s_under_0_6}/{groundtruth_s_peaks} ({fraction_s:.1%})',\n",
    "         ha='left', va='top', fontsize=10,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\"))\n",
    "\n",
    "plt.title('Residual S Peak Arrival Times', fontsize=16)\n",
    "plt.xlabel('Residual S Arrival Time (s)', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram_shaded.png\")\n",
    "plt.savefig(f\"{model_name}_sf{sample_fraction}_residual_s_histogram_shaded.ps\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
