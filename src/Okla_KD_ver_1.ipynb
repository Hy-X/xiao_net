{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a73e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Distillation Training for XiaoNet\n",
    "Teacher: PhaseNet (from STEAD)\n",
    "Student: XiaoNet (v2, v3, v4, or v5)\n",
    "Dataset: OKLA regional seismic data\n",
    "\"\"\"\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seismology & SeisBench\n",
    "import obspy\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path for importing local modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# XiaoNet modules\n",
    "from models.xn_xiao_net_v2 import XiaoNet as XiaoNetV2\n",
    "from models.xn_xiao_net_v3 import XiaoNet as XiaoNetV3\n",
    "from models.xn_xiao_net_v4 import XiaoNetFast as XiaoNetV4\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge as XiaoNetV5\n",
    "from loss.xn_distillation_loss import DistillationLoss\n",
    "from xn_utils import set_seed, setup_device\n",
    "from xn_early_stopping import EarlyStopping\n",
    "\n",
    "print(\"✓ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c646aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = setup_device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22312606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /Users/hongyuxiao/Hongyu_File/xiao_net/config.json\n",
      "{\n",
      "  \"peak_detection\": {\n",
      "    \"sampling_rate\": 100,\n",
      "    \"height\": 0.5,\n",
      "    \"distance\": 100\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"dataset_name\": \"OKLA_1Mil_120s_Ver_3\",\n",
      "    \"sampling_rate\": 100,\n",
      "    \"window_len\": 3001,\n",
      "    \"samples_before\": 3000,\n",
      "    \"windowlen_large\": 6000,\n",
      "    \"sample_fraction\": 0.1\n",
      "  },\n",
      "  \"data_filter\": {\n",
      "    \"min_magnitude\": 1.0,\n",
      "    \"max_magnitude\": 2.0\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 64,\n",
      "    \"num_workers\": 4,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"epochs\": 50,\n",
      "    \"patience\": 5,\n",
      "    \"loss_weights\": [\n",
      "      0.01,\n",
      "      0.4,\n",
      "      0.59\n",
      "    ],\n",
      "    \"optimization\": {\n",
      "      \"mixed_precision\": true,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"pin_memory\": true,\n",
      "      \"prefetch_factor\": 2,\n",
      "      \"persistent_workers\": true\n",
      "    }\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"use_cuda\": true,\n",
      "    \"device_id\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "config_path = Path.cwd().parent / \"config.json\"\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration from: {config_path}\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a55f45a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available PhaseNet pretrained models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['diting',\n",
       " 'ethz',\n",
       " 'geofon',\n",
       " 'instance',\n",
       " 'iquique',\n",
       " 'jma',\n",
       " 'jma_wc',\n",
       " 'lendb',\n",
       " 'neic',\n",
       " 'obs',\n",
       " 'original',\n",
       " 'phasenet_sn',\n",
       " 'pisdl',\n",
       " 'scedc',\n",
       " 'stead',\n",
       " 'volpick']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PhaseNet teacher model (pretrained on STEAD)\n",
    "print(\"Available PhaseNet pretrained models:\")\n",
    "sbm.PhaseNet.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50f1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PhaseNet teacher model...\n",
      "\n",
      "✓ PhaseNet teacher loaded successfully!\n",
      "Total parameters: 268,443\n",
      "Trainable parameters: 268,443\n",
      "Model on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading PhaseNet teacher model...\")\n",
    "model = sbm.PhaseNet.from_pretrained(\"stead\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode for teacher\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ PhaseNet teacher loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "887fb320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OKLA regional seismic dataset...\n",
      "Sampling 10.0% of data for faster training...\n",
      "Sampled dataset size: 113,884\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "Training samples: 79,670\n",
      "Validation samples: 17,090\n",
      "Test samples: 17,124\n",
      "Total samples: 113,884\n"
     ]
    }
   ],
   "source": [
    "# Load OKLA dataset\n",
    "print(\"Loading OKLA regional seismic dataset...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n",
    "# Optional: Use subset for faster experimentation\n",
    "sample_fraction = config.get('data', {}).get('sample_fraction', 0.1)\n",
    "if sample_fraction < 1.0:\n",
    "    print(f\"Sampling {sample_fraction*100}% of data for faster training...\")\n",
    "    # Create a random mask for sampling\n",
    "    mask = np.random.random(len(data)) < sample_fraction\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"Sampled dataset size: {len(data):,}\")\n",
    "\n",
    "# Split into train/dev/test\n",
    "train, dev, test = data.train_dev_test()\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train):,}\")\n",
    "print(f\"Validation samples: {len(dev):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Total samples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaa20617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying magnitude filters: 1.0 < M < 2.0\n",
      "✓ [Data Filter]: Start - magnitude > 1.0\n",
      "✓ [Data Filter]: Applied - magnitude > 1.0, remaining samples: 108,944\n",
      "✓ [Data Filter]: Start - magnitude < 2.0\n",
      "✓ [Data Filter]: Applied - magnitude < 2.0, remaining samples: 36,880\n",
      "\n",
      "✓ Magnitude filtering complete: 36,880 traces in range [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# Magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', 1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 2.0)\n",
    "\n",
    "print(f\"Applying magnitude filters: {min_magnitude} < M < {max_magnitude}\")\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude > {min_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"✓ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask, inplace=True)\n",
    "    print(f\"✓ [Data Filter]: Applied - magnitude < {max_magnitude}, remaining samples: {len(data):,}\")\n",
    "except Exception as exc:\n",
    "    print(\"✗ [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Magnitude filtering complete: {len(data):,} traces in range [{min_magnitude}, {max_magnitude}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78d3b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total dataset size: 36,880\n",
      "Train size: 79,670\n",
      "Validation size: 17,090\n",
      "Test size: 17,124\n",
      "Sampling rate: 100 Hz\n",
      "Window length: 3001 samples\n",
      "Magnitude stats: min=1.00, max=2.00, mean=1.53\n",
      "Metadata columns: ['index', 'station_network_code', 'station_code', 'trace_channel', 'station_latitude_deg', 'station_longitude_deg', 'station_elevation_m', 'trace_p_arrival_sample', 'trace_p_status', 'trace_p_weight', 'path_p_travel_sec', 'trace_s_arrival_sample', 'trace_s_status', 'trace_s_weight', 'source_id', 'source_origin_time', 'source_origin_uncertainty_sec', 'source_latitude_deg', 'source_longitude_deg', 'source_error_sec', 'source_gap_deg', 'source_horizontal_uncertainty_km', 'source_depth_km', 'source_depth_uncertainty_km', 'source_magnitude', 'source_magnitude_type', 'source_magnitude_author', 'source_mechanism_strike_dip_rake', 'source_distance_deg', 'source_distance_km', 'path_back_azimuth_deg', 'trace_snr_db', 'trace_coda_end_sample', 'trace_start_time', 'trace_category', 'trace_name', 'split', 'trace_name_original', 'trace_chunk', 'trace_sampling_rate_hz', 'trace_component_order']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary for training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core sizes\n",
    "print(f\"Total dataset size: {len(data):,}\")\n",
    "print(f\"Train size: {len(train):,}\")\n",
    "print(f\"Validation size: {len(dev):,}\")\n",
    "print(f\"Test size: {len(test):,}\")\n",
    "\n",
    "# Sampling configuration\n",
    "sampling_rate = config.get('data', {}).get('sampling_rate', 'unknown')\n",
    "window_len = config.get('data', {}).get('window_len', 'unknown')\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"Window length: {window_len} samples\")\n",
    "\n",
    "# Metadata summary (if available)\n",
    "if hasattr(data, 'metadata') and data.metadata is not None:\n",
    "    if 'source_magnitude' in data.metadata:\n",
    "        mags = data.metadata['source_magnitude']\n",
    "        print(f\"Magnitude stats: min={mags.min():.2f}, max={mags.max():.2f}, mean={mags.mean():.2f}\")\n",
    "    print(f\"Metadata columns: {list(data.metadata.columns)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab888c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2672e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81187cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b64fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
