{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ee661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import seisbench.models as sbm\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "\n",
    "from seisbench.util import worker_seeding\n",
    "from torch.utils.data import DataLoader\n",
    "from obspy.clients.fdsn import Client\n",
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253c316c-2d22-4da1-b955-32a5e31dcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7482716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from JSON file\n",
    "try:\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"âœ“ Configuration loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âœ— Error: config.json file not found!\")\n",
    "    raise\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"âœ— Error: Invalid JSON in config.json: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Unexpected error loading config: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cac5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False, seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# CUDA initialization and reproducible randomization\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}, seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f080377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Loader the picker\n",
    "try:\n",
    "    model = sbm.PhaseNet.from_pretrained(\"stead\")\n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c820e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhaseNet(\n",
       "  (inc): Conv1d(3, 8, kernel_size=(7,), stride=(1,), padding=same)\n",
       "  (in_bn): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (down_branch): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(8, 8, kernel_size=(7,), stride=(4,), padding=(3,), bias=False)\n",
       "      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): Conv1d(8, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(16, 16, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(32, 32, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): Conv1d(32, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(64, 64, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ModuleList(\n",
       "      (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2-3): 2 x None\n",
       "    )\n",
       "  )\n",
       "  (up_branch): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ConvTranspose1d(128, 64, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ConvTranspose1d(64, 32, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ConvTranspose1d(32, 16, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ConvTranspose1d(16, 8, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(16, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Conv1d(8, 3, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(f\"cuda:{config['device']['device_id']}\" if torch.cuda.is_available() and config['device']['use_cuda'] else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986b17c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhaseNet information:\n",
      "Total parameters: 268,443\n",
      "Trainable parameters: 268,443\n",
      "Model size: 1.02 MB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Print PhaseNet model information\n",
    "phasenet_total_params = sum(p.numel() for p in model.parameters())\n",
    "phasenet_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"PhaseNet information:\")\n",
    "print(f\"Total parameters: {phasenet_total_params:,}\")\n",
    "print(f\"Trainable parameters: {phasenet_trainable_params:,}\")\n",
    "print(f\"Model size: {phasenet_total_params * 4 / (1024**2):.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from models.xn_xiao_net_v5 import XiaoNetEdge\n",
    "\n",
    "# Load XiaoNetEdge with base_channels=8\n",
    "xiaonet_edge = XiaoNetEdge(window_len=3001, in_channels=3, num_phases=3, base_channels=8)\n",
    "xiaonet_edge.to(device)\n",
    "\n",
    "# Count parameters\n",
    "xiaonet_edge_total_params = sum(p.numel() for p in xiaonet_edge.parameters())\n",
    "xiaonet_edge_trainable_params = sum(p.numel() for p in xiaonet_edge.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"XiaoNetEdge Model Architecture:\")\n",
    "print(xiaonet_edge)\n",
    "print(f\"\\nTotal Parameters: {xiaonet_edge_total_params:,}\")\n",
    "print(f\"Trainable Parameters: {xiaonet_edge_trainable_params:,}\")\n",
    "print(f\"\\nParameter Reduction vs PhaseNet: {(1 - xiaonet_edge_total_params/phasenet_total_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f94ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "âœ“ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "    print(\"âœ“ Data loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f44060f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating random sample of 1.0% of the data...\n"
     ]
    }
   ],
   "source": [
    "# Create a random sample\n",
    "sample_fraction = 0.01  # Sample 20% of the data\n",
    "print(f\"Creating random sample of {sample_fraction*100}% of the data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ce4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 11270\n"
     ]
    }
   ],
   "source": [
    "# Create a random mask for sampling\n",
    "mask = np.random.random(len(data)) < sample_fraction\n",
    "data.filter(mask)\n",
    "print(f\"Sampled dataset size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7c3549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ [Data Filter]: Start - magnitude > 1.0\n",
      "âœ“ [Data Filter]: Applied - magnitude > 1.0\n",
      "âœ“ [Data Filter]: Start - magnitude < 2.0\n",
      "âœ“ [Data Filter]: Applied - magnitude < 2.0\n"
     ]
    }
   ],
   "source": [
    "# magnitude filtering (with defaults)\n",
    "min_magnitude = config.get('data_filter', {}).get('min_magnitude', -1.0)\n",
    "max_magnitude = config.get('data_filter', {}).get('max_magnitude', 10.0)\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude above the minimum\n",
    "    print(f\"âœ“ [Data Filter]: Start - magnitude > {min_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] > min_magnitude\n",
    "    data.filter(mask)\n",
    "    print(f\"âœ“ [Data Filter]: Applied - magnitude > {min_magnitude}\")\n",
    "except Exception as exc:\n",
    "    print(\"âœ— [Data Filter]: Error - Failed to apply minimum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Filter events with magnitude below the maximum\n",
    "    print(f\"âœ“ [Data Filter]: Start - magnitude < {max_magnitude}\")\n",
    "    mask = data.metadata[\"source_magnitude\"] < max_magnitude\n",
    "    data.filter(mask)\n",
    "    print(f\"âœ“ [Data Filter]: Applied - magnitude < {max_magnitude}\")\n",
    "except Exception as exc:\n",
    "    print(\"âœ— [Data Filter]: Error - Failed to apply maximum magnitude filter.\")\n",
    "    print(f\"  Details: {exc}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed9bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 3723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Sampled dataset size: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935662b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, dev, test = data.train_dev_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d00a4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: OKLA_1Mil_120s_Ver_3 - 2629 traces\n",
      "Dev: OKLA_1Mil_120s_Ver_3 - 531 traces\n",
      "Test: OKLA_1Mil_120s_Ver_3 - 563 traces\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Dev:\", dev)\n",
    "print(\"Test:\", test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b87844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caeb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bce0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c576a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ [Peak Detection]: sampling_rate=100 Hz, height=0.5, distance=100\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection (with defaults)\n",
    "sampling_rate = config.get('peak_detection', {}).get('sampling_rate', 100)\n",
    "height = config.get('peak_detection', {}).get('height', 0.5)\n",
    "distance = config.get('peak_detection', {}).get('distance', 100)\n",
    "\n",
    "print(f\"âœ“ [Peak Detection]: sampling_rate={sampling_rate} Hz, height={height}, distance={distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45694b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ [DataLoader]: batch_size=64, num_workers=4\n"
     ]
    }
   ],
   "source": [
    "# Parameters for peak detection\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']\n",
    "print(f\"âœ“ [DataLoader]: batch_size={batch_size}, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d542203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION SANITY CHECK\n",
      "============================================================\n",
      "\n",
      "- Dataset Information:\n",
      "  Total dataset size: 3,723 samples\n",
      "  Training set: 2,629 samples\n",
      "  Validation set: 531 samples\n",
      "  Test set: 563 samples\n",
      "  Sample fraction used: 1.0%\n",
      "  Device: cpu\n",
      "\n",
      "- Training Hyperparameters:\n",
      "  Batch size: 64\n",
      "  Number of workers: 4\n",
      "  Learning rate: 0.01\n",
      "  Number of epochs: 50\n",
      "  Patience (early stopping): 5\n",
      "\n",
      "- Peak Detection Parameters:\n",
      "  Sampling rate: 100 Hz\n",
      "  Height threshold: 0.5\n",
      "  Distance: 100 samples\n",
      "\n",
      "============================================================\n",
      "Ready to start training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n- Dataset Information:\")\n",
    "print(f\"  Total dataset size: {len(data):,} samples\")\n",
    "print(f\"  Training set: {len(train):,} samples\")\n",
    "print(f\"  Validation set: {len(dev):,} samples\")\n",
    "print(f\"  Test set: {len(test):,} samples\")\n",
    "print(f\"  Sample fraction used: {sample_fraction*100}%\")\n",
    "\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(\"\\n- Training Hyperparameters:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of workers: {num_workers}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Number of epochs: {config['training']['epochs']}\")\n",
    "print(f\"  Patience (early stopping): {config['training']['patience']}\")\n",
    "\n",
    "print(\"\\n- Peak Detection Parameters:\")\n",
    "print(f\"  Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"  Height threshold: {height}\")\n",
    "print(f\"  Distance: {distance} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ready to start training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17af6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb36bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-8):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfcd77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71b49c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, checkpoint_path='checkpoint.pt', \n",
    "                 best_model_path='best_model.pth', final_model_path='final_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_model_path = best_model_path\n",
    "        self.final_model_path = final_model_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.save_best_model(model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                self.save_final_model(model)\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.save_best_model(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.checkpoint_path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def save_best_model(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Saving best model to {self.best_model_path}')\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "\n",
    "    def save_final_model(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Early stopping triggered. Saving final model to {self.final_model_path}')\n",
    "        torch.save(model.state_dict(), self.final_model_path)\n",
    "\n",
    "# Function to train for one epoch\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 5 == 0:\n",
    "            print(f\"loss: {loss.item():>7f}  [{batch_id * len(batch['X']):>5d}/{size:>5d}]\")\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.fill_between(range(len(history['train_loss'])), \n",
    "                     history['train_loss'], history['val_loss'],\n",
    "                     alpha=0.3, color='red', \n",
    "                     where=(np.array(history['val_loss']) > np.array(history['train_loss'])),\n",
    "                     label='Potential Overfitting Gap')\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "# Training routine with EarlyStopping and scheduler\n",
    "def train_model(train_loader, val_loader, model, optimizer, loss_fn, device, num_epochs=25, patience=7):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} results: Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    plot_training_history(history)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.348694  [    0/ 2629]\n",
      "loss: 0.228015  [  320/ 2629]\n",
      "loss: 0.180008  [  640/ 2629]\n",
      "loss: 0.137867  [  960/ 2629]\n",
      "loss: 0.115066  [ 1280/ 2629]\n",
      "loss: 0.130399  [ 1600/ 2629]\n",
      "loss: 0.105908  [ 1920/ 2629]\n",
      "loss: 0.097156  [ 2240/ 2629]\n",
      "loss: 0.102794  [ 2560/ 2629]\n",
      "Epoch 1 results: Train loss: 0.155222, Val loss: 0.101815\n",
      "Validation loss decreased (inf --> 0.101815). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 2/50\n",
      "loss: 0.105260  [    0/ 2629]\n",
      "loss: 0.104225  [  320/ 2629]\n",
      "loss: 0.081832  [  640/ 2629]\n",
      "loss: 0.089860  [  960/ 2629]\n",
      "loss: 0.085066  [ 1280/ 2629]\n",
      "loss: 0.097162  [ 1600/ 2629]\n",
      "loss: 0.097987  [ 1920/ 2629]\n",
      "loss: 0.088550  [ 2240/ 2629]\n",
      "loss: 0.093276  [ 2560/ 2629]\n",
      "Epoch 2 results: Train loss: 0.095750, Val loss: 0.087829\n",
      "Validation loss decreased (0.101815 --> 0.087829). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 3/50\n",
      "loss: 0.083393  [    0/ 2629]\n",
      "loss: 0.089655  [  320/ 2629]\n",
      "loss: 0.095329  [  640/ 2629]\n",
      "loss: 0.091790  [  960/ 2629]\n",
      "loss: 0.083331  [ 1280/ 2629]\n",
      "loss: 0.077989  [ 1600/ 2629]\n",
      "loss: 0.080797  [ 1920/ 2629]\n",
      "loss: 0.103529  [ 2240/ 2629]\n",
      "loss: 0.093974  [ 2560/ 2629]\n",
      "Epoch 3 results: Train loss: 0.087945, Val loss: 0.084093\n",
      "Validation loss decreased (0.087829 --> 0.084093). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 4/50\n",
      "loss: 0.089452  [    0/ 2629]\n",
      "loss: 0.086517  [  320/ 2629]\n",
      "loss: 0.091711  [  640/ 2629]\n",
      "loss: 0.088753  [  960/ 2629]\n",
      "loss: 0.083592  [ 1280/ 2629]\n",
      "loss: 0.085370  [ 1600/ 2629]\n",
      "loss: 0.086757  [ 1920/ 2629]\n",
      "loss: 0.081474  [ 2240/ 2629]\n",
      "loss: 0.082346  [ 2560/ 2629]\n",
      "Epoch 4 results: Train loss: 0.085256, Val loss: 0.082142\n",
      "Validation loss decreased (0.084093 --> 0.082142). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 5/50\n",
      "loss: 0.090106  [    0/ 2629]\n",
      "loss: 0.086824  [  320/ 2629]\n",
      "loss: 0.076515  [  640/ 2629]\n",
      "loss: 0.093820  [  960/ 2629]\n",
      "loss: 0.077213  [ 1280/ 2629]\n",
      "loss: 0.079559  [ 1600/ 2629]\n",
      "loss: 0.078846  [ 1920/ 2629]\n",
      "loss: 0.074537  [ 2240/ 2629]\n",
      "loss: 0.071863  [ 2560/ 2629]\n",
      "Epoch 5 results: Train loss: 0.083218, Val loss: 0.081045\n",
      "Validation loss decreased (0.082142 --> 0.081045). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 6/50\n",
      "loss: 0.085051  [    0/ 2629]\n",
      "loss: 0.079747  [  320/ 2629]\n",
      "loss: 0.075212  [  640/ 2629]\n",
      "loss: 0.076370  [  960/ 2629]\n",
      "loss: 0.077743  [ 1280/ 2629]\n",
      "loss: 0.082468  [ 1600/ 2629]\n",
      "loss: 0.085668  [ 1920/ 2629]\n",
      "loss: 0.067509  [ 2240/ 2629]\n",
      "loss: 0.073732  [ 2560/ 2629]\n",
      "Epoch 6 results: Train loss: 0.080262, Val loss: 0.080316\n",
      "Validation loss decreased (0.081045 --> 0.080316). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 7/50\n",
      "loss: 0.079991  [    0/ 2629]\n",
      "loss: 0.089143  [  320/ 2629]\n",
      "loss: 0.078048  [  640/ 2629]\n",
      "loss: 0.073006  [  960/ 2629]\n",
      "loss: 0.083826  [ 1280/ 2629]\n",
      "loss: 0.073340  [ 1600/ 2629]\n",
      "loss: 0.071866  [ 1920/ 2629]\n",
      "loss: 0.081808  [ 2240/ 2629]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Call the training function\n",
    "    patience = config['training']['patience'] if 'patience' in config['training'] else 7\n",
    "    trained_model, training_history = train_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        num_epochs=epochs,\n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = evaluate_model(test_loader, trained_model, loss_fn, device)\n",
    "    print(f\"Final test loss: {test_loss:.6f}\")\n",
    "    \n",
    "    print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495746e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45125d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276f6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f35bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8792dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765523b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35adae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c0f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb54ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5bc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db817b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ceba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-5):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3954e41-2285-45c6-b8cb-4e6d4822f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss_phase_picking(\n",
    "    student_pred, teacher_pred, y_true, \n",
    "    alpha=0.5, \n",
    "    peak_weight=0.2,\n",
    "    eps=1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Distillation loss for phase picking (softmax student, softmax teacher).\n",
    "    Optimized for peak detection without temperature softening.\n",
    "    \n",
    "    Args:\n",
    "        student_pred: Student predictions (after softmax) (B, C, L) - C=[noise, P, S]\n",
    "        teacher_pred: Teacher predictions (after softmax) (B, C, L)\n",
    "        y_true: Ground truth labels (B, C, L)\n",
    "        alpha: Weight for distillation vs task loss (0-1)\n",
    "        peak_weight: Weight for peak localization loss (0-1)\n",
    "        eps: Small epsilon for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss optimized for peak detection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle shape mismatch if needed\n",
    "    if student_pred.shape[-1] != teacher_pred.shape[-1]:\n",
    "        teacher_pred = F.interpolate(\n",
    "            teacher_pred, size=student_pred.shape[-1], mode='linear', align_corners=True\n",
    "        )\n",
    "        if y_true.shape[-1] != student_pred.shape[-1]:\n",
    "            y_true = F.interpolate(\n",
    "                y_true, size=student_pred.shape[-1], mode='linear', align_corners=True\n",
    "            )\n",
    "            y_true = y_true / (y_true.sum(dim=1, keepdim=True) + eps)\n",
    "    \n",
    "    # =============================================\n",
    "    # 1. Task Loss: Cross-entropy (student vs ground truth)\n",
    "    # =============================================\n",
    "    # Standard CE for softmax output\n",
    "    task_loss = -(y_true * torch.log(student_pred + eps)).sum(dim=1).mean()\n",
    "    \n",
    "    # =============================================\n",
    "    # 2. Distillation Loss: KL divergence (no temperature)\n",
    "    # =============================================\n",
    "    # Direct KL divergence - NO temperature to preserve peak sharpness\n",
    "    # KL(teacher || student) = sum(teacher * log(teacher/student))\n",
    "    distill_loss = (teacher_pred * torch.log((teacher_pred + eps) / (student_pred + eps))).sum(dim=1).mean()\n",
    "    \n",
    "    # =============================================\n",
    "    # 3. Peak Localization Loss (for phase picking)\n",
    "    # =============================================\n",
    "    peak_loss = torch.tensor(0.0, device=student_pred.device)\n",
    "    \n",
    "    if peak_weight > 0:\n",
    "        for phase_idx in [1, 2]:  # P and S phases\n",
    "            student_phase = student_pred[:, phase_idx, :]  # (B, L)\n",
    "            teacher_phase = teacher_pred[:, phase_idx, :]  # (B, L)\n",
    "            \n",
    "            # Peak location error (normalized)\n",
    "            student_peak_idx = student_phase.argmax(dim=-1)\n",
    "            teacher_peak_idx = teacher_phase.argmax(dim=-1)\n",
    "            seq_len = student_phase.shape[-1]\n",
    "            peak_loc_error = (student_peak_idx.float() - teacher_peak_idx.float()).abs() / seq_len\n",
    "            \n",
    "            # Peak amplitude matching at teacher's peak\n",
    "            batch_indices = torch.arange(student_phase.shape[0], device=student_pred.device)\n",
    "            student_at_teacher_peak = student_phase[batch_indices, teacher_peak_idx]\n",
    "            teacher_at_teacher_peak = teacher_phase[batch_indices, teacher_peak_idx]\n",
    "            peak_amp_error = (student_at_teacher_peak - teacher_at_teacher_peak).abs()\n",
    "            \n",
    "            peak_loss = peak_loss + peak_loc_error.mean() + peak_amp_error.mean()\n",
    "        \n",
    "        peak_loss = peak_loss / 2  # Average over P and S\n",
    "    \n",
    "    # =============================================\n",
    "    # Combined Loss\n",
    "    # =============================================\n",
    "    total_loss = (\n",
    "        (1 - alpha) * task_loss +\n",
    "        alpha * distill_loss +\n",
    "        peak_weight * peak_loss\n",
    "    )\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def distillation_loss_focal_softmax(\n",
    "    student_pred, teacher_pred, y_true,\n",
    "    alpha=0.5,\n",
    "    gamma=2.0,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Focal loss variant for softmax student - focuses on hard samples.\n",
    "    Good for imbalanced data (noise samples >> P/S arrivals).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle shape mismatch\n",
    "    if student_pred.shape[-1] != teacher_pred.shape[-1]:\n",
    "        teacher_pred = F.interpolate(\n",
    "            teacher_pred, size=student_pred.shape[-1], mode='linear', align_corners=True\n",
    "        )\n",
    "        if y_true.shape[-1] != student_pred.shape[-1]:\n",
    "            y_true = F.interpolate(y_true, size=student_pred.shape[-1], mode='linear', align_corners=True)\n",
    "            y_true = y_true / (y_true.sum(dim=1, keepdim=True) + eps)\n",
    "    \n",
    "    # Focal cross-entropy\n",
    "    pt = (student_pred * y_true).sum(dim=1)  # probability of true class\n",
    "    focal_weight = (1 - pt) ** gamma\n",
    "    ce = -(y_true * torch.log(student_pred + eps)).sum(dim=1)\n",
    "    task_loss = (focal_weight * ce).mean()\n",
    "    \n",
    "    # KL distillation (no temperature)\n",
    "    distill_loss = (teacher_pred * torch.log((teacher_pred + eps) / (student_pred + eps))).sum(dim=1).mean()\n",
    "    \n",
    "    total_loss = (1 - alpha) * task_loss + alpha * distill_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE PICKING DISTILLATION LOSSES (softmax student & teacher)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"1. distillation_loss_phase_picking() [RECOMMENDED]\")\n",
    "print(\"   - Cross-entropy task loss (softmax-compatible)\")\n",
    "print(\"   - KL divergence distillation (no temperature)\")\n",
    "print(\"   - Peak localization loss for P/S timing accuracy\")\n",
    "print()\n",
    "print(\"2. distillation_loss_focal_softmax()\")\n",
    "print(\"   - Focal loss: focuses on hard samples near phase arrivals\")\n",
    "print(\"   - Good for imbalanced noise/phase samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f750d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Teacher and Student Models for Knowledge Distillation\n",
    "print(\"=\" * 60)\n",
    "print(\"KNOWLEDGE DISTILLATION SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Teacher model: PhaseNet (pretrained, frozen)\n",
    "teacher_model = model  # PhaseNet loaded earlier\n",
    "teacher_model.eval()   # Set to evaluation mode\n",
    "\n",
    "# Freeze all teacher parameters - no training\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "teacher_trainable = sum(p.numel() for p in teacher_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nðŸ‘¨â€ðŸ« Teacher Model (PhaseNet):\")\n",
    "print(f\"  Total parameters: {teacher_params:,}\")\n",
    "print(f\"  Trainable parameters: {teacher_trainable:,} (frozen âœ“)\")\n",
    "print(f\"  Model size: {teacher_params * 4 / (1024**2):.2f} MB\")\n",
    "\n",
    "# Student model: XiaoNetEdge (will be trained)\n",
    "student_model = xiaonet_edge  # XiaoNetEdge created earlier\n",
    "student_model.train()      # Set to training mode\n",
    "\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "student_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nðŸ‘¨â€ðŸŽ“ Student Model (XiaoNetEdge):\")\n",
    "print(f\"  Total parameters: {student_params:,}\")\n",
    "print(f\"  Trainable parameters: {student_trainable:,}\")\n",
    "print(f\"  Model size: {student_params * 4 / (1024**2):.2f} MB\")\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison:\")\n",
    "print(f\"  Parameter reduction: {(1 - student_params/teacher_params)*100:.1f}%\")\n",
    "print(f\"  Size reduction: {(1 - (student_params * 4 / (1024**2)) / (teacher_params * 4 / (1024**2)))*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ Teacher-Student configuration complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a133ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer for STUDENT model only (teacher is frozen)\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"âœ“ Optimizer configured for student model\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Optimizing {student_trainable:,} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf55edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with knowledge distillation (phase picking optimized)\n",
    "def train_one_epoch_distillation(\n",
    "    dataloader, student_model, teacher_model, optimizer, device,\n",
    "    loss_fn=distillation_loss_phase_picking,\n",
    "    alpha=0.5,\n",
    "    peak_weight=0.2,\n",
    "    eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Train student model for one epoch using knowledge distillation.\n",
    "    Optimized for seismic phase picking.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Training data loader\n",
    "        student_model: Student model to train (XiaoNetEdge)\n",
    "        teacher_model: Teacher model (PhaseNet, frozen)\n",
    "        optimizer: Optimizer for student model\n",
    "        device: Device to train on\n",
    "        loss_fn: Loss function (distillation_loss_phase_picking or distillation_loss_focal_softmax)\n",
    "        alpha: Weight between distillation and task loss (0-1)\n",
    "        peak_weight: Weight for peak localization loss (0-1)\n",
    "        eps: Small epsilon for numerical stability (1e-8)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (total_loss, task_loss, distill_loss, peak_loss) averaged over epoch\n",
    "    \"\"\"\n",
    "    student_model.train()\n",
    "    teacher_model.eval()  # Teacher always in eval mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    task_loss_sum = 0.0\n",
    "    distill_loss_sum = 0.0\n",
    "    peak_loss_sum = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y_true = batch[\"y\"].to(device)\n",
    "        \n",
    "        # =============================================\n",
    "        # Forward Pass\n",
    "        # =============================================\n",
    "        student_pred = student_model(X)\n",
    "        \n",
    "        with torch.no_grad():  # No gradients for teacher\n",
    "            teacher_pred = teacher_model(X)\n",
    "        \n",
    "        # =============================================\n",
    "        # Loss Calculation\n",
    "        # =============================================\n",
    "        loss = loss_fn(\n",
    "            student_pred, teacher_pred, y_true,\n",
    "            alpha=alpha,\n",
    "            peak_weight=peak_weight,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        # =============================================\n",
    "        # Calculate Individual Loss Components (for monitoring)\n",
    "        # =============================================\n",
    "        with torch.no_grad():\n",
    "            # 1. Task Loss (CE between student and ground truth)\n",
    "            task_loss = -(y_true * torch.log(student_pred + eps)).sum(dim=1).mean()\n",
    "            \n",
    "            # 2. Distillation Loss (KL divergence, no temperature)\n",
    "            distill_loss = (teacher_pred * torch.log((teacher_pred + eps) / (student_pred + eps))).sum(dim=1).mean()\n",
    "            \n",
    "            # 3. Peak Localization Loss\n",
    "            peak_loss = torch.tensor(0.0, device=device)\n",
    "            if peak_weight > 0:\n",
    "                for phase_idx in [1, 2]:  # P and S phases\n",
    "                    student_phase = student_pred[:, phase_idx, :]\n",
    "                    teacher_phase = teacher_pred[:, phase_idx, :]\n",
    "                    \n",
    "                    student_peak_idx = student_phase.argmax(dim=-1)\n",
    "                    teacher_peak_idx = teacher_phase.argmax(dim=-1)\n",
    "                    seq_len = student_phase.shape[-1]\n",
    "                    \n",
    "                    peak_loc_error = (student_peak_idx.float() - teacher_peak_idx.float()).abs() / seq_len\n",
    "                    \n",
    "                    batch_indices = torch.arange(student_phase.shape[0], device=device)\n",
    "                    student_at_teacher_peak = student_phase[batch_indices, teacher_peak_idx]\n",
    "                    teacher_at_teacher_peak = teacher_phase[batch_indices, teacher_peak_idx]\n",
    "                    peak_amp_error = (student_at_teacher_peak - teacher_at_teacher_peak).abs()\n",
    "                    \n",
    "                    peak_loss = peak_loss + peak_loc_error.mean() + peak_amp_error.mean()\n",
    "                \n",
    "                peak_loss = peak_loss / 2\n",
    "        \n",
    "        # =============================================\n",
    "        # Backward Pass and Optimization\n",
    "        # =============================================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # =============================================\n",
    "        # Loss Tracking\n",
    "        # =============================================\n",
    "        total_loss += loss.item()\n",
    "        task_loss_sum += task_loss.item()\n",
    "        distill_loss_sum += distill_loss.item()\n",
    "        peak_loss_sum += peak_loss.item()\n",
    "        \n",
    "        # =============================================\n",
    "        # Periodic Progress Logging\n",
    "        # =============================================\n",
    "        if batch_id % 5 == 0:\n",
    "            current_samples = batch_id * len(X)\n",
    "            print(f\"[Epoch Progress] {current_samples:>5d}/{size:>5d} | \"\n",
    "                  f\"Total: {loss.item():.6f} | \"\n",
    "                  f\"Task: {task_loss.item():.6f} | \"\n",
    "                  f\"Distill: {distill_loss.item():.6f} | \"\n",
    "                  f\"Peak: {peak_loss.item():.6f}\")\n",
    "    \n",
    "    # =============================================\n",
    "    # Return Epoch Averages\n",
    "    # =============================================\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_task_loss = task_loss_sum / num_batches\n",
    "    avg_distill_loss = distill_loss_sum / num_batches\n",
    "    avg_peak_loss = peak_loss_sum / num_batches\n",
    "    \n",
    "    print(f\"\\n[Epoch Summary]\")\n",
    "    print(f\"  Total Loss:        {avg_total_loss:.6f}\")\n",
    "    print(f\"  Task Loss (CE):    {avg_task_loss:.6f} (weight: {1-alpha:.1f})\")\n",
    "    print(f\"  Distill Loss (KL): {avg_distill_loss:.6f} (weight: {alpha:.1f})\")\n",
    "    print(f\"  Peak Loss:         {avg_peak_loss:.6f} (weight: {peak_weight:.1f})\")\n",
    "    print()\n",
    "    \n",
    "    return avg_total_loss, avg_task_loss, avg_distill_loss, avg_peak_loss\n",
    "\n",
    "\n",
    "# Evaluation function for student model\n",
    "def evaluate_student_model(dataloader, student_model, device, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate student model on validation/test set.\n",
    "    Uses cross-entropy loss (no teacher involved).\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation/test data loader\n",
    "        student_model: Student model to evaluate\n",
    "        device: Device to evaluate on\n",
    "        eps: Small epsilon for numerical stability (1e-8)\n",
    "    \n",
    "    Returns:\n",
    "        Average validation loss\n",
    "    \"\"\"\n",
    "    student_model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y_true = batch[\"y\"].to(device)\n",
    "            \n",
    "            student_pred = student_model(X)\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            loss = -(y_true * torch.log(student_pred + eps)).sum(dim=1).mean()\n",
    "            val_loss_sum += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss_sum / num_batches\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING FUNCTIONS (Distillation for Phase Picking)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"âœ“ train_one_epoch_distillation()\")\n",
    "print(\"  - Uses phase-picking optimized loss function\")\n",
    "print(\"  - Tracks 4 loss components: Total, Task, Distill, Peak\")\n",
    "print(\"  - eps=1e-8 for numerical stability\")\n",
    "print(\"  - Detailed loss breakdown at each epoch\")\n",
    "print()\n",
    "print(\"âœ“ evaluate_student_model()\")\n",
    "print(\"  - Evaluates student independently from teacher\")\n",
    "print(\"  - Uses cross-entropy loss\")\n",
    "print()\n",
    "print(\"Loss Function Parameters:\")\n",
    "print(\"  - alpha: Weight for distillation vs task loss (default 0.5)\")\n",
    "print(\"  - peak_weight: Weight for peak localization (default 0.2)\")\n",
    "print(\"  - eps: Numerical stability (fixed at 1e-8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000d5c5-617b-482d-86fc-5c412c0d69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingEnhanced:\n",
    "    \"\"\"\n",
    "    Enhanced Early Stopping with detailed tracking for phase picking.\n",
    "    \n",
    "    Features:\n",
    "    - Tracks multiple metrics (val_loss, task_loss, distill_loss, peak_loss)\n",
    "    - Shows improvement percentage for better interpretability\n",
    "    - Maintains loss history for trend analysis\n",
    "    - Robust device handling for model saving/loading\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 patience=7, \n",
    "                 verbose=True, \n",
    "                 delta=0,\n",
    "                 relative_delta=False,\n",
    "                 best_model_path='best_student_model.pth',\n",
    "                 track_metrics=['total_loss', 'task_loss', 'distill_loss']):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: How many epochs without improvement before stopping\n",
    "            verbose: If True, prints detailed messages\n",
    "            delta: Absolute minimum change to qualify as improvement\n",
    "            relative_delta: If True, delta is relative (e.g., 0.01 = 1% improvement)\n",
    "            best_model_path: Path to save best model\n",
    "            track_metrics: List of metrics to track (for logging/analysis)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.relative_delta = relative_delta\n",
    "        self.best_model_path = best_model_path\n",
    "        self.track_metrics = track_metrics\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        # New: Track history for trend analysis\n",
    "        self.loss_history = []\n",
    "        self.best_loss_history = []\n",
    "        self.improvement_history = []\n",
    "    \n",
    "    def __call__(self, val_loss, model, epoch, optimizer=None, metrics_dict=None):\n",
    "        \"\"\"\n",
    "        Check if early stopping should trigger.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss (primary metric)\n",
    "            model: Model to potentially save\n",
    "            epoch: Current epoch number\n",
    "            optimizer: (Optional) Optimizer state to save\n",
    "            metrics_dict: (Optional) Dict with additional metrics {metric_name: value}\n",
    "        \n",
    "        Returns:\n",
    "            Boolean indicating if early stopping was triggered\n",
    "        \"\"\"\n",
    "        score = -val_loss  # Negative because lower loss is better\n",
    "        \n",
    "        # Store loss history\n",
    "        self.loss_history.append(val_loss)\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            # First epoch - always save\n",
    "            self.best_score = score\n",
    "            self.best_loss_history.append(val_loss)\n",
    "            self.improvement_history.append(0.0)\n",
    "            self._save_checkpoint(val_loss, model, epoch, optimizer, metrics_dict)\n",
    "            if self.verbose:\n",
    "                print(f\"[Epoch {epoch:3d}] Initial best val_loss: {val_loss:.6f}\")\n",
    "        \n",
    "        else:\n",
    "            # Calculate improvement\n",
    "            improvement = (self.val_loss_min - val_loss) / self.val_loss_min * 100 if self.val_loss_min > 0 else 0\n",
    "            \n",
    "            # Check if this is an improvement\n",
    "            if self.relative_delta:\n",
    "                # Relative improvement threshold\n",
    "                threshold = self.val_loss_min * (1 - self.delta)\n",
    "                is_improvement = val_loss < threshold\n",
    "            else:\n",
    "                # Absolute improvement threshold\n",
    "                is_improvement = score > self.best_score + self.delta\n",
    "            \n",
    "            if not is_improvement:\n",
    "                # No improvement\n",
    "                self.counter += 1\n",
    "                status = \"âŒ No improvement\"\n",
    "                if self.verbose:\n",
    "                    print(f\"[Epoch {epoch:3d}] {status} | \"\n",
    "                          f\"val_loss: {val_loss:.6f} | \"\n",
    "                          f\"patience: {self.counter}/{self.patience}\")\n",
    "                \n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n{'='*70}\")\n",
    "                        print(f\"ðŸ›‘ EARLY STOPPING TRIGGERED!\")\n",
    "                        print(f\"   Best model from epoch {self.best_epoch} with val_loss: {self.val_loss_min:.6f}\")\n",
    "                        print(f\"   Waited {self.patience} epochs without improvement\")\n",
    "                        print(f\"{'='*70}\\n\")\n",
    "                    return True\n",
    "            \n",
    "            else:\n",
    "                # Improvement found\n",
    "                self.best_score = score\n",
    "                self.best_loss_history.append(val_loss)\n",
    "                self.improvement_history.append(improvement)\n",
    "                self._save_checkpoint(val_loss, model, epoch, optimizer, metrics_dict)\n",
    "                self.counter = 0\n",
    "                \n",
    "                status = \"âœ“ Improvement!\"\n",
    "                if self.verbose:\n",
    "                    print(f\"[Epoch {epoch:3d}] {status} | \"\n",
    "                          f\"val_loss: {val_loss:.6f} | \"\n",
    "                          f\"improvement: {improvement:+.2f}% | \"\n",
    "                          f\"patience reset: {self.counter}/{self.patience}\")\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    # ...existing code in EarlyStoppingEnhanced class...\n",
    "\n",
    "    def _save_checkpoint(self, val_loss, model, epoch, optimizer=None, metrics_dict=None):\n",
    "        \"\"\"Save model checkpoint with metadata.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }\n",
    "        \n",
    "        # Save optional metrics\n",
    "        if metrics_dict is not None:\n",
    "            checkpoint['metrics'] = metrics_dict\n",
    "        \n",
    "        # Save optimizer state for resuming\n",
    "        if optimizer is not None:\n",
    "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        \n",
    "        # Save best model (overwrites)\n",
    "        torch.save(checkpoint, self.best_model_path)\n",
    "        \n",
    "        # NEW: Also save epoch-specific checkpoint\n",
    "        epoch_path = self.best_model_path.replace('.pth', f'_epoch_{epoch}.pth')\n",
    "        torch.save(checkpoint, epoch_path)\n",
    "        print(f\"  ðŸ’¾ Saved checkpoint: {epoch_path}\")\n",
    "        \n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_epoch = epoch\n",
    "\n",
    "    def load_best_model(self, model, device, optimizer=None):\n",
    "        \"\"\"\n",
    "        Load the best saved model.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load weights into\n",
    "            device: Device to load model to\n",
    "            optimizer: (Optional) Optimizer to restore state\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (best_epoch, best_val_loss)\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(self.best_model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        \n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        best_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['val_loss']\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"âœ“ Loaded best model from epoch {best_epoch}\")\n",
    "            print(f\"  Validation loss: {best_val_loss:.6f}\")\n",
    "            if 'metrics' in checkpoint:\n",
    "                print(f\"  Metrics: {checkpoint['metrics']}\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return best_epoch, best_val_loss\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Return summary statistics of training.\"\"\"\n",
    "        if len(self.loss_history) == 0:\n",
    "            return \"No training history\"\n",
    "        \n",
    "        return {\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'best_val_loss': self.val_loss_min,\n",
    "            'final_val_loss': self.loss_history[-1],\n",
    "            'num_epochs': len(self.loss_history),\n",
    "            'num_improvements': len(self.best_loss_history),\n",
    "            'improvement_pct': (self.loss_history[0] - self.val_loss_min) / self.loss_history[0] * 100 if self.loss_history[0] > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ENHANCED EARLY STOPPING FOR PHASE PICKING\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"âœ“ EarlyStoppingEnhanced class features:\")\n",
    "print(\"  - Tracks improvement percentage for better interpretability\")\n",
    "print(\"  - Supports relative delta (% improvement) in addition to absolute\")\n",
    "print(\"  - Maintains loss history for trend analysis\")\n",
    "print(\"  - Detailed epoch-by-epoch logging with visual indicators\")\n",
    "print(\"  - Robust device handling for save/load\")\n",
    "print(\"  - Summary statistics via get_summary()\")\n",
    "print()\n",
    "print(\"Usage differences from standard early stopping:\")\n",
    "print(\"  - Call with: early_stop(val_loss, model, epoch, optimizer, metrics_dict)\")\n",
    "print(\"  - metrics_dict = {'task_loss': 0.045, 'distill_loss': 0.060, ...}\")\n",
    "print(\"  - Check early_stop.early_stop flag to trigger training stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c3485-3fee-45dd-991a-b94b3ad8d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"\n",
    "    Comprehensive plotting of all training losses for knowledge distillation.\n",
    "    Uses separate panels for non-comparable losses.\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary with keys:\n",
    "            - 'train_loss': Total training loss (required)\n",
    "            - 'train_task_loss': Task loss (CE vs ground truth)\n",
    "            - 'train_distill_loss': Distillation loss (KL vs teacher)\n",
    "            - 'train_peak_loss': Peak localization loss\n",
    "            - 'val_loss': Validation loss (CE only, no teacher)\n",
    "            - 'p_f1', 's_f1', 'noise_f1': Per-class F1 scores (optional)\n",
    "        save_path: Where to save the figure\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Check what data is available\n",
    "    has_task_loss = 'train_task_loss' in history and len(history.get('train_task_loss', [])) > 0\n",
    "    has_distill_loss = 'train_distill_loss' in history and len(history.get('train_distill_loss', [])) > 0\n",
    "    has_peak_loss = 'train_peak_loss' in history and len(history.get('train_peak_loss', [])) > 0\n",
    "    has_f1_scores = any(k in history and len(history.get(k, [])) > 0 for k in ['p_f1', 's_f1', 'noise_f1'])\n",
    "    \n",
    "    # Create figure with 3x2 layout for comprehensive visualization\n",
    "    fig = plt.figure(figsize=(18, 14))\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 1: Total Training Loss vs Validation Loss (COMPARABLE)\n",
    "    # =============================================\n",
    "    ax1 = fig.add_subplot(3, 2, 1)\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Total Loss', \n",
    "             color='blue', linewidth=2, marker='o', markersize=3)\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss (CE)', \n",
    "             color='red', linewidth=2, marker='s', markersize=3)\n",
    "    \n",
    "    # Highlight overfitting region\n",
    "    train_arr = np.array(history['train_loss'])\n",
    "    val_arr = np.array(history['val_loss'])\n",
    "    ax1.fill_between(epochs, train_arr, val_arr,\n",
    "                     alpha=0.2, color='red',\n",
    "                     where=(val_arr > train_arr),\n",
    "                     label='Overfitting Region')\n",
    "    \n",
    "    # Mark best validation loss\n",
    "    best_epoch = np.argmin(val_arr) + 1\n",
    "    best_val = val_arr[best_epoch - 1]\n",
    "    ax1.axvline(x=best_epoch, color='green', linestyle=':', alpha=0.7)\n",
    "    ax1.plot(best_epoch, best_val, 'g*', markersize=15, label=f'Best Val (Epoch {best_epoch})')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=11)\n",
    "    ax1.set_ylabel('Loss', fontsize=11)\n",
    "    ax1.set_title('Panel 1: Total Train vs Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 2: Task Loss Only (Train vs Val - COMPARABLE)\n",
    "    # =============================================\n",
    "    ax2 = fig.add_subplot(3, 2, 2)\n",
    "    \n",
    "    if has_task_loss:\n",
    "        ax2.plot(epochs, history['train_task_loss'], label='Train Task Loss (CE)', \n",
    "                 color='green', linewidth=2, marker='o', markersize=3)\n",
    "        ax2.plot(epochs, history['val_loss'], label='Val Loss (CE)', \n",
    "                 color='red', linewidth=2, linestyle='--', marker='s', markersize=3)\n",
    "        \n",
    "        task_arr = np.array(history['train_task_loss'])\n",
    "        ax2.fill_between(epochs, task_arr, val_arr,\n",
    "                        alpha=0.2, color='orange',\n",
    "                        where=(val_arr > task_arr))\n",
    "        \n",
    "        # Add text box\n",
    "        ax2.text(0.95, 0.95, \n",
    "                f'Latest:\\nTask: {history[\"train_task_loss\"][-1]:.4f}\\nVal: {history[\"val_loss\"][-1]:.4f}', \n",
    "                transform=ax2.transAxes, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8), fontsize=9)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Task Loss Not Tracked', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('Cross-Entropy Loss', fontsize=11)\n",
    "    ax2.set_title('Panel 2: Task Loss (Comparable - same scale)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=9)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 3: Distillation Loss (SEPARATE - not comparable to CE)\n",
    "    # =============================================\n",
    "    ax3 = fig.add_subplot(3, 2, 3)\n",
    "    \n",
    "    if has_distill_loss:\n",
    "        ax3.plot(epochs, history['train_distill_loss'], label='Distillation Loss (KL)', \n",
    "                 color='orange', linewidth=2, marker='D', markersize=4)\n",
    "        ax3.fill_between(epochs, history['train_distill_loss'], alpha=0.3, color='orange')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(list(epochs), history['train_distill_loss'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax3.plot(epochs, p(list(epochs)), 'r--', alpha=0.5, label='Trend')\n",
    "        \n",
    "        # Add text box\n",
    "        ax3.text(0.95, 0.95, \n",
    "                f'KL Divergence\\n(Student vs Teacher)\\n\\nLatest: {history[\"train_distill_loss\"][-1]:.4f}', \n",
    "                transform=ax3.transAxes, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=9)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Distillation Loss Not Tracked', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    ax3.set_xlabel('Epoch', fontsize=11)\n",
    "    ax3.set_ylabel('KL Divergence', fontsize=11)\n",
    "    ax3.set_title('Panel 3: Distillation Loss (Separate Scale - KL)', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(loc='best', fontsize=9)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 4: Peak Localization Loss (SEPARATE - different unit)\n",
    "    # =============================================\n",
    "    ax4 = fig.add_subplot(3, 2, 4)\n",
    "    \n",
    "    if has_peak_loss:\n",
    "        ax4.plot(epochs, history['train_peak_loss'], label='Peak Loss', \n",
    "                 color='purple', linewidth=2, marker='^', markersize=4)\n",
    "        ax4.fill_between(epochs, history['train_peak_loss'], alpha=0.3, color='purple')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(list(epochs), history['train_peak_loss'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax4.plot(epochs, p(list(epochs)), 'r--', alpha=0.5, label='Trend')\n",
    "        \n",
    "        # Add text box\n",
    "        ax4.text(0.95, 0.95, \n",
    "                f'Peak Localization\\n(P/S timing accuracy)\\n\\nLatest: {history[\"train_peak_loss\"][-1]:.4f}', \n",
    "                transform=ax4.transAxes, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.8), fontsize=9)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Peak Loss Not Tracked', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    ax4.set_xlabel('Epoch', fontsize=11)\n",
    "    ax4.set_ylabel('Peak Loss (Position + Amplitude)', fontsize=11)\n",
    "    ax4.set_title('Panel 4: Peak Localization Loss (Separate Scale)', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(loc='best', fontsize=9)\n",
    "    ax4.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 5: F1 Scores (Different metric - accuracy based)\n",
    "    # =============================================\n",
    "    ax5 = fig.add_subplot(3, 2, 5)\n",
    "    \n",
    "    if has_f1_scores:\n",
    "        if 'p_f1' in history and len(history.get('p_f1', [])) > 0:\n",
    "            ax5.plot(epochs, history['p_f1'], label='P-wave F1', \n",
    "                    color='blue', linewidth=2, marker='o', markersize=3)\n",
    "        if 's_f1' in history and len(history.get('s_f1', [])) > 0:\n",
    "            ax5.plot(epochs, history['s_f1'], label='S-wave F1', \n",
    "                    color='red', linewidth=2, marker='s', markersize=3)\n",
    "        if 'noise_f1' in history and len(history.get('noise_f1', [])) > 0:\n",
    "            ax5.plot(epochs, history['noise_f1'], label='Noise F1', \n",
    "                    color='gray', linewidth=2, marker='^', markersize=3)\n",
    "        \n",
    "        ax5.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Target (0.8)')\n",
    "        ax5.set_ylim([0, 1.05])\n",
    "        \n",
    "        # Add text box with final F1 scores\n",
    "        f1_text = 'Final F1 Scores:'\n",
    "        if 'p_f1' in history and len(history.get('p_f1', [])) > 0:\n",
    "            f1_text += f'\\n  P: {history[\"p_f1\"][-1]:.3f}'\n",
    "        if 's_f1' in history and len(history.get('s_f1', [])) > 0:\n",
    "            f1_text += f'\\n  S: {history[\"s_f1\"][-1]:.3f}'\n",
    "        if 'noise_f1' in history and len(history.get('noise_f1', [])) > 0:\n",
    "            f1_text += f'\\n  Noise: {history[\"noise_f1\"][-1]:.3f}'\n",
    "        ax5.text(0.95, 0.05, f1_text, \n",
    "                transform=ax5.transAxes, verticalalignment='bottom', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8), fontsize=9)\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'F1 Scores Not Tracked', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    ax5.set_xlabel('Epoch', fontsize=11)\n",
    "    ax5.set_ylabel('F1 Score', fontsize=11)\n",
    "    ax5.set_title('Panel 5: Per-Class F1 Scores (Accuracy Metric)', fontsize=12, fontweight='bold')\n",
    "    ax5.legend(loc='best', fontsize=9)\n",
    "    ax5.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Panel 6: Generalization Gap & Loss Component Ratios\n",
    "    # =============================================\n",
    "    ax6 = fig.add_subplot(3, 2, 6)\n",
    "    \n",
    "    # Plot generalization gap\n",
    "    gap = val_arr - train_arr\n",
    "    ax6_twin = ax6.twinx()\n",
    "    \n",
    "    ax6.plot(epochs, gap, label='Generalization Gap (Val - Train)', \n",
    "            color='purple', linewidth=2, marker='o', markersize=3)\n",
    "    ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax6.fill_between(epochs, 0, gap, alpha=0.3, color='purple', where=(gap > 0))\n",
    "    \n",
    "    # If we have component losses, plot their ratios\n",
    "    if has_task_loss and has_distill_loss:\n",
    "        task_arr = np.array(history['train_task_loss'])\n",
    "        distill_arr = np.array(history['train_distill_loss'])\n",
    "        ratio = distill_arr / (task_arr + 1e-8)\n",
    "        ax6_twin.plot(epochs, ratio, label='Distill/Task Ratio', \n",
    "                     color='orange', linewidth=2, linestyle='--', marker='s', markersize=3)\n",
    "        ax6_twin.set_ylabel('Loss Ratio (Distill/Task)', fontsize=11, color='orange')\n",
    "        ax6_twin.tick_params(axis='y', labelcolor='orange')\n",
    "        ax6_twin.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    # Overfitting assessment\n",
    "    avg_gap = np.mean(gap[len(gap)//2:]) if len(gap) > 1 else gap[0]\n",
    "    status = 'Good' if avg_gap < 0.01 else 'Moderate' if avg_gap < 0.05 else 'High'\n",
    "    color = 'green' if status == 'Good' else 'orange' if status == 'Moderate' else 'red'\n",
    "    ax6.text(0.05, 0.95, f'Overfitting: {status}\\nAvg Gap: {avg_gap:.4f}', \n",
    "            transform=ax6.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor=color, alpha=0.3), fontsize=9)\n",
    "    \n",
    "    ax6.set_xlabel('Epoch', fontsize=11)\n",
    "    ax6.set_ylabel('Generalization Gap', fontsize=11, color='purple')\n",
    "    ax6.tick_params(axis='y', labelcolor='purple')\n",
    "    ax6.set_title('Panel 6: Generalization Gap & Loss Ratios', fontsize=12, fontweight='bold')\n",
    "    ax6.legend(loc='upper left', fontsize=9)\n",
    "    ax6.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # =============================================\n",
    "    # Final Layout & Save\n",
    "    # =============================================\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Training history plot saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # =============================================\n",
    "    # Print Detailed Summary\n",
    "    # =============================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE TRAINING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nðŸ“Š LOSS OVERVIEW:\")\n",
    "    print(f\"   Best Validation Loss: {best_val:.6f} at Epoch {best_epoch}\")\n",
    "    print(f\"   Final Train Total Loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"   Final Validation Loss: {history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    if has_task_loss:\n",
    "        print(f\"\\nðŸ“ˆ TASK LOSS (Cross-Entropy vs Ground Truth):\")\n",
    "        print(f\"   Final: {history['train_task_loss'][-1]:.6f}\")\n",
    "        print(f\"   Min:   {min(history['train_task_loss']):.6f}\")\n",
    "    \n",
    "    if has_distill_loss:\n",
    "        print(f\"\\nðŸ“ˆ DISTILLATION LOSS (KL Divergence vs Teacher):\")\n",
    "        print(f\"   Final: {history['train_distill_loss'][-1]:.6f}\")\n",
    "        print(f\"   Min:   {min(history['train_distill_loss']):.6f}\")\n",
    "    \n",
    "    if has_peak_loss:\n",
    "        print(f\"\\nðŸ“ˆ PEAK LOCALIZATION LOSS:\")\n",
    "        print(f\"   Final: {history['train_peak_loss'][-1]:.6f}\")\n",
    "        print(f\"   Min:   {min(history['train_peak_loss']):.6f}\")\n",
    "    \n",
    "    if has_f1_scores:\n",
    "        print(f\"\\nðŸ“Š FINAL F1 SCORES:\")\n",
    "        if 'p_f1' in history and len(history.get('p_f1', [])) > 0:\n",
    "            print(f\"   P-wave:  {history['p_f1'][-1]:.4f}\")\n",
    "        if 's_f1' in history and len(history.get('s_f1', [])) > 0:\n",
    "            print(f\"   S-wave:  {history['s_f1'][-1]:.4f}\")\n",
    "        if 'noise_f1' in history and len(history.get('noise_f1', [])) > 0:\n",
    "            print(f\"   Noise:   {history['noise_f1'][-1]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ GENERALIZATION:\")\n",
    "    print(f\"   Final Gap (Val-Train): {gap[-1]:.6f}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "def init_history():\n",
    "    \"\"\"Initialize training history dictionary with all tracked losses.\"\"\"\n",
    "    return {\n",
    "        # Required losses\n",
    "        'train_loss': [],          # Total combined loss\n",
    "        'val_loss': [],            # Validation CE loss\n",
    "        \n",
    "        # Component losses (from distillation_loss_phase_picking)\n",
    "        'train_task_loss': [],     # CE vs ground truth\n",
    "        'train_distill_loss': [],  # KL vs teacher\n",
    "        'train_peak_loss': [],     # Peak localization\n",
    "        \n",
    "        # Optional metrics\n",
    "        'p_f1': [],\n",
    "        's_f1': [],\n",
    "        'noise_f1': []\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE PLOTTING FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"âœ“ plot_training_history() - 6-panel visualization:\")\n",
    "print(\"   Panel 1: Total Train vs Validation Loss (comparable)\")\n",
    "print(\"   Panel 2: Task Loss Train vs Val (comparable - same CE scale)\")\n",
    "print(\"   Panel 3: Distillation Loss (separate - KL scale)\")\n",
    "print(\"   Panel 4: Peak Localization Loss (separate - position scale)\")\n",
    "print(\"   Panel 5: Per-Class F1 Scores (accuracy metric)\")\n",
    "print(\"   Panel 6: Generalization Gap & Loss Ratios\")\n",
    "print()\n",
    "print(\"âœ“ init_history() - Initialize history dict for tracking:\")\n",
    "print(\"   - train_loss, val_loss\")\n",
    "print(\"   - train_task_loss, train_distill_loss, train_peak_loss\")\n",
    "print(\"   - p_f1, s_f1, noise_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331e906-c6a2-4f18-89a5-83063adb281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Training routine with EarlyStoppingEnhanced for Knowledge Distillation\n",
    "def train_model_distillation(\n",
    "    train_loader, val_loader, student_model, teacher_model, \n",
    "    optimizer, device, \n",
    "    num_epochs=25, \n",
    "    patience=7, \n",
    "    alpha=0.5,           # Weight for distillation loss\n",
    "    peak_weight=0.2,     # Weight for peak localization loss\n",
    "    eps=1e-8,            # Numerical stability\n",
    "    best_model_path='best_student_distilled.pth',\n",
    "    plot_path='student_distillation_history.png',\n",
    "    loss_fn=distillation_loss_phase_picking  # Use phase picking optimized loss\n",
    "):\n",
    "    \"\"\"\n",
    "    Train student model using knowledge distillation optimized for phase picking.\n",
    "    Uses updated loss function and EarlyStoppingEnhanced.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        student_model: Student model to train (XiaoNetEdge)\n",
    "        teacher_model: Teacher model (PhaseNet, frozen)\n",
    "        optimizer: Optimizer for student model\n",
    "        device: Device to train on\n",
    "        num_epochs: Maximum number of epochs\n",
    "        patience: EarlyStoppingEnhanced patience\n",
    "        alpha: Weight for distillation vs task loss (0-1)\n",
    "        peak_weight: Weight for peak localization loss\n",
    "        eps: Numerical stability constant (1e-8)\n",
    "        best_model_path: Path to save best model\n",
    "        plot_path: Path to save training history plot\n",
    "        loss_fn: Loss function (distillation_loss_phase_picking or distillation_loss_focal_softmax)\n",
    "    \n",
    "    Returns:\n",
    "        student_model: Trained student model (with best weights loaded)\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    # =============================================\n",
    "    # Initialize EarlyStoppingEnhanced\n",
    "    # =============================================\n",
    "    early_stopping = EarlyStoppingEnhanced(\n",
    "        patience=patience, \n",
    "        verbose=True,\n",
    "        delta=0.0001,           # Minimum absolute improvement threshold\n",
    "        relative_delta=False,   # Use absolute delta\n",
    "        best_model_path=best_model_path,\n",
    "        track_metrics=['task_loss', 'distill_loss', 'peak_loss']\n",
    "    )\n",
    "    \n",
    "    # =============================================\n",
    "    # Learning Rate Scheduler\n",
    "    # =============================================\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # =============================================\n",
    "    # Initialize History Tracking\n",
    "    # =============================================\n",
    "    history = init_history()\n",
    "    \n",
    "    # =============================================\n",
    "    # Freeze Teacher Model\n",
    "    # =============================================\n",
    "    teacher_model.eval()\n",
    "    for param in teacher_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # =============================================\n",
    "    # Print Training Configuration\n",
    "    # =============================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KNOWLEDGE DISTILLATION TRAINING (Phase Picking Optimized)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nðŸ“‹ Configuration:\")\n",
    "    print(f\"   Loss Function:    {loss_fn.__name__}\")\n",
    "    print(f\"   Alpha (distill):  {alpha}\")\n",
    "    print(f\"   Peak Weight:      {peak_weight}\")\n",
    "    print(f\"   Epsilon:          {eps}\")\n",
    "    print(f\"   Max Epochs:       {num_epochs}\")\n",
    "    print(f\"   Patience:         {patience}\")\n",
    "    print(f\"   Initial LR:       {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    print(f\"   Device:           {device}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # =============================================\n",
    "    # Training Loop\n",
    "    # =============================================\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Training Phase (with distillation)\n",
    "        # -----------------------------------------\n",
    "        train_total, train_task, train_distill, train_peak = train_one_epoch_distillation(\n",
    "            dataloader=train_loader,\n",
    "            student_model=student_model,\n",
    "            teacher_model=teacher_model,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            alpha=alpha,\n",
    "            peak_weight=peak_weight,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Validation Phase (student only)\n",
    "        # -----------------------------------------\n",
    "        val_loss = evaluate_student_model(val_loader, student_model, device, eps=eps)\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Record History\n",
    "        # -----------------------------------------\n",
    "        history['train_loss'].append(train_total)\n",
    "        history['train_task_loss'].append(train_task)\n",
    "        history['train_distill_loss'].append(train_distill)\n",
    "        history['train_peak_loss'].append(train_peak)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Optional: Calculate F1 scores if function exists\n",
    "        # try:\n",
    "        #     f1_scores = calculate_f1_scores(student_model, val_loader, device)\n",
    "        #     history['p_f1'].append(f1_scores['p'])\n",
    "        #     history['s_f1'].append(f1_scores['s'])\n",
    "        #     history['noise_f1'].append(f1_scores['noise'])\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Epoch Summary\n",
    "        # -----------------------------------------\n",
    "        print(f\"\\nðŸ“Š Epoch {epoch+1} Results:\")\n",
    "        print(f\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "        print(f\"   â”‚ Train Total Loss:    {train_total:>12.6f} â”‚\")\n",
    "        print(f\"   â”‚   â”œâ”€ Task Loss (CE):  {train_task:>12.6f} â”‚ (weight: {1-alpha:.1f})\")\n",
    "        print(f\"   â”‚   â”œâ”€ Distill Loss:    {train_distill:>12.6f} â”‚ (weight: {alpha:.1f})\")\n",
    "        print(f\"   â”‚   â””â”€ Peak Loss:       {train_peak:>12.6f} â”‚ (weight: {peak_weight:.1f})\")\n",
    "        print(f\"   â”‚ Val Loss (CE):        {val_loss:>12.6f} â”‚\")\n",
    "        print(f\"   â”‚ Learning Rate:        {optimizer.param_groups[0]['lr']:>12.2e} â”‚\")\n",
    "        print(f\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Step Scheduler\n",
    "        # -----------------------------------------\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Early Stopping Check\n",
    "        # -----------------------------------------\n",
    "        metrics_dict = {\n",
    "            'task_loss': train_task,\n",
    "            'distill_loss': train_distill,\n",
    "            'peak_loss': train_peak\n",
    "        }\n",
    "        \n",
    "        should_stop = early_stopping(\n",
    "            val_loss=val_loss,\n",
    "            model=student_model,\n",
    "            epoch=epoch + 1,\n",
    "            optimizer=optimizer,\n",
    "            metrics_dict=metrics_dict\n",
    "        )\n",
    "        \n",
    "        if should_stop:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ›‘ EARLY STOPPING TRIGGERED\")\n",
    "            print(f\"   Best model saved at epoch {early_stopping.best_epoch}\")\n",
    "            print(f\"   Best validation loss: {early_stopping.val_loss_min:.6f}\")\n",
    "            print(\"=\" * 70)\n",
    "            break\n",
    "    \n",
    "    # =============================================\n",
    "    # Load Best Model\n",
    "    # =============================================\n",
    "    print(\"\\nðŸ“¥ Loading best model weights...\")\n",
    "    best_epoch, best_val_loss = early_stopping.load_best_model(student_model, device, optimizer)\n",
    "    \n",
    "    # =============================================\n",
    "    # Save Training History to JSON\n",
    "    # =============================================\n",
    "    history_path = best_model_path.replace('.pth', '_history.json')\n",
    "    history_serializable = {k: [float(v) for v in vals] for k, vals in history.items() if len(vals) > 0}\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history_serializable, f, indent=2)\n",
    "    print(f\"âœ“ Training history saved to: {history_path}\")\n",
    "    \n",
    "    # =============================================\n",
    "    # Plot Training History\n",
    "    # =============================================\n",
    "    print(\"\\nðŸ“ˆ Generating training plots...\")\n",
    "    plot_training_history(history, save_path=plot_path)\n",
    "    \n",
    "    # =============================================\n",
    "    # Print Early Stopping Summary\n",
    "    # =============================================\n",
    "    summary = early_stopping.get_summary()\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ“‹ EARLY STOPPING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return student_model, history\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Main Execution\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KNOWLEDGE DISTILLATION SETUP\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get configuration parameters (with phase-picking optimized defaults)\n",
    "    patience = config['training'].get('patience', 10)\n",
    "    alpha = config['training'].get('alpha', 0.5)        # Balanced task/distill\n",
    "    peak_weight = config['training'].get('peak_weight', 0.2)  # Peak localization\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Training Configuration:\")\n",
    "    print(f\"   Epochs:       {epochs}\")\n",
    "    print(f\"   Patience:     {patience}\")\n",
    "    print(f\"   Alpha:        {alpha}\")\n",
    "    print(f\"   Peak Weight:  {peak_weight}\")\n",
    "    print(f\"   Device:       {device}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify teacher model\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "    teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "    print(f\"\\nâœ“ Teacher model (PhaseNet) ready\")\n",
    "    print(f\"   Parameters: {teacher_params:,}\")\n",
    "    \n",
    "    # Verify student model\n",
    "    student_model.to(device)\n",
    "    student_params = sum(p.numel() for p in student_model.parameters())\n",
    "    print(f\"\\nâœ“ Student model (XiaoNetEdge) ready\")\n",
    "    print(f\"   Parameters: {student_params:,}\")\n",
    "    print(f\"   Compression: {teacher_params / student_params:.1f}x smaller\")\n",
    "    \n",
    "    # Run training\n",
    "    try:\n",
    "        trained_student, training_history = train_model_distillation(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            student_model=student_model,\n",
    "            teacher_model=teacher_model,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=epochs,\n",
    "            patience=patience,\n",
    "            alpha=alpha,\n",
    "            peak_weight=peak_weight,\n",
    "            eps=1e-8,\n",
    "            best_model_path='best_student_distilled.pth',\n",
    "            plot_path='student_distillation_history.png',\n",
    "            loss_fn=distillation_loss_phase_picking\n",
    "        )\n",
    "        \n",
    "        # =============================================\n",
    "        # Final Evaluation on Test Set\n",
    "        # =============================================\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ðŸ“Š FINAL EVALUATION ON TEST SET\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        student_test_loss = evaluate_student_model(test_loader, trained_student, device, eps=1e-8)\n",
    "        teacher_test_loss = evaluate_student_model(test_loader, teacher_model, device, eps=1e-8)\n",
    "        \n",
    "        print(f\"\\n   Teacher (PhaseNet) Test Loss:    {teacher_test_loss:.6f}\")\n",
    "        print(f\"   Student (XiaoNetEdge) Test Loss: {student_test_loss:.6f}\")\n",
    "        print(f\"   Performance Gap:                 {student_test_loss - teacher_test_loss:+.6f}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        gap_pct = (student_test_loss - teacher_test_loss) / teacher_test_loss * 100\n",
    "        if gap_pct < 5:\n",
    "            status = \"ðŸŽ‰ EXCELLENT - Student matches teacher!\"\n",
    "        elif gap_pct < 10:\n",
    "            status = \"âœ“ GOOD - Student within 10% of teacher\"\n",
    "        elif gap_pct < 20:\n",
    "            status = \"âš ï¸ ACCEPTABLE - Student within 20% of teacher\"\n",
    "        else:\n",
    "            status = \"âŒ NEEDS IMPROVEMENT - Consider more training or larger model\"\n",
    "        \n",
    "        print(f\"\\n   Status: {status}\")\n",
    "        print(f\"   Gap: {gap_pct:+.1f}%\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ“ KNOWLEDGE DISTILLATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   ðŸ“ Best model:        best_student_distilled.pth\")\n",
    "        print(f\"   ðŸ“ Training history:  best_student_distilled_history.json\")\n",
    "        print(f\"   ðŸ“ Training plot:     student_distillation_history.png\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âŒ ERROR DURING TRAINING\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_model_residuals_with_time(model, data_generator, device, \n",
    "                                       height=0.5, distance=100, sampling_rate=100,\n",
    "                                       tolerance=0.6, n_samples=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate seismic phase picking model with residuals and measure inference time.\n",
    "    \n",
    "    Returns:\n",
    "        dict with residuals, counts, mean/std, tolerance counts, and avg inference time\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.signal import find_peaks\n",
    "    import torch\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if n_samples is None:\n",
    "        n_samples = len(data_generator)\n",
    "    \n",
    "    all_residual_p_arrival_times = []\n",
    "    all_residual_s_arrival_times = []\n",
    "    \n",
    "    groundtruth_p_peaks = 0\n",
    "    groundtruth_s_peaks = 0\n",
    "    count_residuals_p_under_tol = 0\n",
    "    count_residuals_s_under_tol = 0\n",
    "    inference_times = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        sample = data_generator[np.random.randint(len(data_generator))]\n",
    "        X_tensor = torch.tensor(sample[\"X\"], device=device).unsqueeze(0)\n",
    "\n",
    "        # Ground truth peaks\n",
    "        y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "        y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "        groundtruth_p_peaks += len(y_p_peaks)\n",
    "        groundtruth_s_peaks += len(y_s_peaks)\n",
    "        y_p_times = y_p_peaks / sampling_rate\n",
    "        y_s_times = y_s_peaks / sampling_rate\n",
    "\n",
    "        # Model prediction with timing\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor)[0].cpu().numpy()\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        # Predicted peaks\n",
    "        p_prob = pred[0]\n",
    "        s_prob = pred[1]\n",
    "        p_peaks, _ = find_peaks(p_prob, height=height, distance=distance)\n",
    "        s_peaks, _ = find_peaks(s_prob, height=height, distance=distance)\n",
    "        p_times = p_peaks / sampling_rate\n",
    "        s_times = s_peaks / sampling_rate\n",
    "\n",
    "        # Residuals: smallest absolute difference per ground truth peak\n",
    "        for y_p_time in y_p_times:\n",
    "            residuals = p_times - y_p_time\n",
    "            if len(residuals) > 0:\n",
    "                min_res = residuals[np.argmin(np.abs(residuals))]\n",
    "                all_residual_p_arrival_times.append(min_res)\n",
    "                if np.abs(min_res) < tolerance:\n",
    "                    count_residuals_p_under_tol += 1\n",
    "        for y_s_time in y_s_times:\n",
    "            residuals = s_times - y_s_time\n",
    "            if len(residuals) > 0:\n",
    "                min_res = residuals[np.argmin(np.abs(residuals))]\n",
    "                all_residual_s_arrival_times.append(min_res)\n",
    "                if np.abs(min_res) < tolerance:\n",
    "                    count_residuals_s_under_tol += 1\n",
    "\n",
    "    # Compute statistics\n",
    "    all_residual_p_arrival_times = np.array(all_residual_p_arrival_times)\n",
    "    all_residual_s_arrival_times = np.array(all_residual_s_arrival_times)\n",
    "    mean_p = np.mean(all_residual_p_arrival_times)\n",
    "    std_p = np.std(all_residual_p_arrival_times)\n",
    "    mean_s = np.mean(all_residual_s_arrival_times)\n",
    "    std_s = np.std(all_residual_s_arrival_times)\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    std_inference_time = np.std(inference_times)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== {model_name} Residual Evaluation ===\")\n",
    "    print(f\"Ground truth P peaks: {groundtruth_p_peaks}, S peaks: {groundtruth_s_peaks}\")\n",
    "    print(f\"P residuals under {tolerance}s: {count_residuals_p_under_tol}\")\n",
    "    print(f\"S residuals under {tolerance}s: {count_residuals_s_under_tol}\")\n",
    "    print(f\"P residuals: mean={mean_p:.4f}s, std={std_p:.4f}s, total={len(all_residual_p_arrival_times)}\")\n",
    "    print(f\"S residuals: mean={mean_s:.4f}s, std={std_s:.4f}s, total={len(all_residual_s_arrival_times)}\")\n",
    "    print(f\"Avg inference time per sample: {avg_inference_time*1000:.2f} ms Â± {std_inference_time*1000:.2f} ms\")\n",
    "\n",
    "    # Plot histograms\n",
    "    def plot_hist(residuals, phase_name, color, filename):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        counts, bins, patches = plt.hist(residuals, bins=30, color=color, edgecolor='black', range=(-1, 1))\n",
    "        for count, bin_, patch in zip(counts, bins, patches):\n",
    "            plt.text(bin_ + (bins[1]-bins[0])/2, count, f'{int(count)}', ha='center', va='bottom')\n",
    "        plt.text(0.95, 0.95, f'Total Picks: {len(residuals)}', ha='right', va='top', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_name} Residual {phase_name} Arrival Times')\n",
    "        plt.xlabel('Residual (s)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "    plot_hist(all_residual_p_arrival_times, 'P-phase', 'skyblue', f\"{model_name}_residual_p_hist.png\")\n",
    "    plot_hist(all_residual_s_arrival_times, 'S-phase', 'salmon', f\"{model_name}_residual_s_hist.png\")\n",
    "\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'residuals_p': all_residual_p_arrival_times,\n",
    "        'residuals_s': all_residual_s_arrival_times,\n",
    "        'count_p_under_tol': count_residuals_p_under_tol,\n",
    "        'count_s_under_tol': count_residuals_s_under_tol,\n",
    "        'mean_p': mean_p,\n",
    "        'std_p': std_p,\n",
    "        'mean_s': mean_s,\n",
    "        'std_s': std_s,\n",
    "        'groundtruth_p_peaks': groundtruth_p_peaks,\n",
    "        'groundtruth_s_peaks': groundtruth_s_peaks,\n",
    "        'avg_inference_time': avg_inference_time,\n",
    "        'std_inference_time': std_inference_time\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fe619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Evaluate Teacher Model\n",
    "# ------------------------------\n",
    "teacher_results = evaluate_model_residuals_with_time(\n",
    "    model=teacher_model,\n",
    "    data_generator=dev_generator,\n",
    "    device=device,\n",
    "    height=0.5,\n",
    "    distance=100,\n",
    "    sampling_rate=100,\n",
    "    tolerance=0.6,\n",
    "    n_samples=1000,  # or None for full dataset\n",
    "    model_name=\"Teacher (PhaseNet)\"\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate Student Model\n",
    "# ------------------------------\n",
    "student_results = evaluate_model_residuals_with_time(\n",
    "    model=student_model,\n",
    "    data_generator=dev_generator,\n",
    "    device=device,\n",
    "    height=0.5,\n",
    "    distance=100,\n",
    "    sampling_rate=100,\n",
    "    tolerance=0.6,\n",
    "    n_samples=1000,  # keep same number of samples for fair comparison\n",
    "    model_name=\"Student (XiaoNet)\"\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Speedup\n",
    "# ------------------------------\n",
    "speedup = teacher_results['avg_inference_time'] / student_results['avg_inference_time']\n",
    "\n",
    "# ------------------------------\n",
    "# Print Full Comparison Table\n",
    "# ------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Teacher vs Student Model Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':35} | {'Teacher':20} | {'Student':20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Ground truth P peaks':35} | {teacher_results['groundtruth_p_peaks']:20} | {student_results['groundtruth_p_peaks']:20}\")\n",
    "print(f\"{'Ground truth S peaks':35} | {teacher_results['groundtruth_s_peaks']:20} | {student_results['groundtruth_s_peaks']:20}\")\n",
    "print(f\"{'P residuals under 0.6s':35} | {teacher_results['count_p_under_tol']:20} | {student_results['count_p_under_tol']:20}\")\n",
    "print(f\"{'S residuals under 0.6s':35} | {teacher_results['count_s_under_tol']:20} | {student_results['count_s_under_tol']:20}\")\n",
    "print(f\"{'P residuals mean Â± std (s)':35} | {teacher_results['mean_p']:.4f} Â± {teacher_results['std_p']:.4f} | {student_results['mean_p']:.4f} Â± {student_results['std_p']:.4f}\")\n",
    "print(f\"{'S residuals mean Â± std (s)':35} | {teacher_results['mean_s']:.4f} Â± {teacher_results['std_s']:.4f} | {student_results['mean_s']:.4f} Â± {student_results['std_s']:.4f}\")\n",
    "print(f\"{'Avg inference time per sample (ms)':35} | {teacher_results['avg_inference_time']*1000:.2f} Â± {teacher_results['std_inference_time']*1000:.2f} | {student_results['avg_inference_time']*1000:.2f} Â± {student_results['std_inference_time']*1000:.2f}\")\n",
    "print(f\"{'Student vs Teacher Speedup':35} | {'-':20} | {speedup:.2f}x faster\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load model function\n",
    "def load_model(model_class, model_filename, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    Args:\n",
    "        model_class: Model class (e.g., sbm.PhaseNet or your custom XiaoNet)\n",
    "        model_filename: Filename of the saved model weights\n",
    "        device: Device to load the model onto\n",
    "        **kwargs: Additional arguments for model initialization\n",
    "    Returns:\n",
    "        Model with loaded weights\n",
    "    \"\"\"\n",
    "    model = model_class(**kwargs)\n",
    "    model.load_state_dict(torch.load(model_filename, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load your student model (adjust the class and parameters as needed)\n",
    "# If you have a custom student model:\n",
    "# from your_module import XiaoNet\n",
    "# student_model = load_model(XiaoNet, \"best_student_distilled.pth\", device)\n",
    "\n",
    "# Create output folders\n",
    "output_folder = \"comparison_examples\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Parameters for peak detection\n",
    "sampling_rate = 100  # Hz\n",
    "height = 0.5\n",
    "distance = 100\n",
    "\n",
    "print(f\"\\nGenerating {100} comparison examples...\")\n",
    "print(f\"Parameters: height={height}, distance={distance}, sampling_rate={sampling_rate}Hz\")\n",
    "\n",
    "# Generate comparison plots\n",
    "for i in range(1, 101):\n",
    "    # Get a sample\n",
    "    sample = dev_generator[np.random.randint(len(dev_generator))]\n",
    "    \n",
    "    # Find ground truth peaks\n",
    "    y_p_peaks, _ = find_peaks(sample[\"y\"][0], height=height, distance=distance)\n",
    "    y_s_peaks, _ = find_peaks(sample[\"y\"][1], height=height, distance=distance)\n",
    "    y_p_times = y_p_peaks / sampling_rate\n",
    "    y_s_times = y_s_peaks / sampling_rate\n",
    "    \n",
    "    # Get teacher predictions\n",
    "    with torch.no_grad():\n",
    "        teacher_pred = teacher_model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))\n",
    "        teacher_pred = teacher_pred[0].cpu().numpy()\n",
    "    \n",
    "    # Get student predictions\n",
    "    with torch.no_grad():\n",
    "        student_pred = student_model(torch.tensor(sample[\"X\"], device=device).unsqueeze(0))\n",
    "        student_pred = student_pred[0].cpu().numpy()\n",
    "    \n",
    "    # Find peaks in teacher predictions\n",
    "    teacher_p_prob = teacher_pred[0]\n",
    "    teacher_s_prob = teacher_pred[1]\n",
    "    teacher_p_peaks, _ = find_peaks(teacher_p_prob, height=height, distance=distance)\n",
    "    teacher_s_peaks, _ = find_peaks(teacher_s_prob, height=height, distance=distance)\n",
    "    teacher_p_times = teacher_p_peaks / sampling_rate\n",
    "    teacher_s_times = teacher_s_peaks / sampling_rate\n",
    "    \n",
    "    # Find peaks in student predictions\n",
    "    student_p_prob = student_pred[0]\n",
    "    student_s_prob = student_pred[1]\n",
    "    student_p_peaks, _ = find_peaks(student_p_prob, height=height, distance=distance)\n",
    "    student_s_peaks, _ = find_peaks(student_s_prob, height=height, distance=distance)\n",
    "    student_p_times = student_p_peaks / sampling_rate\n",
    "    student_s_times = student_s_peaks / sampling_rate\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(5, 2, hspace=0.3, wspace=0.3, height_ratios=[3, 1, 1, 1, 1])\n",
    "    \n",
    "    # Row 1: Input waveforms (shared)\n",
    "    ax_waveform = fig.add_subplot(gs[0, :])\n",
    "    ax_waveform.plot(sample[\"X\"].T, linewidth=0.8)\n",
    "    ax_waveform.set_title('Input Seismic Waveforms (3 channels)', fontsize=14, fontweight='bold')\n",
    "    ax_waveform.set_ylabel('Amplitude')\n",
    "    ax_waveform.grid(True, alpha=0.3)\n",
    "    ax_waveform.legend(['Channel 1', 'Channel 2', 'Channel 3'], loc='upper right')\n",
    "    \n",
    "    # Row 2: Ground Truth\n",
    "    ax_gt_left = fig.add_subplot(gs[1, 0])\n",
    "    ax_gt_right = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    for ax in [ax_gt_left, ax_gt_right]:\n",
    "        ax.plot(sample[\"y\"][0], label='P-phase GT', color='red', alpha=0.6)\n",
    "        ax.plot(sample[\"y\"][1], label='S-phase GT', color='blue', alpha=0.6)\n",
    "        ax.plot(y_p_peaks, sample[\"y\"][0, y_p_peaks], 'o', \n",
    "               label=f'P peaks ({len(y_p_peaks)})', color='darkred', markersize=8)\n",
    "        ax.plot(y_s_peaks, sample[\"y\"][1, y_s_peaks], 'o', \n",
    "               label=f'S peaks ({len(y_s_peaks)})', color='darkblue', markersize=8)\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax_gt_left.set_title('Ground Truth Labels', fontsize=12, fontweight='bold')\n",
    "    ax_gt_right.set_title('Ground Truth Labels', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Row 3: Teacher Predictions\n",
    "    ax_teacher = fig.add_subplot(gs[2, 0])\n",
    "    ax_teacher.plot(teacher_p_prob, label='P-phase Prob', color='red', alpha=0.6)\n",
    "    ax_teacher.plot(teacher_p_peaks, teacher_p_prob[teacher_p_peaks], 'x', \n",
    "                   label=f'P peaks ({len(teacher_p_peaks)})', color='darkred', markersize=10, markeredgewidth=2)\n",
    "    ax_teacher.plot(teacher_s_prob, label='S-phase Prob', color='blue', alpha=0.6)\n",
    "    ax_teacher.plot(teacher_s_peaks, teacher_s_prob[teacher_s_peaks], 'x', \n",
    "                   label=f'S peaks ({len(teacher_s_peaks)})', color='darkblue', markersize=10, markeredgewidth=2)\n",
    "    ax_teacher.set_title('Teacher Model Predictions (PhaseNet)', fontsize=12, fontweight='bold', color='green')\n",
    "    ax_teacher.set_ylabel('Probability')\n",
    "    ax_teacher.legend(loc='upper right', fontsize=8)\n",
    "    ax_teacher.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 3: Student Predictions\n",
    "    ax_student = fig.add_subplot(gs[2, 1])\n",
    "    ax_student.plot(student_p_prob, label='P-phase Prob', color='red', alpha=0.6)\n",
    "    ax_student.plot(student_p_peaks, student_p_prob[student_p_peaks], 'x', \n",
    "                   label=f'P peaks ({len(student_p_peaks)})', color='darkred', markersize=10, markeredgewidth=2)\n",
    "    ax_student.plot(student_s_prob, label='S-phase Prob', color='blue', alpha=0.6)\n",
    "    ax_student.plot(student_s_peaks, student_s_prob[student_s_peaks], 'x', \n",
    "                   label=f'S peaks ({len(student_s_peaks)})', color='darkblue', markersize=10, markeredgewidth=2)\n",
    "    ax_student.set_title('Student Model Predictions', fontsize=12, fontweight='bold', color='purple')\n",
    "    ax_student.set_ylabel('Probability')\n",
    "    ax_student.legend(loc='upper right', fontsize=8)\n",
    "    ax_student.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 4: Probability comparison overlay - Teacher\n",
    "    ax_overlay_teacher = fig.add_subplot(gs[3, 0])\n",
    "    ax_overlay_teacher.plot(sample[\"y\"][0], label='GT P-phase', color='red', \n",
    "                           linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax_overlay_teacher.plot(teacher_p_prob, label='Teacher P-phase', color='red', alpha=0.8)\n",
    "    ax_overlay_teacher.plot(sample[\"y\"][1], label='GT S-phase', color='blue', \n",
    "                           linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax_overlay_teacher.plot(teacher_s_prob, label='Teacher S-phase', color='blue', alpha=0.8)\n",
    "    ax_overlay_teacher.set_title('Teacher vs Ground Truth Overlay', fontsize=11)\n",
    "    ax_overlay_teacher.set_ylabel('Probability')\n",
    "    ax_overlay_teacher.legend(loc='upper right', fontsize=7)\n",
    "    ax_overlay_teacher.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 4: Probability comparison overlay - Student\n",
    "    ax_overlay_student = fig.add_subplot(gs[3, 1])\n",
    "    ax_overlay_student.plot(sample[\"y\"][0], label='GT P-phase', color='red', \n",
    "                           linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax_overlay_student.plot(student_p_prob, label='Student P-phase', color='red', alpha=0.8)\n",
    "    ax_overlay_student.plot(sample[\"y\"][1], label='GT S-phase', color='blue', \n",
    "                           linestyle='--', alpha=0.5, linewidth=2)\n",
    "    ax_overlay_student.plot(student_s_prob, label='Student S-phase', color='blue', alpha=0.8)\n",
    "    ax_overlay_student.set_title('Student vs Ground Truth Overlay', fontsize=11)\n",
    "    ax_overlay_student.set_ylabel('Probability')\n",
    "    ax_overlay_student.legend(loc='upper right', fontsize=7)\n",
    "    ax_overlay_student.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 5: Direct Teacher vs Student comparison\n",
    "    ax_compare_p = fig.add_subplot(gs[4, 0])\n",
    "    ax_compare_p.plot(teacher_p_prob, label='Teacher P-phase', color='green', alpha=0.7, linewidth=2)\n",
    "    ax_compare_p.plot(student_p_prob, label='Student P-phase', color='purple', \n",
    "                     alpha=0.7, linewidth=2, linestyle='--')\n",
    "    ax_compare_p.plot(sample[\"y\"][0], label='Ground Truth', color='black', \n",
    "                     alpha=0.4, linewidth=1, linestyle=':')\n",
    "    ax_compare_p.set_title('P-phase: Teacher vs Student', fontsize=11)\n",
    "    ax_compare_p.set_xlabel('Sample Index')\n",
    "    ax_compare_p.set_ylabel('Probability')\n",
    "    ax_compare_p.legend(loc='upper right', fontsize=8)\n",
    "    ax_compare_p.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax_compare_s = fig.add_subplot(gs[4, 1])\n",
    "    ax_compare_s.plot(teacher_s_prob, label='Teacher S-phase', color='green', alpha=0.7, linewidth=2)\n",
    "    ax_compare_s.plot(student_s_prob, label='Student S-phase', color='purple', \n",
    "                     alpha=0.7, linewidth=2, linestyle='--')\n",
    "    ax_compare_s.plot(sample[\"y\"][1], label='Ground Truth', color='black', \n",
    "                     alpha=0.4, linewidth=1, linestyle=':')\n",
    "    ax_compare_s.set_title('S-phase: Teacher vs Student', fontsize=11)\n",
    "    ax_compare_s.set_xlabel('Sample Index')\n",
    "    ax_compare_s.set_ylabel('Probability')\n",
    "    ax_compare_s.legend(loc='upper right', fontsize=8)\n",
    "    ax_compare_s.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle(f'Example {i}: Teacher vs Student Model Comparison', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Save figure\n",
    "    plot_filename = os.path.join(output_folder, f\"Comparison_{i:03d}.png\")\n",
    "    plt.savefig(plot_filename, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Calculate residuals for both models\n",
    "    teacher_residuals_p = []\n",
    "    teacher_residuals_s = []\n",
    "    student_residuals_p = []\n",
    "    student_residuals_s = []\n",
    "    \n",
    "    for y_p_time in y_p_times:\n",
    "        if len(teacher_p_times) > 0:\n",
    "            residuals = teacher_p_times - y_p_time\n",
    "            teacher_residuals_p.append(residuals[np.argmin(np.abs(residuals))])\n",
    "        if len(student_p_times) > 0:\n",
    "            residuals = student_p_times - y_p_time\n",
    "            student_residuals_p.append(residuals[np.argmin(np.abs(residuals))])\n",
    "    \n",
    "    for y_s_time in y_s_times:\n",
    "        if len(teacher_s_times) > 0:\n",
    "            residuals = teacher_s_times - y_s_time\n",
    "            teacher_residuals_s.append(residuals[np.argmin(np.abs(residuals))])\n",
    "        if len(student_s_times) > 0:\n",
    "            residuals = student_s_times - y_s_time\n",
    "            student_residuals_s.append(residuals[np.argmin(np.abs(residuals))])\n",
    "    \n",
    "    # Save results to text file\n",
    "    results_filename = os.path.join(output_folder, f\"Comparison_{i:03d}_Results.txt\")\n",
    "    with open(results_filename, \"w\") as f:\n",
    "        f.write(f\"Example {i} - Comparison Results\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"GROUND TRUTH:\\n\")\n",
    "        f.write(f\"  P arrival times: {y_p_times}\\n\")\n",
    "        f.write(f\"  S arrival times: {y_s_times}\\n\\n\")\n",
    "        \n",
    "        f.write(\"TEACHER MODEL (PhaseNet):\\n\")\n",
    "        f.write(f\"  Predicted P times: {teacher_p_times}\\n\")\n",
    "        f.write(f\"  Predicted S times: {teacher_s_times}\\n\")\n",
    "        f.write(f\"  P residuals: {teacher_residuals_p}\\n\")\n",
    "        f.write(f\"  S residuals: {teacher_residuals_s}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STUDENT MODEL:\\n\")\n",
    "        f.write(f\"  Predicted P times: {student_p_times}\\n\")\n",
    "        f.write(f\"  Predicted S times: {student_s_times}\\n\")\n",
    "        f.write(f\"  P residuals: {student_residuals_p}\\n\")\n",
    "        f.write(f\"  S residuals: {student_residuals_s}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE COMPARISON:\\n\")\n",
    "        if teacher_residuals_p and student_residuals_p:\n",
    "            f.write(f\"  Teacher P MAE: {np.mean(np.abs(teacher_residuals_p)):.4f}s\\n\")\n",
    "            f.write(f\"  Student P MAE: {np.mean(np.abs(student_residuals_p)):.4f}s\\n\")\n",
    "        if teacher_residuals_s and student_residuals_s:\n",
    "            f.write(f\"  Teacher S MAE: {np.mean(np.abs(teacher_residuals_s)):.4f}s\\n\")\n",
    "            f.write(f\"  Student S MAE: {np.mean(np.abs(student_residuals_s)):.4f}s\\n\")\n",
    "    \n",
    "    if (i % 10 == 0):\n",
    "        print(f\"  Generated {i}/100 comparison examples...\")\n",
    "\n",
    "print(f\"\\nâœ“ All comparison examples saved to '{output_folder}/'\")\n",
    "print(f\"âœ“ Generated {100} comparison plots\")\n",
    "\n",
    "# Save parameters\n",
    "params_filename = os.path.join(output_folder, \"Comparison_Parameters.txt\")\n",
    "with open(params_filename, \"w\") as f:\n",
    "    f.write(\"COMPARISON PARAMETERS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Sampling Rate: {sampling_rate} Hz\\n\")\n",
    "    f.write(f\"Height Parameter: {height}\\n\")\n",
    "    f.write(f\"Distance Parameter: {distance}\\n\")\n",
    "\n",
    "print(f\"âœ“ Parameters saved to '{params_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b8659-87fe-4bf9-bed1-8e9eac464f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
