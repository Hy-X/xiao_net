{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2ee661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import seisbench.models as sbm\n",
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "\n",
    "from seisbench.util import worker_seeding\n",
    "from torch.utils.data import DataLoader\n",
    "from obspy.clients.fdsn import Client\n",
    "from scipy.signal import find_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7482716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from JSON file\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96b06235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peak_detection': {'sampling_rate': 100, 'height': 0.5, 'distance': 100},\n",
       " 'training': {'batch_size': 64,\n",
       "  'num_workers': 4,\n",
       "  'learning_rate': 0.01,\n",
       "  'epochs': 50,\n",
       "  'patience': 5,\n",
       "  'loss_weights': [0.01, 0.4, 0.59],\n",
       "  'optimization': {'mixed_precision': True,\n",
       "   'gradient_accumulation_steps': 1,\n",
       "   'pin_memory': True,\n",
       "   'prefetch_factor': 2,\n",
       "   'persistent_workers': True}},\n",
       " 'device': {'use_cuda': True, 'device_id': 0}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cac5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f080377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Loader the picker\n",
    "#model = sbm.EQTransformer.from_pretrained(\"original\")\n",
    "model = sbm.PhaseNet.from_pretrained(\"stead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c820e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhaseNet(\n",
       "  (inc): Conv1d(3, 8, kernel_size=(7,), stride=(1,), padding=same)\n",
       "  (in_bn): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (down_branch): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(8, 8, kernel_size=(7,), stride=(4,), padding=(3,), bias=False)\n",
       "      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): Conv1d(8, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(16, 16, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(32, 32, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): Conv1d(32, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(64, 64, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ModuleList(\n",
       "      (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (1): BatchNorm1d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2-3): 2 x None\n",
       "    )\n",
       "  )\n",
       "  (up_branch): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): ConvTranspose1d(128, 64, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): ConvTranspose1d(64, 32, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(64, 32, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): ConvTranspose1d(32, 16, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): ConvTranspose1d(16, 8, kernel_size=(7,), stride=(4,), bias=False)\n",
       "      (1): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Conv1d(16, 8, kernel_size=(7,), stride=(1,), padding=same, bias=False)\n",
       "      (3): BatchNorm1d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Conv1d(8, 3, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device(f\"cuda:{config['device']['device_id']}\" if torch.cuda.is_available() and config['device']['use_cuda'] else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33f94ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "data = sbd.OKLA_1Mil_120s_Ver_3(sampling_rate=100, force=True, component_order=\"ENZ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f44060f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating random sample of 2.0% of the data...\n"
     ]
    }
   ],
   "source": [
    "# Create a random sample\n",
    "sample_fraction = 0.02  # Sample 20% of the data\n",
    "print(f\"Creating random sample of {sample_fraction*100}% of the data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ed9bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 22782\n"
     ]
    }
   ],
   "source": [
    "# Create a random mask for sampling\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "mask = np.random.random(len(data)) < sample_fraction\n",
    "data.filter(mask)\n",
    "\n",
    "print(f\"Sampled dataset size: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "189aeaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>station_network_code</th>\n",
       "      <th>station_code</th>\n",
       "      <th>trace_channel</th>\n",
       "      <th>station_latitude_deg</th>\n",
       "      <th>station_longitude_deg</th>\n",
       "      <th>station_elevation_m</th>\n",
       "      <th>trace_p_arrival_sample</th>\n",
       "      <th>trace_p_status</th>\n",
       "      <th>trace_p_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>trace_snr_db</th>\n",
       "      <th>trace_coda_end_sample</th>\n",
       "      <th>trace_start_time</th>\n",
       "      <th>trace_category</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>split</th>\n",
       "      <th>trace_name_original</th>\n",
       "      <th>trace_chunk</th>\n",
       "      <th>trace_sampling_rate_hz</th>\n",
       "      <th>trace_component_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>2V</td>\n",
       "      <td>TG11</td>\n",
       "      <td>EHE</td>\n",
       "      <td>35.2689</td>\n",
       "      <td>-97.8146</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-11T10:04:26.195000Z</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>bucket0$53,:3,:12001</td>\n",
       "      <td>train</td>\n",
       "      <td>2V.TG11.EHE.EHN.EHZ.2023-09-11T1004262023-09-1...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>2V</td>\n",
       "      <td>TG11</td>\n",
       "      <td>EHE</td>\n",
       "      <td>35.2689</td>\n",
       "      <td>-97.8146</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-15T03:02:11.464999Z</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>bucket0$97,:3,:12001</td>\n",
       "      <td>train</td>\n",
       "      <td>2V.TG11.EHE.EHN.EHZ.2023-10-15T0302112023-10-1...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>2V</td>\n",
       "      <td>TG11</td>\n",
       "      <td>EHE</td>\n",
       "      <td>35.2689</td>\n",
       "      <td>-97.8146</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-02T09:08:31.285000Z</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>bucket0$124,:3,:12001</td>\n",
       "      <td>train</td>\n",
       "      <td>2V.TG11.EHE.EHN.EHZ.2023-11-02T0908312023-11-0...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>205</td>\n",
       "      <td>2V</td>\n",
       "      <td>TG11</td>\n",
       "      <td>EHE</td>\n",
       "      <td>35.2689</td>\n",
       "      <td>-97.8146</td>\n",
       "      <td>407.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-29T04:45:18.075000Z</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>bucket5$28,:3,:12001</td>\n",
       "      <td>test</td>\n",
       "      <td>2V.TG11.EHE.EHN.EHZ.2023-11-29T0445182023-11-2...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>208</td>\n",
       "      <td>2V</td>\n",
       "      <td>TG11</td>\n",
       "      <td>EHE</td>\n",
       "      <td>35.2689</td>\n",
       "      <td>-97.8146</td>\n",
       "      <td>407.0</td>\n",
       "      <td>5999.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-30T00:58:44.934999Z</td>\n",
       "      <td>earthquake_local</td>\n",
       "      <td>bucket0$153,:3,:12001</td>\n",
       "      <td>train</td>\n",
       "      <td>2V.TG11.EHE.EHN.EHZ.2023-11-30T0058442023-11-3...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index station_network_code station_code trace_channel  \\\n",
       "72      72                   2V         TG11           EHE   \n",
       "128    128                   2V         TG11           EHE   \n",
       "171    171                   2V         TG11           EHE   \n",
       "205    205                   2V         TG11           EHE   \n",
       "208    208                   2V         TG11           EHE   \n",
       "\n",
       "     station_latitude_deg  station_longitude_deg  station_elevation_m  \\\n",
       "72                35.2689               -97.8146                407.0   \n",
       "128               35.2689               -97.8146                407.0   \n",
       "171               35.2689               -97.8146                407.0   \n",
       "205               35.2689               -97.8146                407.0   \n",
       "208               35.2689               -97.8146                407.0   \n",
       "\n",
       "     trace_p_arrival_sample trace_p_status  trace_p_weight  ...  trace_snr_db  \\\n",
       "72                   6000.0         manual             1.0  ...           NaN   \n",
       "128                  6000.0         manual             1.0  ...           NaN   \n",
       "171                  6000.0         manual             1.0  ...           NaN   \n",
       "205                  6000.0         manual             1.0  ...           NaN   \n",
       "208                  5999.0         manual             1.0  ...           NaN   \n",
       "\n",
       "     trace_coda_end_sample             trace_start_time    trace_category  \\\n",
       "72                     NaN  2023-09-11T10:04:26.195000Z  earthquake_local   \n",
       "128                    NaN  2023-10-15T03:02:11.464999Z  earthquake_local   \n",
       "171                    NaN  2023-11-02T09:08:31.285000Z  earthquake_local   \n",
       "205                    NaN  2023-11-29T04:45:18.075000Z  earthquake_local   \n",
       "208                    NaN  2023-11-30T00:58:44.934999Z  earthquake_local   \n",
       "\n",
       "                trace_name  split  \\\n",
       "72    bucket0$53,:3,:12001  train   \n",
       "128   bucket0$97,:3,:12001  train   \n",
       "171  bucket0$124,:3,:12001  train   \n",
       "205   bucket5$28,:3,:12001   test   \n",
       "208  bucket0$153,:3,:12001  train   \n",
       "\n",
       "                                   trace_name_original  trace_chunk  \\\n",
       "72   2V.TG11.EHE.EHN.EHZ.2023-09-11T1004262023-09-1...                \n",
       "128  2V.TG11.EHE.EHN.EHZ.2023-10-15T0302112023-10-1...                \n",
       "171  2V.TG11.EHE.EHN.EHZ.2023-11-02T0908312023-11-0...                \n",
       "205  2V.TG11.EHE.EHN.EHZ.2023-11-29T0445182023-11-2...                \n",
       "208  2V.TG11.EHE.EHN.EHZ.2023-11-30T0058442023-11-3...                \n",
       "\n",
       "     trace_sampling_rate_hz  trace_component_order  \n",
       "72                      100                    ZNE  \n",
       "128                     100                    ZNE  \n",
       "171                     100                    ZNE  \n",
       "205                     100                    ZNE  \n",
       "208                     100                    ZNE  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample metadata:\")\n",
    "data.metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "935662b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, dev, test = data.train_dev_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d00a4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: OKLA_1Mil_120s_Ver_3 - 15909 traces\n",
      "Dev: OKLA_1Mil_120s_Ver_3 - 3429 traces\n",
      "Test: OKLA_1Mil_120s_Ver_3 - 3444 traces\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Dev:\", dev)\n",
    "print(\"Test:\", test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b87844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up data augmentation\n",
    "\n",
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_pP_arrival_sample\": \"P\",\n",
    "    \"trace_P_arrival_sample\": \"P\",\n",
    "    \"trace_P1_arrival_sample\": \"P\",\n",
    "    \"trace_Pg_arrival_sample\": \"P\",\n",
    "    \"trace_Pn_arrival_sample\": \"P\",\n",
    "    \"trace_PmP_arrival_sample\": \"P\",\n",
    "    \"trace_pwP_arrival_sample\": \"P\",\n",
    "    \"trace_pwPm_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\",\n",
    "    \"trace_S_arrival_sample\": \"S\",\n",
    "    \"trace_S1_arrival_sample\": \"S\",\n",
    "    \"trace_Sg_arrival_sample\": \"S\",\n",
    "    \"trace_SmS_arrival_sample\": \"S\",\n",
    "    \"trace_Sn_arrival_sample\": \"S\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6caeb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generators for training and validation\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bce0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase lists for labeling\n",
    "p_phases = [key for key, val in phase_dict.items() if val == \"P\"]\n",
    "s_phases = [key for key, val in phase_dict.items() if val == \"S\"]\n",
    "\n",
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(dev)\n",
    "test_generator = sbg.GenericGenerator(test)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
    "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
    "    sbg.Normalize(demean_axis=-1, detrend_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(sigma=30, dim=0),\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n",
    "test_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45694b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for peak detection\n",
    "sampling_rate = config['peak_detection']['sampling_rate']\n",
    "height = config['peak_detection']['height']\n",
    "distance = config['peak_detection']['distance']\n",
    "\n",
    "batch_size = config['training']['batch_size']\n",
    "num_workers = config['training']['num_workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17af6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for machine learning\n",
    "\n",
    "train_loader = DataLoader(train_generator,batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n",
    "val_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,pin_memory=True,prefetch_factor=4,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd3ceba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_fn(y_pred, y_true, eps=1e-5):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h = h.mean(-1).sum(-1)\n",
    "    h = h.mean()\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46a133ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and number of epochs\n",
    "learning_rate = config['training']['learning_rate']\n",
    "epochs = config['training']['epochs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8121db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0b00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, checkpoint_path='checkpoint.pt', \n",
    "                 best_model_path='best_model.pth', final_model_path='final_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_model_path = best_model_path\n",
    "        self.final_model_path = final_model_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.save_best_model(model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                self.save_final_model(model)\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.save_best_model(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.checkpoint_path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def save_best_model(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Saving best model to {self.best_model_path}')\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "\n",
    "    def save_final_model(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Early stopping triggered. Saving final model to {self.final_model_path}')\n",
    "        torch.save(model.state_dict(), self.final_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5e97a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train for one epoch\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        pred = model(batch[\"X\"].to(device))\n",
    "        loss = loss_fn(pred, batch[\"y\"].to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 5 == 0:\n",
    "            print(f\"loss: {loss.item():>7f}  [{batch_id * len(batch['X']):>5d}/{size:>5d}]\")\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd322460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pred = model(batch[\"X\"].to(device))\n",
    "            val_loss += loss_fn(pred, batch[\"y\"].to(device)).item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef876741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.fill_between(range(len(history['train_loss'])), \n",
    "                     history['train_loss'], history['val_loss'],\n",
    "                     alpha=0.3, color='red', \n",
    "                     where=(np.array(history['val_loss']) > np.array(history['train_loss'])),\n",
    "                     label='Potential Overfitting Gap')\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ead9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.214218  [    0/15909]\n",
      "loss: 0.134593  [  320/15909]\n",
      "loss: 0.113847  [  640/15909]\n",
      "loss: 0.100160  [  960/15909]\n",
      "loss: 0.092996  [ 1280/15909]\n",
      "loss: 0.105860  [ 1600/15909]\n",
      "loss: 0.081091  [ 1920/15909]\n",
      "loss: 0.084028  [ 2240/15909]\n",
      "loss: 0.077439  [ 2560/15909]\n",
      "loss: 0.076635  [ 2880/15909]\n",
      "loss: 0.085372  [ 3200/15909]\n",
      "loss: 0.074468  [ 3520/15909]\n",
      "loss: 0.077225  [ 3840/15909]\n",
      "loss: 0.079956  [ 4160/15909]\n",
      "loss: 0.075479  [ 4480/15909]\n",
      "loss: 0.084582  [ 4800/15909]\n",
      "loss: 0.081816  [ 5120/15909]\n",
      "loss: 0.074436  [ 5440/15909]\n",
      "loss: 0.066858  [ 5760/15909]\n",
      "loss: 0.083428  [ 6080/15909]\n",
      "loss: 0.081881  [ 6400/15909]\n",
      "loss: 0.067153  [ 6720/15909]\n",
      "loss: 0.069370  [ 7040/15909]\n",
      "loss: 0.074499  [ 7360/15909]\n",
      "loss: 0.071951  [ 7680/15909]\n",
      "loss: 0.077865  [ 8000/15909]\n",
      "loss: 0.071058  [ 8320/15909]\n",
      "loss: 0.074053  [ 8640/15909]\n",
      "loss: 0.068093  [ 8960/15909]\n",
      "loss: 0.062439  [ 9280/15909]\n",
      "loss: 0.085430  [ 9600/15909]\n",
      "loss: 0.072582  [ 9920/15909]\n",
      "loss: 0.061198  [10240/15909]\n",
      "loss: 0.076199  [10560/15909]\n",
      "loss: 0.069212  [10880/15909]\n",
      "loss: 0.072804  [11200/15909]\n",
      "loss: 0.072486  [11520/15909]\n",
      "loss: 0.073234  [11840/15909]\n",
      "loss: 0.073602  [12160/15909]\n",
      "loss: 0.060575  [12480/15909]\n",
      "loss: 0.069801  [12800/15909]\n",
      "loss: 0.063911  [13120/15909]\n",
      "loss: 0.071901  [13440/15909]\n",
      "loss: 0.073369  [13760/15909]\n",
      "loss: 0.072792  [14080/15909]\n",
      "loss: 0.066686  [14400/15909]\n",
      "loss: 0.054938  [14720/15909]\n",
      "loss: 0.067864  [15040/15909]\n",
      "loss: 0.072024  [15360/15909]\n",
      "loss: 0.071247  [15680/15909]\n",
      "Epoch 1 results: Train loss: 0.077890, Val loss: 0.066892\n",
      "Validation loss decreased (inf --> 0.066892). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 2/50\n",
      "loss: 0.075451  [    0/15909]\n",
      "loss: 0.065802  [  320/15909]\n",
      "loss: 0.073412  [  640/15909]\n",
      "loss: 0.068876  [  960/15909]\n",
      "loss: 0.072493  [ 1280/15909]\n",
      "loss: 0.060278  [ 1600/15909]\n",
      "loss: 0.061294  [ 1920/15909]\n",
      "loss: 0.071180  [ 2240/15909]\n",
      "loss: 0.065939  [ 2560/15909]\n",
      "loss: 0.062926  [ 2880/15909]\n",
      "loss: 0.070781  [ 3200/15909]\n",
      "loss: 0.056691  [ 3520/15909]\n",
      "loss: 0.068558  [ 3840/15909]\n",
      "loss: 0.059186  [ 4160/15909]\n",
      "loss: 0.065080  [ 4480/15909]\n",
      "loss: 0.059642  [ 4800/15909]\n",
      "loss: 0.061912  [ 5120/15909]\n",
      "loss: 0.058633  [ 5440/15909]\n",
      "loss: 0.065780  [ 5760/15909]\n",
      "loss: 0.065491  [ 6080/15909]\n",
      "loss: 0.062509  [ 6400/15909]\n",
      "loss: 0.066811  [ 6720/15909]\n",
      "loss: 0.072158  [ 7040/15909]\n",
      "loss: 0.061156  [ 7360/15909]\n",
      "loss: 0.061186  [ 7680/15909]\n",
      "loss: 0.066546  [ 8000/15909]\n",
      "loss: 0.067288  [ 8320/15909]\n",
      "loss: 0.065329  [ 8640/15909]\n",
      "loss: 0.071537  [ 8960/15909]\n",
      "loss: 0.067746  [ 9280/15909]\n",
      "loss: 0.079550  [ 9600/15909]\n",
      "loss: 0.070165  [ 9920/15909]\n",
      "loss: 0.059367  [10240/15909]\n",
      "loss: 0.067391  [10560/15909]\n",
      "loss: 0.067689  [10880/15909]\n",
      "loss: 0.071806  [11200/15909]\n",
      "loss: 0.062839  [11520/15909]\n",
      "loss: 0.060049  [11840/15909]\n",
      "loss: 0.060087  [12160/15909]\n",
      "loss: 0.063387  [12480/15909]\n",
      "loss: 0.063336  [12800/15909]\n",
      "loss: 0.069347  [13120/15909]\n",
      "loss: 0.067652  [13440/15909]\n",
      "loss: 0.066918  [13760/15909]\n",
      "loss: 0.063970  [14080/15909]\n",
      "loss: 0.060914  [14400/15909]\n",
      "loss: 0.065208  [14720/15909]\n",
      "loss: 0.064879  [15040/15909]\n",
      "loss: 0.059649  [15360/15909]\n",
      "loss: 0.062773  [15680/15909]\n",
      "Epoch 2 results: Train loss: 0.064984, Val loss: 0.063632\n",
      "Validation loss decreased (0.066892 --> 0.063632). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 3/50\n",
      "loss: 0.060380  [    0/15909]\n",
      "loss: 0.073640  [  320/15909]\n",
      "loss: 0.062755  [  640/15909]\n",
      "loss: 0.063370  [  960/15909]\n",
      "loss: 0.069644  [ 1280/15909]\n",
      "loss: 0.061897  [ 1600/15909]\n",
      "loss: 0.059683  [ 1920/15909]\n",
      "loss: 0.065157  [ 2240/15909]\n",
      "loss: 0.057962  [ 2560/15909]\n",
      "loss: 0.062510  [ 2880/15909]\n",
      "loss: 0.058537  [ 3200/15909]\n",
      "loss: 0.058875  [ 3520/15909]\n",
      "loss: 0.063203  [ 3840/15909]\n",
      "loss: 0.065081  [ 4160/15909]\n",
      "loss: 0.061708  [ 4480/15909]\n",
      "loss: 0.064536  [ 4800/15909]\n",
      "loss: 0.059598  [ 5120/15909]\n",
      "loss: 0.059341  [ 5440/15909]\n",
      "loss: 0.056541  [ 5760/15909]\n",
      "loss: 0.056825  [ 6080/15909]\n",
      "loss: 0.057821  [ 6400/15909]\n",
      "loss: 0.060903  [ 6720/15909]\n",
      "loss: 0.072986  [ 7040/15909]\n",
      "loss: 0.059816  [ 7360/15909]\n",
      "loss: 0.066662  [ 7680/15909]\n",
      "loss: 0.065738  [ 8000/15909]\n",
      "loss: 0.058153  [ 8320/15909]\n",
      "loss: 0.063377  [ 8640/15909]\n",
      "loss: 0.062910  [ 8960/15909]\n",
      "loss: 0.061857  [ 9280/15909]\n",
      "loss: 0.065283  [ 9600/15909]\n",
      "loss: 0.060784  [ 9920/15909]\n",
      "loss: 0.072521  [10240/15909]\n",
      "loss: 0.066684  [10560/15909]\n",
      "loss: 0.053668  [10880/15909]\n",
      "loss: 0.071822  [11200/15909]\n",
      "loss: 0.057234  [11520/15909]\n",
      "loss: 0.052798  [11840/15909]\n",
      "loss: 0.056742  [12160/15909]\n",
      "loss: 0.066830  [12480/15909]\n",
      "loss: 0.059161  [12800/15909]\n",
      "loss: 0.062720  [13120/15909]\n",
      "loss: 0.067256  [13440/15909]\n",
      "loss: 0.064991  [13760/15909]\n",
      "loss: 0.062600  [14080/15909]\n",
      "loss: 0.067577  [14400/15909]\n",
      "loss: 0.074536  [14720/15909]\n",
      "loss: 0.063765  [15040/15909]\n",
      "loss: 0.062731  [15360/15909]\n",
      "loss: 0.053492  [15680/15909]\n",
      "Epoch 3 results: Train loss: 0.062911, Val loss: 0.063351\n",
      "Validation loss decreased (0.063632 --> 0.063351). Saving model...\n",
      "Saving best model to best_model.pth\n",
      "Epoch 4/50\n",
      "loss: 0.061454  [    0/15909]\n",
      "loss: 0.059901  [  320/15909]\n",
      "loss: 0.066199  [  640/15909]\n",
      "loss: 0.056228  [  960/15909]\n",
      "loss: 0.061013  [ 1280/15909]\n",
      "loss: 0.060694  [ 1600/15909]\n",
      "loss: 0.061795  [ 1920/15909]\n",
      "loss: 0.061509  [ 2240/15909]\n",
      "loss: 0.069160  [ 2560/15909]\n",
      "loss: 0.062531  [ 2880/15909]\n",
      "loss: 0.058620  [ 3200/15909]\n",
      "loss: 0.055167  [ 3520/15909]\n",
      "loss: 0.055208  [ 3840/15909]\n",
      "loss: 0.061822  [ 4160/15909]\n",
      "loss: 0.060940  [ 4480/15909]\n",
      "loss: 0.061719  [ 4800/15909]\n",
      "loss: 0.067245  [ 5120/15909]\n",
      "loss: 0.049640  [ 5440/15909]\n",
      "loss: 0.059240  [ 5760/15909]\n",
      "loss: 0.058829  [ 6080/15909]\n",
      "loss: 0.066397  [ 6400/15909]\n",
      "loss: 0.056573  [ 6720/15909]\n",
      "loss: 0.067950  [ 7040/15909]\n",
      "loss: 0.056011  [ 7360/15909]\n",
      "loss: 0.068827  [ 7680/15909]\n",
      "loss: 0.072635  [ 8000/15909]\n",
      "loss: 0.048528  [ 8320/15909]\n",
      "loss: 0.060082  [ 8640/15909]\n",
      "loss: 0.061574  [ 8960/15909]\n",
      "loss: 0.054649  [ 9280/15909]\n",
      "loss: 0.064534  [ 9600/15909]\n",
      "loss: 0.066643  [ 9920/15909]\n",
      "loss: 0.063994  [10240/15909]\n",
      "loss: 0.064095  [10560/15909]\n",
      "loss: 0.060158  [10880/15909]\n",
      "loss: 0.064468  [11200/15909]\n",
      "loss: 0.063931  [11520/15909]\n",
      "loss: 0.054670  [11840/15909]\n",
      "loss: 0.054749  [12160/15909]\n",
      "loss: 0.060999  [12480/15909]\n",
      "loss: 0.059803  [12800/15909]\n",
      "loss: 0.066847  [13120/15909]\n",
      "loss: 0.054640  [13440/15909]\n",
      "loss: 0.055051  [13760/15909]\n",
      "loss: 0.073770  [14080/15909]\n",
      "loss: 0.052916  [14400/15909]\n",
      "loss: 0.063840  [14720/15909]\n",
      "loss: 0.060225  [15040/15909]\n",
      "loss: 0.070857  [15360/15909]\n",
      "loss: 0.070979  [15680/15909]\n"
     ]
    }
   ],
   "source": [
    "# Training routine with EarlyStopping and scheduler\n",
    "def train_model(train_loader, val_loader, model, optimizer, loss_fn, device, num_epochs=25, patience=7):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} results: Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    plot_training_history(history)\n",
    "    return model, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the training function\n",
    "    patience = config['training']['patience'] if 'patience' in config['training'] else 7\n",
    "    trained_model, training_history = train_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        num_epochs=epochs,\n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = evaluate_model(test_loader, trained_model, loss_fn, device)\n",
    "    print(f\"Final test loss: {test_loss:.6f}\")\n",
    "    \n",
    "    print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22683322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
